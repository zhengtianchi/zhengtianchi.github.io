[{"categories":null,"content":"golang context context 是 go 中控制协程的一种比较方便的方式。 ","date":"5051-08-529","objectID":"/context/:0:0","tags":["golang"],"title":"golang context","uri":"/context/"},{"categories":null,"content":"Select + Chan 我们都知道一个 goroutine 启动后，我们是无法控制他的，大部分情况是等待它自己结束，那么如果这个 goroutine 是一个不会自己结束的后台 goroutine 呢？比如监控等，会一直运行的。 这种情况下比较笨的办法是全局变量，其他地方通过修改这个变量完成结束通知，然后后台 goroutine 不停的检查这个变量，如果发现被通知关闭了，就自我结束。这种方式首先我们要保证这个变量在多线程下的安全，基于此，有一种经典的处理方式：chan + select 。 func main() { stop := make(chan bool) go func() { for { select { case \u003c-stop: // 收到了停滞信号 fmt.Println(\"监控退出，停止了...\") return default: fmt.Println(\"goroutine监控中...\") time.Sleep(2 * time.Second) } } }() time.Sleep(10 * time.Second) fmt.Println(\"可以了，通知监控停止\") stop\u003c- true //为了检测监控过是否停止，如果没有监控输出，就表示停止了 time.Sleep(5 * time.Second) } 这里还有一个额外的知识，go 中通道的接收是有阻塞和非阻塞的（发送只有阻塞），这里 select 中的 case 语句接收通道数据其实是非阻塞的。非阻塞接收通道数据的 CPU 消耗较高，但是可以获取是否通道中有数据的状态。但是当 case 上读一个通道时，如果这个通道是 nil，则该 case 永远阻塞 例子中我们定义一个 stop 的 chan，通知他结束后台 goroutine。实现也非常简单，在后台 goroutine 中，使用 select 判断 stop 是否可以接收到值，如果可以接收到，就表示可以退出停止了；如果没有接收到，就会执行 default 里的监控逻辑，继续监控，只到收到 stop 的通知。 有了以上的逻辑，我们就可以在其他 goroutine 中，给 stop chan 发送值了，例子中是在 main goroutine 中发送的，控制让这个监控的 goroutine 结束。 发送了 stop\u003c-true 结束的指令后，我这里使用 time.Sleep(5 * time.Second) 故意停顿5秒来检测我们结束监控 goroutine 是否成功。如果成功的话，不会再有 goroutine监控中... 的输出了；如果没有成功，监控 goroutine 就会继续打印 goroutine监控中... 输出。 这种 chan+select 的方式，是比较优雅的结束一个 goroutine 的方式，不过这种方式也有局限性，如果有很多 goroutine 都需要控制结束怎么办呢？如果这些 goroutine 又衍生了其他更多的 goroutine 怎么办呢？如果一层层的无穷尽的 goroutine 呢？这就非常复杂了，即使我们定义很多 chan 也很难解决这个问题，因为 goroutine 的关系链就导致了这种场景非常复杂。 ","date":"5051-08-529","objectID":"/context/:0:1","tags":["golang"],"title":"golang context","uri":"/context/"},{"categories":null,"content":"Context 上面说的这种场景是存在的，比如一个网络请求 Request，每个 Request 都需要开启一个 goroutine 做一些事情，这些 goroutine 又可能会开启其他的 goroutine。所以我们需要一种可以跟踪 goroutine 的方案，才可以达到控制他们的目的，这就是 Go 语言为我们提供的 Context，称之为上下文非常贴切，它就是 goroutine 的上下文。 下面我们就使用 Go Context 重写上面的示例。 func main() { ctx, cancel := context.WithCancel(context.Background()) go func(ctx context.Context) { for { select { case \u003c-ctx.Done(): fmt.Println(\"监控退出，停止了...\") return default: fmt.Println(\"goroutine监控中...\") time.Sleep(2 * time.Second) } } }(ctx) time.Sleep(10 * time.Second) fmt.Println(\"可以了，通知监控停止\") cancel() //为了检测监控过是否停止，如果没有监控输出，就表示停止了 time.Sleep(5 * time.Second) } 重写也很简单，把之前使用 select+chan 控制的协程改为 Context 控制即可： context.Background() 返回一个空的 Context，这个空的 Context 一般用于整个 Context 树的根节点。然后我们使用 context.WithCancel(parent) 函数，创建一个可取消的子 Context，然后当作参数传给 goroutine 使用，这样就可以使用这个子 Context 跟踪这个 goroutine。 在 goroutine 中，使用 select 调用 \u003c-ctx.Done() 判断是否要结束，如果接受到值的话，就可以返回结束 goroutine 了；如果接收不到，就会继续进行监控。 那么 Context 是如何发送结束指令的呢？这就是示例中的 cancel 函数啦，它是我们调用 context.WithCancel(parent) 函数生成子 Context 的时候返回的，第二个返回值就是这个取消函数，它是 CancelFunc 类型的。我们调用它就可以发出取消指令，然后我们的监控 goroutine 就会收到信号，就会返回结束。 ","date":"5051-08-529","objectID":"/context/:0:2","tags":["golang"],"title":"golang context","uri":"/context/"},{"categories":null,"content":"Context 控制多个 goroutine 使用 Context 控制一个 goroutine 的例子如上，非常简单，下面我们看看控制多个 goroutine 的例子，其实也比较简单。 func main() { ctx, cancel := context.WithCancel(context.Background()) go watch(ctx,\"【监控1】\") go watch(ctx,\"【监控2】\") go watch(ctx,\"【监控3】\") time.Sleep(10 * time.Second) fmt.Println(\"可以了，通知监控停止\") cancel() //为了检测监控过是否停止，如果没有监控输出，就表示停止了 time.Sleep(5 * time.Second) } func watch(ctx context.Context, name string) { for { select { case \u003c-ctx.Done(): fmt.Println(name,\"监控退出，停止了...\") return default: fmt.Println(name,\"goroutine监控中...\") time.Sleep(2 * time.Second) } } } 示例中启动了 3 个监控 goroutine 进行不断的监控，每一个都使用了 Context 进行跟踪，当我们使用 cancel 函数通知取消时，这 3 个 goroutine 都会被结束。这就是 Context 的控制能力，它就像一个控制器一样，按下开关后，所有基于这个 Context 或者衍生的子 Context 都会收到通知，这时就可以进行清理操作了，最终释放 goroutine，这就优雅的解决了 goroutine 启动后不可控的问题。 ","date":"5051-08-529","objectID":"/context/:0:3","tags":["golang"],"title":"golang context","uri":"/context/"},{"categories":null,"content":"Context 接口 Context 的接口定义的比较简洁，我们看下这个接口的方法。 type Context interface { Deadline() (deadline time.Time, ok bool) Done() \u003c-chan struct{} Err() error Value(key interface{}) interface{} } 这个接口共有 4 个方法，了解这些方法的意思非常重要，这样我们才可以更好的使用他们。 Deadline 方法是获取设置的截止时间的意思，第一个返回式是截止时间，到了这个时间点，Context 会自动发起取消请求；第二个返回值 ok 表示是否设置截止时间，如果没有设置时间，当需要取消的时候，需要调用取消函数进行取消。 Done 方法返回一个只读的 chan，类型为 struct{}，我们在 goroutine 中，如果该方法返回的 chan 可以读取，则意味着 parent context 已经发起了取消请求，我们通过 Done 方法收到这个信号后，就应该做清理操作，然后退出 goroutine，释放资源。 Err 方法返回取消的错误原因，因为什么 Context 被取消。 Value 方法获取该 Context 上绑定的值，是一个键值对，所以要通过一个 Key 才可以获取对应的值，这个值一般是线程安全的。 以上四个方法中常用的就是 Done 了，如果 Context 取消的时候，我们就可以得到一个关闭的 chan，关闭的 chan 是可以读取的，所以只要 ctx.Done() 可以读取的时候，就意味着收到 Context 取消的信号了，以下是这个方法的经典用法。 func Stream(ctx context.Context, out chan\u003c- Value) error { for { v, err := DoSomething(ctx) if err != nil { return err } select { case \u003c-ctx.Done(): return ctx.Err() case out \u003c- v: } } } Context 接口并不需要我们实现，Go 内置已经帮我们实现了 2 个，我们代码中最开始都是以这两个内置的作为最顶层的 partent context，衍生出更多的子 Context。 var ( background = new(emptyCtx) todo = new(emptyCtx) ) func Background() Context { return background } func TODO() Context { return todo } 一个是 Background，主要用于 main 函数、初始化以及测试代码中，作为 Context 这个树结构的最顶层的 Context，也就是根 Context。 一个是 TODO，它目前还不知道具体的使用场景，如果我们不知道该使用什么 Context 的时候，可以使用这个。 他们两个本质上都是 emptyCtx 结构体类型，是一个不可取消，没有设置截止时间，没有携带任何值的 Context。 type emptyCtx int func (*emptyCtx) Deadline() (deadline time.Time, ok bool) { return } func (*emptyCtx) Done() \u003c-chan struct{} { return nil } func (*emptyCtx) Err() error { return nil } func (*emptyCtx) Value(key interface{}) interface{} { return nil } ","date":"5051-08-529","objectID":"/context/:0:4","tags":["golang"],"title":"golang context","uri":"/context/"},{"categories":null,"content":"Context 的继承衍生 有了如上的根 Context，那么是如何衍生更多的子 Context 的呢？这就要靠 context 包为我们提供的 With 系列的函数了。 func WithCancel(parent Context) (ctx Context, cancel CancelFunc) func WithDeadline(parent Context, deadline time.Time) (Context, CancelFunc) func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) func WithValue(parent Context, key, val interface{}) Context 这四个 With 函数，接收的都有一个 partent 参数，就是父 Context，我们要基于这个父 Context 创建出子 Context 的意思，这种方式可以理解为子 Context 对父 Context 的继承，也可以理解为基于父 Context 的衍生。 通过这些函数，就创建了一颗 Context 树，树的每个节点都可以有任意多个子节点，节点层级可以有任意多个。每个父节点可以控制自己所有的子节点。 WithCancel 函数，传递一个父 Context 作为参数，返回子 Context，以及一个取消函数用来取消 Context。 WithDeadline 函数，和 WithCancel 差不多，它会多传递一个截止时间参数，意味着到了这个时间点，会自动取消 Context，当然我们也可以不等到这个时候，可以提前通过取消函数进行取消。 WithTimeout 和 WithDeadline 基本上一样，这个表示是超时自动取消，是多少时间后自动取消 Context 的意思。 WithValue 函数和取消 Context 无关，它是为了生成一个绑定了一个键值对数据的 Context，这个绑定的数据可以通过 Context.Value方法访问到，后面我们会专门讲。 大家可能留意到，前三个函数都返回一个取消函数 CancelFunc，这是一个函数类型，它的定义非常简单。 type CancelFunc func() 这就是取消函数的类型，该函数可以取消一个 Context，以及这个节点 Context 下所有的所有的 Context，不管有多少层级。 ","date":"5051-08-529","objectID":"/context/:0:5","tags":["golang"],"title":"golang context","uri":"/context/"},{"categories":null,"content":"WithValue 传递元数据 通过 Context 我们也可以传递一些必须的元数据，这些数据会附加在 Context 上以供使用。 var key string=\"name\" func main() { ctx, cancel := context.WithCancel(context.Background()) //附加值 valueCtx:=context.WithValue(ctx,key,\"【监控1】\") go watch(valueCtx) time.Sleep(10 * time.Second) fmt.Println(\"可以了，通知监控停止\") cancel() //为了检测监控过是否停止，如果没有监控输出，就表示停止了 time.Sleep(5 * time.Second) } func watch(ctx context.Context) { for { select { case \u003c-ctx.Done(): //取出值 fmt.Println(ctx.Value(key),\"监控退出，停止了...\") return default: //取出值 fmt.Println(ctx.Value(key),\"goroutine监控中...\") time.Sleep(2 * time.Second) } } } 在前面的例子，我们通过传递参数的方式，把 name 的值传递给监控函数。在这个例子里，我们实现一样的效果，但是通过的是 Context 的 Value 的方式。 我们可以使用 context.WithValue 方法附加一对 K-V 的键值对，这里 Key 必须是等价性的，也就是具有可比性；Value 值要是线程安全的。 这样我们就生成了一个新的 Context，这个新的 Context 带有这个键值对，在使用的时候，可以通过 Value 方法读取 ctx.Value(key)。 记住，使用 WithValue 传值，一般是必须的值，不要什么值都传递。 ","date":"5051-08-529","objectID":"/context/:0:6","tags":["golang"],"title":"golang context","uri":"/context/"},{"categories":null,"content":"Context 使用原则 不要把 Context 放在结构体中，要以参数的方式传递。 以 Context 作为参数的函数方法，应该把 Context 作为第一个参数，放在第一位。 给一个函数方法传递 Context 的时候，不要传递 nil，如果不知道传递什么，就使用 context.TODO。 Context 的 Value 相关方法应该传递必须的数据，不要什么数据都使用这个传递。 Context 是线程安全的，可以放心的在多个 goroutine 中传递。 ","date":"5051-08-529","objectID":"/context/:0:7","tags":["golang"],"title":"golang context","uri":"/context/"},{"categories":["kubernetes"],"content":"cni 详解，非常详细的总结","date":"1017-08-151","objectID":"/cni/","tags":["kubernetes"],"title":"kubernetes cni","uri":"/cni/"},{"categories":["kubernetes"],"content":"k8s cni explain in detail ","date":"1017-08-151","objectID":"/cni/:0:0","tags":["kubernetes"],"title":"kubernetes cni","uri":"/cni/"},{"categories":["kubernetes"],"content":"1. Network 在没有搭建 kubernetes 集群时，如果想让两台不同的物理节点上的容器之间进行通信，这就需要第三方工具来实现，常用的 calico 和 flannel。 ","date":"1017-08-151","objectID":"/cni/:1:0","tags":["kubernetes"],"title":"kubernetes cni","uri":"/cni/"},{"categories":["kubernetes"],"content":"2. docker network mode docker 有四种网络模式： none : 容器有独立的 Network namespace. 将容器添加到一个容器专门的网络堆栈中，没有对外连接。 只能使用 loopback 网络设备，容器只能使用 127.0.0.1 的本机网络。 host : 此网络驱动直接使用宿主机的网络 将容器添加到主机的网络堆栈中，没有隔离 容器将不会虚拟出自己的网卡，配置自己的 IP 等，而是使用宿主机的 IP 和端口。 bridge : 默认网络方式 为每一个容器分配 IP ，并将容器连接到一个 docker0 虚拟网桥 自定义网桥 : 用户自定义网桥，有更多的灵活性、隔离性等。 Network plugins : 可以安装和使用第三方的网络插件 ","date":"1017-08-151","objectID":"/cni/:2:0","tags":["kubernetes"],"title":"kubernetes cni","uri":"/cni/"},{"categories":["kubernetes"],"content":"3. CNI CNI(container network interface) CNCF下的一个项目，由coreOS提出 通过插件的方式统一配置: flannel : 基于overlay 不支持网络策略 calico : 基于BGP 支持网络策略 canal : 支持网络策略 通过给 Kubelet 传递 CNI 选项： –network-plugin=cni 命令行选项来选择 CNI 插件。 –cni-conf-dir （默认是 /etc/cni/net.d） 读取文件并使用该文件中的 CNI 配置来设置每个 pod 的网络。 –cni-bin-dir （默认是 /opt/cni/bin）配置引用的任何所需的 CNI 插件都必须存在于该目录下。 各种解决方案对比如下： 特性/方案 FLANNEL CALICO MACVLAN OPENVSWITCH 直接路由 方案特性 通过虚拟设备 flannel0 实现对 docker0 的管理 基于 BGP 协议的纯三层的网络方案 基于 Linux Kernel 的 macvlan 技术 基于隧道的虚拟路由技术 基于 Linux Kernel 的 vRouter 技术 对底层网络的要求 三层互通(vxlan,udp) 二层互通(host-gw) 三层互通（ipip） 二层互通(bgp) 二层互通 三层互通 二层互通 配置难易度 简单，基于 etcd 简单，基于 etcd 简单，直接使用宿主机网络，需要仔细规划IP地址范围 复杂，需要手工配置各个节点的bridge 简单，使用宿主机 vRoute 功能，需要仔细规划每个node的IP地址范围 网络性能 host-gw \u003e Vxlan \u003e UDP BGP 模式性能损失小，IPIP模式较小 性能损失可忽略 效能损失较小 性能损失小 网络连通性限制 无 在不支持 BGP 协议的网络环境中无法使用 基于 macvlan 的容器无法与宿主机网络通信 无 在无法实现大二层互通的网络环境下无法使用 ","date":"1017-08-151","objectID":"/cni/:3:0","tags":["kubernetes"],"title":"kubernetes cni","uri":"/cni/"},{"categories":["kubernetes"],"content":"4. Flannel 网络原理 ","date":"1017-08-151","objectID":"/cni/:4:0","tags":["kubernetes"],"title":"kubernetes cni","uri":"/cni/"},{"categories":["kubernetes"],"content":"4.1 Flannel 简介 Flannel 是 CoreOS 团队针对 Kubernetes 设计的一个网络规划服务，简单来说，它的功能是让集群中的不同节点主机创建的 Docker 容器都具有全集群唯一的虚拟 IP 地址。 Flannel 实质上是一种覆盖网络(overlaynetwork)。 overlay 覆盖网络： 也就是将 TCP 数据包装在另一种网络包里面进行路由转发和通信 在基础网络的基础上叠加的一种虚拟网络技术模式，该网络的主机通过虚拟链路连接起来。 目前已经支持udp、vxlan、host-gw、aws-vpc、gce 和 alloc 路由等数据转发方式，默认的节点间数据通信方式是 UDP 转发。 4.1.1 flannel 模式 flanneld可以在启动时通过配置文件指定不同的backend进行网络通信，目前比较成熟的backend有UDP、VXLAN和host-gateway三种。目前，VXLAN是官方比较推崇的一种backend实现方式。 UDP模式和VXLAN模式基于三层网络层即可实现，而host-gateway模式就必须要求集群所有机器在同一个广播域，也就是需要在二层网络同一个交换机下才能实现。 host-gateway一般用于对网络性能要求比较高的场景，但需要基础网络架构的支持；UDP则用于测试及一般比较老的不支持VXLAN的Linux内核。 Flannel host-gw 模式必须要求集群宿主机之间是二层连通的.就是node1和node2在一个局域网.通过arp协议可以访问到. 总结： 三层通，二层不通，使用 VXLAN或UDP， 推荐使用 VXLAN 二层通的情况下，考虑性能问题，推荐使用 host-gw 模式，当然，二层都通了，vxlan 和 udp 也是可以使用的 其中 UDP 和 VXLAN 都是隧道模式，host-gw 则是纯三层网络方案。 Host-gw模式 你设置 Flannel 使用 host-gw 模式之后，flanneld 会在宿主机上创建这样一条规则，以 Node 1 为例： 当需要从容器1请求到容器2时. $ ip route ... 10.244.1.0/24 via 10.168.0.3 dev eth0 根据配置,会从eth0出去,mac地址是node2地址,目的ip肯定是10.244.1.3 node2收到后,会根据路由,将包转发给容器cni0,cni0转发给10.244.1.3. 这条路由规则的含义是：目的 IP 地址属于 10.244.1.0/24 网段的 IP 包，应该经过本机的 eth0 设备发出去（即：dev eth0）；并且，它下一跳地址（next-hop）是 10.168.0.3（即：via 10.168.0.3）。 所谓下一跳地址就是：如果 IP 包从主机 A 发到主机 B，需要经过路由设备 X 的中转。那么 X 的 IP 地址就应该配置为主机 A 的下一跳地址。 而从 host-gw 示意图中我们可以看到，这个下一跳地址对应的，正是我们的目的宿主机 Node 2。 可以看到node2充当了容器间请求的网关!这也是flannel-gw的由来 一旦配置了下一跳地址，那么接下来，当 IP 包从网络层进入链路层封装成帧的时候，eth0 设备就会使用下一跳地址对应的 MAC 地址，作为该数据帧的目的 MAC 地址。显然，这个 MAC 地址，正是 Node 2 的 MAC 地址。 这样，这个数据帧就会从 Node 1 通过宿主机的二层网络顺利到达 Node 2 上。 而 Node 2 的内核网络栈从二层数据帧里拿到 IP 包后，会“看到”这个 IP 包的目的 IP 地址是 10.244.1.3，即 Infra-container-2 的 IP 地址。这时候，根据 Node 2 上的路由表，该目的地址会匹配到第二条路由规则（也就是 10.244.1.0 对应的路由规则），从而进入 cni0 网桥，进而进入到 Infra-container-2 当中。 可以看到，host-gw 模式的工作原理，其实就是将每个 Flannel 子网（Flannel Subnet，比如：10.244.1.0/24）的“下一跳”，设置成了该子网对应的宿主机的 IP 地址。 也就是说，这台“主机”（Host）会充当这条容器通信路径里的“网关”（Gateway）。这也正是“host-gw”的含义。 而且 Flannel 子网和主机的信息，都是保存在 Etcd 当中的。flanneld 只需要 WACTH 这些数据的变化，然后实时更新路由表即可。 而在这种模式下，容器通信的过程就免除了额外的封包和解包带来的性能损耗。根据实际的测试，host-gw 的性能损失大约在 10% 左右，而其他所有基于 VXLAN“隧道”机制的网络方案，性能损失都在 20%~30% 左右。 host-gw 模式能够正常工作的核心，就在于 IP 包在封装成帧发送出去的时候，会使用路由表里的“下一跳”来设置目的 MAC 地址。这样，它就会经过二层网络到达目的宿主机。 所以说，Flannel host-gw 模式必须要求集群宿主机之间是二层连通的。 这种模式下，容器通信的过程就免除了额外的封包和解包带来的性能损耗。根据实际的测试，host-gw 的性能损失大约在 10% 左右，而其他所有基于 VXLAN“隧道”机制的网络方案，性能损失都在 20%~30% 左右 Flannel host-gw 模式必须要求集群宿主机之间是二层连通的.就是node1和node2在一个局域网.通过arp协议可以访问到. 那么如果node1和node2不在一个局域网咋整,那么就需要通过几个子网来达到通信的目的.就是多个路由器. 那么还有一个保证,这几个路由器的转发表也要有相应的配置,不能去的时候可以去,回不来了就尴尬了 udp模式 flannel0 该方案使用时会在各个 Work 节点上运行一个Flannel 进程，同时创建一个 flannel0 设备 ，而这个 flannel0 它是一个 TUN 设备（Tunnel 设备）。 在 Linux 中，TUN 设备是一种工作在三层（Network Layer）的虚拟网络设备。TUN 设备的功能非常简单，即：在操作系统内核和用户应用程序之间传递 IP 包。 当操作系统将一个 IP 包发送给 flannel0 设备之后，flannel0 就会把这个 IP 包，交给创建这个设备的应用程序，也就是 Flannel 进程。 这是一个从内核态向用户态的流动方向。 反之，如果 Flannel 进程向 flannel0 设备发送了一个 IP 包，那么这个 IP 包就会出现在宿主机网络栈中，然后根据宿主机的路由表进行下一步处理。 这是一个从用户态向内核态的流动方向。 Subnet 子网（Subnet) 是 Flannel 项目里一个非常重要的概念。 事实上，在由 Flannel 管理的容器网络里，一台宿主机上的所有容器，都属于该宿主机被分配的一个“子网”。 在我们的例子中，Node 1 的子网是 100.96.1.0/24，container-1 的 IP 地址是 100.96.1.2。Node 2 的子网是 100.96.2.0/24，container-2 的 IP 地址是 100.96.2.3。 而这些子网与宿主机的对应关系，正是保存在 Etcd 当中，如下所示： $ etcdctl ls /coreos.com/network/subnets /coreos.com/network/subnets/100.96.1.0-24 /coreos.com/network/subnets/100.96.2.0-24 /coreos.com/network/subnets/100.96.3.0-24 所以，flanneld 进程在处理由 flannel0 传入的 IP 包时，就可以根据目的 IP 的地址（比如 100.96.2.3），匹配到对应的子网（比如 100.96.2.0/24），然后从 Etcd 中找到这个子网对应的宿主机的 IP 地址，如下所示： $ etcdctl get /coreos.com/network/subnets/100.96.2.0-24 {\"PublicIP\":\"10.168.0.3\"} 即根据容器IP确定子网，根据子网确定目标宿主机IP。 具体步骤 step 1：容器到宿主机 container-1 容器里的进程发起的 IP 包，其源地址就是 100.96.1.2，目的地址就是 100.96.2.3。 由于目的地址 100.96.2.3 并不在 Node 1 的 docker0 网桥的网段里，所以这个 IP 包会被交给默认路由规则，通过容器的网关进入 docker0 网桥（如果是同一台宿主机上的容器间通信，走的是直连规则），从而出现在宿主机上。 step 2：宿主机路由到 flannel0 设备 这时候，这个 IP 包的下一个目的地，就取决于宿主机上的路由规则了。 Flannel 已经在宿主机上创建出了一系列的路由规则。 以 Node 1 为例，如下所示： `# 在Node 1上 $ ip route default via 10.168.0.1 dev eth0 100.96.0.0/16 dev flannel0 proto kernel scope link src 100.96.1.0 100.96.1.0/24 dev docker0 proto kernel scope link src 100","date":"1017-08-151","objectID":"/cni/:4:1","tags":["kubernetes"],"title":"kubernetes cni","uri":"/cni/"},{"categories":["kubernetes"],"content":"4.2 flannel 网络特点 Flannel 网络特点有： 使集群中的不同 Node 主机创建的Docker容器都具有全集群唯一的虚拟 IP 地址。 建立一个覆盖网络（overlay network），通过这个覆盖网络，将数据包原封不动的传递到目标容器。覆盖网络是建立在另一个网络之上并由其基础设施支持的虚拟网络。覆盖网络通过将一个分组封装在另一个分组内来将网络服务与底层基础设施分离。在将封装的数据包转发到端点后，将其解封装。 创建一个新的虚拟网卡 flannel0 接收 docker0 网桥的数据，通过维护路由表，对接收到的数据进行封包和转发（vxlan）。 etcd 保证了所有 node 上 flanneld 所看到的配置是一致的。同时每个node上的 flanneld 监听 etcd 上的数据变化，实时感知集群中 node 的变化。 ","date":"1017-08-151","objectID":"/cni/:4:2","tags":["kubernetes"],"title":"kubernetes cni","uri":"/cni/"},{"categories":["kubernetes"],"content":"4.3 flannel 网络原理及通信流程 Flannel是 CoreOS 团队针对 Kubernetes 设计的一个**覆盖网络（Overlay Network）**工具： 目的在于帮助每一个使用 Kuberentes 的 CoreOS 主机拥有一个完整的子网。 功能是让集群中的不同节点主机创建的 Docker 容器都具有全集群唯一的虚拟IP地址。让所有的容器相当于在同一个直连的网络，底层通过 UDP/VxLAN等进行报文的封装和转发。 其中，Overlay Network：是覆盖网络，在基础网络的基础上叠加的一种虚拟网络技术模式，该网络的主机通过虚拟链路连接起来。 VXLAN：将原数据包包装在 UDP 中，并使用基础网络的 IP/MAC 作为外层报文头进行封装，然后在以太网上传输，到达目的地后由隧道端点解封并将数据发送给目标地址。其中，VXLAN网络结构如下： Flannel：是 Overlay Network 的一种，也是将原数据包封装在另一种网络包里面进行的路由转发和通信，目前已经支持 UDP、VXLAN、AWS VPC和GCE路由数据转发方式。 具体的通信流程，不同物理节点上的容器通信流程： node1 节点的 容器 container1 产生数据，根据 容器的路由表，将数据发送给 cni0 Cni0 : 网桥设备，也就是 docker0 网桥，每创建一个 容器 都会创建一对 veth pair。其中一端是 容器 中的 eth0，另一端是 docker0 网桥中的端口（网卡）。容器 中从网卡 eth0 发出的流量都会发送到 docker0 网桥设备的端口（网卡）上。 Cni0 根据节点的路由表，将数据发送到隧道设备 flannel0 Flannel0: overlay 网络的设备，用来进行 vxlan 或其他方式 报文的处理（封包和解包）。不同 node 之间的 pod 数据流量都从 overlay 设备以隧道的形式发送到对端。 Flannel0 查看数据包的目的 ip，从 flanneld 获得对端隧道设备的必要信息，封装数据包。 Flanneld：flannel 在每个主机中运行 flanneld 作为 agent，它会为所在主机从集群的网络地址空间中，获取一个小的网段subnet，本主机内所有容器的 IP 地址都将从中分配。同时 Flanneld 监听 etcd 数据库，为 Flannel0 设备提供封装数据时必要的 mac，ip 等网络数据信息。 Flanneld 将数据包通过本机网卡发送到对端设备。对端节点的网卡接收到数据包，发现数据包为 overlay 数据包，解开外层封装，并发送内层封装到flanneld 设备。 Flannel0 设备查看数据包，根据路由表匹配，将数据发送给 Cni0 设备。 Cni0 匹配路由表，发送数据给网桥上对应的端口。 注意： Flannel 配置第三层 IPv4 Overlay 网络，会创建一个大型内部网络，并且跨越集群中的每个节点 在 Overlay 网络中，每个节点都有个子网 subnet ，用于给容器分配ip地址 同一主机中的 容器可以使用 docker0 直接通信，而不同主机上的容器需要使用 flanneld 进行封装在传输。 其中，一个具体的例子，Flannel 网络结构如下： 上图表示： 数据从源容器 cache1 container 中（IP地址为10.1.15.2）发出后，经由所在主机的 Docker0 虚拟网卡转发到 flannel0 虚拟网卡，这是个 P2P 的虚拟网卡，flanneld 服务监听在网卡的另外一端。 Flannel 通过 Etcd 服务维护了一张节点间的路由。 源主机的 flanneld 服务将原本的数据内容 UDP 封装后根据自己的路由表投递给目的节点的 flanneld 服务，数据到达以后被解包，然后直 接进入目的节点的 flannel0 虚拟网卡，然后被转发到目的主机的 Docker0 虚拟网卡，最后就像本机容器通信一下的有 Docker0 路由到达目标容器 backend1 container （IP地址为10.1.20.3）。 ","date":"1017-08-151","objectID":"/cni/:4:3","tags":["kubernetes"],"title":"kubernetes cni","uri":"/cni/"},{"categories":["kubernetes"],"content":"4.4 Flannel 网络的安装配置 可以直接使用 flannel 的 yaml 文件，在 kubernentes 中一键安装。 ","date":"1017-08-151","objectID":"/cni/:4:4","tags":["kubernetes"],"title":"kubernetes cni","uri":"/cni/"},{"categories":["kubernetes"],"content":"5. Calico 网络原理 ","date":"1017-08-151","objectID":"/cni/:5:0","tags":["kubernetes"],"title":"kubernetes cni","uri":"/cni/"},{"categories":["kubernetes"],"content":"5.1 Calico 简介 calico 是完全利用路由规则实现动态组网，通过 BGP 协议通告路由。 calico 的好处是 endpoints 组成的网络是单纯的三层网络，报文的流向完全通过路由规则控制，没有 overlay 等额外开销。 5.1.1 calico 模式 BGP工作模式： 不会有任何网桥!!!! 需要为每一个容器设置一个Vethpear,设置到宿主机上. 1.目的mac地址是node2的mac地址;目的ip地址是容器ip10.233.2.3. 2.node2收到后,mac地址是自己,那么接受包; 目的ip不是自己,那么去查询route表,应该发给容器4.ending!! 从流程也能看出Flannel host-gw和Calico要求集群宿主机之间是二层连通的 如果node1和node2不在一个子网,那node1根本没办法把包发送给node2. ——————————————— bgp工作模式和flannel的host-gw模式几乎一样； 也就是说，Calico 也会在每台宿主机上，添加一个格式如下所示的路由规则： \u003c目的容器IP地址段\u003e via \u003c网关的IP地址\u003e dev eth0 其中，网关的 IP 地址，正是目的容器所在宿主机的 IP 地址。 而正如前所述，这个三层网络方案得以正常工作的核心，是为每个容器的 IP 地址，找到它所对应的、“下一跳”的网关。 不过，不同于 Flannel 通过 Etcd 和宿主机上的 flanneld 来维护路由信息的做法，Calico 项目使用了一个“重型武器”来自动地在整个集群中分发路由信息。 所谓 BGP，就是在大规模网络中实现节点路由信息共享的一种协议。用来维护集群路由信息.如果我们不用BGP,我们就需要手动去维护路由表,以达到网络互通.不太现实,而BGP ,每个边界网关上都会运行着一个小程序，它们会将各自的路由表信息，通过 TCP 传输给其他的边界网关。而其他边界网关上的这个小程序，则会对收到的这些数据进行分析，然后将需要的信息添加到自己的路由表里. BGP 在每个边界网关（路由器）上运行，彼此之间通信更新路由表信息。而 BGP 的这个能力，正好可以取代 Flannel 维护主机上路由表的功能。 除了对路由信息的维护方式之外，Calico 项目与 Flannel 的 host-gw 模式的另一个不同之处，就是它不会在宿主机上创建任何网桥设备。 Calico 的 CNI 插件会为每个容器设置一个 Veth Pair 设备，然后把其中的一端放置在宿主机上（它的名字以 cali 前缀开头）。 此外，由于 Calico 没有使用 CNI 的网桥模式，Calico 的 CNI 插件还需要在宿主机上为每个容器的 Veth Pair 设备配置一条路由规则，用于接收传入的 IP 包。比如，宿主机 Node 2 上的 Container 4 对应的路由规则，如下所示： 即：发往 10.233.2.3 的 IP 包，应该进入 cali5863f3 设备。 10.233.2.3 dev cali5863f3 scope link 有了这样的 Veth Pair 设备之后，容器发出的 IP 包就会经过 Veth Pair 设备出现在宿主机上。然后，宿主机网络栈就会根据路由规则的下一跳 IP 地址，把它们转发给正确的网关。接下来的流程就跟 Flannel host-gw 模式完全一致了。 其中，这里最核心的“下一跳”路由规则，就是由 Calico 的 Felix 进程负责维护的。这些路由规则信息，则是通过 BGP Client 也就是 BIRD 组件，使用 BGP 协议传输而来的。 Calico 项目实际上将集群里的所有节点，都当作是边界路由器来处理，它们一起组成了一个全连通的网络，互相之间通过 BGP 协议交换路由规则。这些节点，我们称为 BGP Peer。 需要注意的是，Calico 维护的网络在默认配置下，是一个被称为“Node-to-Node Mesh”的模式。这时候，每台宿主机上的 BGP Client 都需要跟其他所有节点的 BGP Client 进行通信以便交换路由信息。但是，随着节点数量 N 的增加，这些连接的数量就会以 N²的规模快速增长，从而给集群本身的网络带来巨大的压力。 所以，Node-to-Node Mesh 模式一般推荐用在少于 100 个节点的集群里。而在更大规模的集群中，你需要用到的是一个叫作 Route Reflector 的模式。 在这种模式下，Calico 会指定一个或者几个专门的节点，来负责跟所有节点建立 BGP 连接从而学习到全局的路由规则。而其他节点，只需要跟这几个专门的节点交换路由信息，就可以获得整个集群的路由规则信息了。 bird是bgd的客户端，与集群中其它节点的bird进行通信，以便于交换各自的路由信息； 随着节点数量N的增加，这些路由规则将会以指数级的规模快速增长，给集群本身网络带来巨大压力，官方建议小于100个节点； 限制：和flannel的host-gw限制一样，要求物理机在二层是连能的，不能跨网段；因此要求集群内的机器是同一个网段下的 IPIP模式： 场景：用在跨网段通信的情况下，bgp模式在跨网段的场景将不能工作； tunl0：创建的虚拟网卡设备，此时的作用就和flannel的VxLAN工作模式类似（此处的tunl0不是flannel的UDP模式中的tun0） 举个例子，假如我们有两台处于不同子网的宿主机 Node 1 和 Node 2，对应的 IP 地址分别是 192.168.1.2 和 192.168.2.2。需要注意的是，这两台机器通过路由器实现了三层转发，所以这两个 IP 地址之间是可以相互通信的。 而我们现在的需求，还是 Container 1 要访问 Container 4。 Felix 进程在 Node 1 上添加的路由规则， Node 1 上添加如下所示的一条路由规则： 10.233.2.0/16 via 192.168.2.2 eth0 上面这条规则里的下一跳地址是 192.168.2.2，可是它对应的 Node 2 跟 Node 1 却根本不在一个子网里，没办法通过二层网络把 IP 包发送到下一跳地址。 在这种情况下，你就需要为 Calico 打开 IPIP 模式。 在 Calico 的 IPIP 模式下，Felix 进程在 Node 1 上添加的路由规则，会稍微不同，如下所示： 10.233.2.0/24 via 192.168.2.2 tunl0 这一次，要负责将 IP 包发出去的设备，变成了 tunl0，Calico 使用的这个 tunl0 设备，是一个 IP 隧道（IP tunnel）设备。 IP 包进入 IP 隧道设备之后，就会被 Linux 内核的 IPIP 驱动接管。IPIP 驱动会将这个 IP 包直接封装在一个宿主机网络的 IP 包中，如下所示： 其中，经过封装后的新的 IP 包的目的地址（图中的 Outer IP Header 部分），正是原 IP 包的下一跳地址，即 Node 2 的 IP 地址：192.168.2.2。 而原 IP 包本身，则会被直接封装成新 IP 包的 Payload。 这样，原先从容器到 Node 2 的 IP 包，就被伪装成了一个从 Node 1 到 Node 2 的 IP 包。 由于宿主机之间已经使用路由器配置了三层转发，也就是设置了宿主机之间的“下一跳”。所以这个 IP 包在离开 Node 1 之后，就可以经过路由器，最终“跳”到 Node 2 上。 这时，Node 2 的网络内核栈会使用 IPIP 驱动进行解包，从而拿到原始的 IP 包。然后，原始 IP 包就会经过路由规则和 Veth Pair 设备到达目的容器内部。 也就是说： 不同网段，二层不通，但是三层能通，这个时候不能使用bgp，因为二层不通，对端网段路由表都加不上的，就要使用三层的 IPIP 模式 ","date":"1017-08-151","objectID":"/cni/:5:1","tags":["kubernetes"],"title":"kubernetes cni","uri":"/cni/"},{"categories":["kubernetes"],"content":"5.2 Calico 原理 5.2.1 Calico 原理： Calico 在每一个计算节点利用 Linux Kernel 实现了一个高效的 vRouter 来负责数据转发 每个 vRouter 通过 BGP 协议负责把自己上运行的 workload 的路由信息向整个 Calico 网络内传播——小规模部署可以直接互联，大规模下可通过指定的 BGP route reflector 来完成。 保证最终所有的 workload 之间的数据流量都是通过 IP 路由的方式完成互联的。 Calico 节点组网可以直接利用数据中心的网络结构（无论是 L2 或者 L3），不需要额外的 NAT，隧道或者 Overlay Network。 Calico 基于 iptables 还提供了丰富而灵活的网络 Policy，保证通过各个节点上的 ACLs 来提供 Workload 的多租户隔离、安全组以及其他可达性限制等功能。 5.2.2 Calico 组成： 1）Calico 的 CNI 插件。这就是 Calico 与 Kubernetes 对接的部分。 2）Felix。它是一个 DaemonSet，负责在宿主机上插入路由规则（即：写入 Linux 内核的 FIB 转发信息库），以及维护 Calico 所需的网络设备等工作。 3）BIRD。它就是 BGP 的客户端，专门负责在集群里分发路由规则信息。 ","date":"1017-08-151","objectID":"/cni/:5:2","tags":["kubernetes"],"title":"kubernetes cni","uri":"/cni/"},{"categories":["kubernetes"],"content":"5.3 Calico 组网 组网示意图： +--------------------+ +--------------------+ | +------------+ | | +------------+ | | | | | | | | | | | ConA | | | | ConB | | | | | | | | | | | +-----+------+ | | +-----+------+ | | |veth | | |veth | | wl-A | | wl-B | | | | | | | +-------node-A-------+ +-------node-B-------+ | | | | | | type1. in the same lan | | | +-------------------------------+ | | | | type2. in different network | | +-------------+ | | | | | +-------------+ Routers |-------------+ | | +-------------+ 注意： calico 组网的核心原理就是 IP 路由，每个容器或者虚拟机会分配一个 workload-endpoint(wl)。 endpoint : 接入到 calico 网络中的网卡称为 endpoint workload-Endpoint : 虚拟机、容器使用的 endpoint 从容器 ConA 中发送给 ConB 的报文被 nodeA 的 wl-A 接收，根据 nodeA 上的路由规则，经过各种 iptables 规则后，转发到nodeB。 如果 nodeA 和 nodeB 在同一个二层网段，下一条地址直接就是 node-B，经过二层交换机即可到达。 如果 nodeA 和 nodeB 在不同的网段，报文被路由到下一跳，经过三层交换或路由器，一步步跳转到 node-B。 nodeA 怎样得知下一跳的地址？答案是 node 之间通过 BGP 协议交换路由信息。 ","date":"1017-08-151","objectID":"/cni/:5:3","tags":["kubernetes"],"title":"kubernetes cni","uri":"/cni/"},{"categories":["kubernetes"],"content":"5.4 Calico 安装配置 5.4.1 搭建 etcd 集群 安装 etcd 两台 etcd 集群，分别装有 docker yum install -y etcd 配置 /etc/etcd/etcd.conf 配置文件， 根据节点不同修改 ip 地址 ETCD_DATA_DIR=\"/var/lib/etcd/cluster.etcd\" ETCD_LISTEN_PEER_URLS=\"http://192.168.26.91:2380,http://localhost:2380\" ETCD_LISTEN_CLIENT_URLS=\"http://192.168.26.91:2379,http://localhost:2379\" ETCD_NAME=\"etcd-91\" ETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://192.168.26.91:2380\" ETCD_ADVERTISE_CLIENT_URLS=\"http://localhost:2379,http://192.168.26.91:2379\" ETCD_INITIAL_CLUSTER=\"etcd-91=http://192.168.26.91:2380,etcd-92=http://192.168.26.92:238 ETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\" ETCD_INITIAL_CLUSTER_STATE=\"new\" 启动 etcd 集群 systemctl start etcd 配置 docker 的存储为 etcd , 在两个节点上 修改 docker 启动选项，添加：（配置为自己的etcd节点） // 节点一： --cluster-store=etcd://192.168.26.91:2379 // 节点二： --cluster-store=etcd://192.168.26.92:2379 5.4.2 安装配置 calico 创建 calico 容器，用来进行两个物理节点上容器的互通, 在所有节点上执行 下载安装 calicoctl， 地址：curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.16.3/calicoctl 添加可执行权限：chmod +x calicoctl 建立 calico node 的容器，容器镜像为：quay.io/calico/node calicoctl node run --node-image=quay.io/calico/node -c /etc/calico/calicoctl.cfg 可以看到对方的主机的信息 calicoctl node status 创建 docker 网络，在任意主机上创建网络（可以同步给远端） --driver calico 指定使用calico 的 libnetwork CNM driver。 --ipam-driver calico-ipam 指定使用 calico 的IPAM driver 管理IP。 calico 为 global 网络，etcd 会将 calnet1 同步到所有主机。 docker network create --driver calico --ipam-driver calico-ipam calnet1 每在主机上创建一个容器，则会在物理机上创建一张虚拟网卡出来 在 vms91 上创建一个容器 : docker run --name c91 --net calnet1 -itd busybox 在 vms92 上创建一个容器 : docker run --name c92 --net calnet1 -itd busybox 两个容器就可以互相通信了 ","date":"1017-08-151","objectID":"/cni/:5:4","tags":["kubernetes"],"title":"kubernetes cni","uri":"/cni/"},{"categories":["kubernetes"],"content":"5.5 calico 网卡分析 创建好 calico node 后，在没有创建容器时，只有 本地网卡和docker0网桥，没有其他网卡。 当创建一个busybox 容器后，在容器中 和 在 物理机中都多了一张网卡 在容器中的网卡名称为：6: cali0@if7 在物理机中的网卡名称为：7: calif6391d136be@if6 可以看到： 物理机网卡的序号 7 与 容器网卡的 @if7 对应 物理机网卡的 @if6 与 物理机网卡的 序号6 对应 建立了 veth pair 关系，详细：https://www.cnblogs.com/bakari/p/10613710.html 在容器中的路由表 route -n，可以看到，所有的数据包的默认网关是 cali0 网卡 在物理集中查看路由表 route -n， 可以看到， 凡是去往本机容器中的地址的数据包，默认网卡是 calif6391d136be 凡是去往其他物理机中容器地址的数据包，默认网卡都是 本机的网卡出去 也就是说，在calico的节点上，通过egb协议，相互学习路由 每台主机都知道不同的容器在哪台主机上，所以会动态的设置路由。 ","date":"1017-08-151","objectID":"/cni/:5:5","tags":["kubernetes"],"title":"kubernetes cni","uri":"/cni/"},{"categories":["kubernetes"],"content":"6 总结 Kubernetes通过一个叫做CNI的接口，维护了一个单独的网桥来代替docker0。这个网桥的名字就叫作：CNI网桥，它在宿主机上的设备名称默认是：cni0。 容器“跨主通信”的三种主流实现方法：UDP、host-gw、VXLAN。 之前介绍了UDP和VXLAN，它们都属于隧道模式，需要封装和解封装。接下来介绍一种纯三层网络方案，host-gw模式和Calico项目 Host-gw模式通过在宿主机上添加一个路由规则： \u003c目的容器IP地址段\u003e via \u003c网关的IP地址\u003e dev eth0 IP包在封装成帧发出去的时候，会使用路由表里的“下一跳”来设置目的MAC地址。这样，它就会通过二层网络到达目的宿主机。 这个三层网络方案得以正常工作的核心，是为每个容器的IP地址，找到它所对应的，“下一跳”的网关。所以说，Flannel host-gw模式必须要求集群宿主机之间是二层连通的，如果宿主机分布在了不同的VLAN里（三层连通），由于需要经过的中间的路由器不一定有相关的路由配置（出于安全考虑，公有云环境下，宿主机之间的网关，肯定不会允许用户进行干预和设置），部分节点就无法找到容器IP的“下一条”网关了，host-gw就无法工作了。 Calico项目提供的网络解决方案，与Flannel的host-gw模式几乎一样，也会在宿主机上添加一个路由规则： \u003c目的容器IP地址段\u003e via \u003c网关的IP地址\u003e dev eth0 其中，网关的IP地址，正是目的容器所在宿主机的IP地址，而正如前面所述，这个三层网络方案得以正常工作的核心，是为每个容器的IP地址，找到它所对应的，“下一跳”的网关。区别是如何维护路由信息： Host-gw : Flannel通过Etcd和宿主机上的flanneld来维护路由信息 Calico: 通过BGP（边界网关协议）来实现路由自治，所谓BGP，就是在大规模网络中实现节点路由信息共享的一种协议。 三层网络主要通过维护路由规则，将数据包直接转发到对应的宿主机上。 Flannel host-gw 主要通过 etcd 中的子网信息来维护路由规则； Calico 则通过 BGP 协议收集路由信息，由 Felix进程来维护； 隧道模式主要通过在 IP 包外再封装一层 MAC 包头来实现。 额外的封包和解包工作会导致集群网络性能下降，在实际测试中，Calico IPIP 模式与 Flannel VXLAN 模式的性能大致相当。 技术分类： 隧道技术（需要封装包和解包，因为需要伪装成宿主机的IP包，需要三层链通）：Flannel UDP / VXLAN / Calico IPIP 三层网络（不需要封包和解封包，需要二层链通）：Flannel host-gw / Calico 普通模式 ","date":"1017-08-151","objectID":"/cni/:6:0","tags":["kubernetes"],"title":"kubernetes cni","uri":"/cni/"},{"categories":["kubernetes"],"content":"三层和隧道的异同： 相同之处是都实现了跨主机容器的三层互通，而且都是通过对目的 MAC 地址的操作来实现的；不同之处是三层通过配置下一条主机的路由规则来实现互通，隧道则是通过通过在 IP 包外再封装一层 MAC 包头来实现。 三层的优点：少了封包和解包的过程，性能肯定是更高的。 三层的缺点：需要自己想办法维护路由规则。 隧道的优点：简单，原因是大部分工作都是由 Linux 内核的模块实现了，应用层面工作量较少。 隧道的缺点：主要的问题就是性能低。 ","date":"1017-08-151","objectID":"/cni/:6:1","tags":["kubernetes"],"title":"kubernetes cni","uri":"/cni/"},{"categories":["kubernetes"],"content":"使用场景 在大规模集群里，三层网络方案在宿主机上的路由规则可能会非常多，这会导致错误排查变得困难。此外，在系统故障的时候，路由规则出现重叠冲突的概率也会变大。 1）如果是在公有云上，由于宿主机网络本身比较“直白”，一般推荐更加简单的 Flannel host-gw 模式。 2）但不难看到，在私有部署环境里，Calico 项目才能够覆盖更多的场景，并为你提供更加可靠的组网方案和架构思路。 ","date":"1017-08-151","objectID":"/cni/:6:2","tags":["kubernetes"],"title":"kubernetes cni","uri":"/cni/"},{"categories":["docker"],"content":"一文搞定Containerd","date":"8085-08-846","objectID":"/containerd/","tags":["docker"],"title":"一文搞定Containerd","uri":"/containerd/"},{"categories":["docker"],"content":"在学习 Containerd 之前我们有必要对 Docker 的发展历史做一个简单的回顾，因为这里面牵涉到的组件实战是有点多，有很多我们会经常听到，但是不清楚这些组件到底是干什么用的，比如 libcontainer、runc、containerd、CRI、OCI 等等。 ","date":"8085-08-846","objectID":"/containerd/:0:0","tags":["docker"],"title":"一文搞定Containerd","uri":"/containerd/"},{"categories":["docker"],"content":"Docker 从 Docker 1.11 版本开始，Docker 容器运行就不是简单通过 Docker Daemon 来启动了，而是通过集成 containerd、runc 等多个组件来完成的。虽然 Docker Daemon 守护进程模块在不停的重构，但是基本功能和定位没有太大的变化，一直都是 CS 架构，守护进程负责和 Docker Client 端交互，并管理 Docker 镜像和容器。现在的架构中组件 containerd 就会负责集群节点上容器的生命周期管理，并向上为 Docker Daemon 提供 gRPC 接口。 docker 架构 当我们要创建一个容器的时候，现在 Docker Daemon 并不能直接帮我们创建了，而是请求 containerd 来创建一个容器，containerd 收到请求后，也并不会直接去操作容器，而是创建一个叫做 containerd-shim 的进程，让这个进程去操作容器，我们指定容器进程是需要一个父进程来做状态收集、维持 stdin 等 fd 打开等工作的，假如这个父进程就是 containerd，那如果 containerd 挂掉的话，整个宿主机上所有的容器都得退出了，而引入 containerd-shim 这个垫片就可以来规避这个问题了。 然后创建容器需要做一些 namespaces 和 cgroups 的配置，以及挂载 root 文件系统等操作，这些操作其实已经有了标准的规范，那就是 OCI（开放容器标准），runc 就是它的一个参考实现（Docker 被逼无耐将 libcontainer 捐献出来改名为 runc 的），这个标准其实就是一个文档，主要规定了容器镜像的结构、以及容器需要接收哪些操作指令，比如 create、start、stop、delete 等这些命令。runc 就可以按照这个 OCI 文档来创建一个符合规范的容器，既然是标准肯定就有其他 OCI 实现，比如 Kata、gVisor 这些容器运行时都是符合 OCI 标准的。 所以真正启动容器是通过 containerd-shim 去调用 runc 来启动容器的，runc 启动完容器后本身会直接退出，containerd-shim 则会成为容器进程的父进程, 负责收集容器进程的状态, 上报给 containerd, 并在容器中 pid 为 1 的进程退出后接管容器中的子进程进行清理, 确保不会出现僵尸进程。 而 Docker 将容器操作都迁移到 containerd 中去是因为当前做 Swarm，想要进军 PaaS 市场，做了这个架构切分，让 Docker Daemon 专门去负责上层的封装编排，当然后面的结果我们知道 Swarm 在 Kubernetes 面前是惨败，然后 Docker 公司就把 containerd 项目捐献给了 CNCF 基金会，这个也是现在的 Docker 架构。 ","date":"8085-08-846","objectID":"/containerd/:1:0","tags":["docker"],"title":"一文搞定Containerd","uri":"/containerd/"},{"categories":["docker"],"content":"CRI 我们知道 Kubernetes 提供了一个 CRI 的容器运行时接口，那么这个 CRI 到底是什么呢？这个其实也和 Docker 的发展密切相关的。 在 Kubernetes 早期的时候，当时 Docker 实在是太火了，Kubernetes 当然会先选择支持 Docker，而且是通过硬编码的方式直接调用 Docker API，后面随着 Docker 的不断发展以及 Google 的主导，出现了更多容器运行时，Kubernetes 为了支持更多更精简的容器运行时，Google 就和红帽主导推出了 CRI 标准，用于将 Kubernetes 平台和特定的容器运行时（当然主要是为了干掉 Docker）解耦。 CRI（Container Runtime Interface 容器运行时接口）本质上就是 Kubernetes 定义的一组与容器运行时进行交互的接口，所以只要实现了这套接口的容器运行时都可以对接到 Kubernetes 平台上来。不过 Kubernetes 推出 CRI 这套标准的时候还没有现在的统治地位，所以有一些容器运行时可能不会自身就去实现 CRI 接口，于是就有了 shim（垫片）， 一个 shim 的职责就是作为适配器将各种容器运行时本身的接口适配到 Kubernetes 的 CRI 接口上，其中 dockershim 就是 Kubernetes 对接 Docker 到 CRI 接口上的一个垫片实现。 cri shim Kubelet 通过 gRPC 框架与容器运行时或 shim 进行通信，其中 kubelet 作为客户端，CRI shim（也可能是容器运行时本身）作为服务器。 CRI 定义的 API(https://github.com/kubernetes/kubernetes/blob/release-1.5/pkg/kubelet/api/v1alpha1/runtime/api.proto) 主要包括两个 gRPC 服务，ImageService 和 RuntimeService，ImageService 服务主要是拉取镜像、查看和删除镜像等操作，RuntimeService 则是用来管理 Pod 和容器的生命周期，以及与容器交互的调用（exec/attach/port-forward）等操作，可以通过 kubelet 中的标志 --container-runtime-endpoint 和 --image-service-endpoint 来配置这两个服务的套接字。 kubelet cri 不过这里同样也有一个例外，那就是 Docker，由于 Docker 当时的江湖地位很高，Kubernetes 是直接内置了 dockershim 在 kubelet 中的，所以如果你使用的是 Docker 这种容器运行时的话是不需要单独去安装配置适配器之类的，当然这个举动似乎也麻痹了 Docker 公司。 dockershim 现在如果我们使用的是 Docker 的话，当我们在 Kubernetes 中创建一个 Pod 的时候，首先就是 kubelet 通过 CRI 接口调用 dockershim，请求创建一个容器，kubelet 可以视作一个简单的 CRI Client, 而 dockershim 就是接收请求的 Server，不过他们都是在 kubelet 内置的。 dockershim 收到请求后, 转化成 Docker Daemon 能识别的请求, 发到 Docker Daemon 上请求创建一个容器，请求到了 Docker Daemon 后续就是 Docker 创建容器的流程了，去调用 containerd，然后创建 containerd-shim 进程，通过该进程去调用 runc 去真正创建容器。 其实我们仔细观察也不难发现使用 Docker 的话其实是调用链比较长的，真正容器相关的操作其实 containerd 就完全足够了，Docker 太过于复杂笨重了，当然 Docker 深受欢迎的很大一个原因就是提供了很多对用户操作比较友好的功能，但是对于 Kubernetes 来说压根不需要这些功能，因为都是通过接口去操作容器的，所以自然也就可以将容器运行时切换到 containerd 来。 切换到containerd 切换到 containerd 可以消除掉中间环节，操作体验也和以前一样，但是由于直接用容器运行时调度容器，所以它们对 Docker 来说是不可见的。 因此，你以前用来检查这些容器的 Docker 工具就不能使用了。 你不能再使用 docker ps 或 docker inspect 命令来获取容器信息。由于不能列出容器，因此也不能获取日志、停止容器，甚至不能通过 docker exec 在容器中执行命令。 当然我们仍然可以下载镜像，或者用 docker build 命令构建镜像，但用 Docker 构建、下载的镜像，对于容器运行时和 Kubernetes，均不可见。为了在 Kubernetes 中使用，需要把镜像推送到镜像仓库中去。 从上图可以看出在 containerd 1.0 中，对 CRI 的适配是通过一个单独的 CRI-Containerd 进程来完成的，这是因为最开始 containerd 还会去适配其他的系统（比如 swarm），所以没有直接实现 CRI，所以这个对接工作就交给 CRI-Containerd 这个 shim 了。 然后到了 containerd 1.1 版本后就去掉了 CRI-Containerd 这个 shim，直接把适配逻辑作为插件的方式集成到了 containerd 主进程中，现在这样的调用就更加简洁了。 containerd cri 与此同时 Kubernetes 社区也做了一个专门用于 Kubernetes 的 CRI 运行时 CRI-O，直接兼容 CRI 和 OCI 规范。 cri-o 这个方案和 containerd 的方案显然比默认的 dockershim 简洁很多，不过由于大部分用户都比较习惯使用 Docker，所以大家还是更喜欢使用 dockershim 方案。 但是随着 CRI 方案的发展，以及其他容器运行时对 CRI 的支持越来越完善，Kubernetes 社区在2020年7月份就开始着手移除 dockershim 方案了：https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim，现在的移除计划是在 1.20 版本中将 kubelet 中内置的 dockershim 代码分离，将内置的 dockershim 标记为维护模式，当然这个时候仍然还可以使用 dockershim，目标是在 1.23⁄1.24 版本发布没有 dockershim 的版本（代码还在，但是要默认支持开箱即用的 docker 需要自己构建 kubelet，会在某个宽限期过后从 kubelet 中删除内置的 dockershim 代码）。那么这是否就意味这 Kubernetes 不再支持 Docker 了呢？当然不是的，这只是废弃了内置的 dockershim 功能而已，Docker 和其他容器运行时将一视同仁，不会单独对待内置支持，如果我们还想直接使用 Docker 这种容器运行时应该怎么办呢？可以将 dockershim 的功能单独提取出来独立维护一个 cri-dockerd 即可，就类似于 containerd 1.0 版本中提供的 CRI-Containerd，当然还有一种办法就是 Docker 官方社区将 CRI 接口内置到 Dockerd 中去实现。 但是我们也清楚 Dockerd 也是去直接调用的 Containerd，而 containerd 1.1 版本后就内置实现了 CRI，所以 Docker 也没必要再去单独实现 CRI 了，当 Kubernetes 不再内置支持开箱即用的 Docker 的以后，最好的方式当然也就是直接使用 Containerd 这种容器运行时，而且该容器运行时也已经经过了生产环境实践的，接下来我们就来学习下 Containerd 的使用。 ","date":"8085-08-846","objectID":"/containerd/:2:0","tags":["docker"],"title":"一文搞定Containerd","uri":"/containerd/"},{"categories":["docker"],"content":"Containerd 我们知道很早之前的 Docker Engine 中就有了 containerd，只不过现在是将 containerd 从 Docker Engine 里分离出来，作为一个独立的开源项目，目标是提供一个更加开放、稳定的容器运行基础设施。分离出来的 containerd 将具有更多的功能，涵盖整个容器运行时管理的所有需求，提供更强大的支持。 containerd 是一个工业级标准的容器运行时，它强调简单性、健壮性和可移植性，containerd 可以负责干下面这些事情： 管理容器的生命周期（从创建容器到销毁容器） 拉取/推送容器镜像 存储管理（管理镜像及容器数据的存储） 调用 runc 运行容器（与 runc 等容器运行时交互） 管理容器网络接口及网络 ","date":"8085-08-846","objectID":"/containerd/:3:0","tags":["docker"],"title":"一文搞定Containerd","uri":"/containerd/"},{"categories":["docker"],"content":"架构 containerd 可用作 Linux 和 Windows 的守护程序，它管理其主机系统完整的容器生命周期，从镜像传输和存储到容器执行和监测，再到底层存储到网络附件等等。 containerd 架构 上图是 containerd 官方提供的架构图，可以看出 containerd 采用的也是 C/S 架构，服务端通过 unix domain socket 暴露低层的 gRPC API 接口出去，客户端通过这些 API 管理节点上的容器，每个 containerd 只负责一台机器，Pull 镜像，对容器的操作（启动、停止等），网络，存储都是由 containerd 完成。具体运行容器由 runc 负责，实际上只要是符合 OCI 规范的容器都可以支持。 为了解耦，containerd 将系统划分成了不同的组件，每个组件都由一个或多个模块协作完成（Core 部分），每一种类型的模块都以插件的形式集成到 Containerd 中，而且插件之间是相互依赖的，例如，上图中的每一个长虚线的方框都表示一种类型的插件，包括 Service Plugin、Metadata Plugin、GC Plugin、Runtime Plugin 等，其中 Service Plugin 又会依赖 Metadata Plugin、GC Plugin 和 Runtime Plugin。每一个小方框都表示一个细分的插件，例如 Metadata Plugin 依赖 Containers Plugin、Content Plugin 等。比如: Content Plugin: 提供对镜像中可寻址内容的访问，所有不可变的内容都被存储在这里。 Snapshot Plugin: 用来管理容器镜像的文件系统快照，镜像中的每一层都会被解压成文件系统快照，类似于 Docker 中的 graphdriver。 总体来看 containerd 可以分为三个大块：Storage、Metadata 和 Runtime。 containerd 架构2 ","date":"8085-08-846","objectID":"/containerd/:3:1","tags":["docker"],"title":"一文搞定Containerd","uri":"/containerd/"},{"categories":["docker"],"content":"安装 这里我使用的系统是 Linux Mint 20.2，首先需要安装 seccomp 依赖： ➜ ~ apt-get update ➜ ~ apt-get install libseccomp2 -y 由于 containerd 需要调用 runc，所以我们也需要先安装 runc，不过 containerd 提供了一个包含相关依赖的压缩包 cri-containerd-cni-${VERSION}.${OS}-${ARCH}.tar.gz，可以直接使用这个包来进行安装。首先从 release 页面下载最新版本的压缩包，当前为 1.5.5 版本： ➜ ~ wget https://github.com/containerd/containerd/releases/download/v1.5.5/cri-containerd-cni-1.5.5-linux-amd64.tar.gz # 如果有限制，也可以替换成下面的 URL 加速下载 # wget https://download.fastgit.org/containerd/containerd/releases/download/v1.5.5/cri-containerd-cni-1.5.5-linux-amd64.tar.gz 可以通过 tar 的 -t 选项直接看到压缩包中包含哪些文件： ➜ ~ tar -tf cri-containerd-cni-1.4.3-linux-amd64.tar.gz etc/ etc/cni/ etc/cni/net.d/ etc/cni/net.d/10-containerd-net.conflist etc/crictl.yaml etc/systemd/ etc/systemd/system/ etc/systemd/system/containerd.service usr/ usr/local/ usr/local/bin/ usr/local/bin/containerd-shim-runc-v2 usr/local/bin/ctr usr/local/bin/containerd-shim usr/local/bin/containerd-shim-runc-v1 usr/local/bin/crictl usr/local/bin/critest usr/local/bin/containerd usr/local/sbin/ usr/local/sbin/runc opt/ opt/cni/ opt/cni/bin/ opt/cni/bin/vlan opt/cni/bin/host-local opt/cni/bin/flannel opt/cni/bin/bridge opt/cni/bin/host-device opt/cni/bin/tuning opt/cni/bin/firewall opt/cni/bin/bandwidth opt/cni/bin/ipvlan opt/cni/bin/sbr opt/cni/bin/dhcp opt/cni/bin/portmap opt/cni/bin/ptp opt/cni/bin/static opt/cni/bin/macvlan opt/cni/bin/loopback opt/containerd/ opt/containerd/cluster/ opt/containerd/cluster/version opt/containerd/cluster/gce/ opt/containerd/cluster/gce/cni.template opt/containerd/cluster/gce/configure.sh opt/containerd/cluster/gce/cloud-init/ opt/containerd/cluster/gce/cloud-init/master.yaml opt/containerd/cluster/gce/cloud-init/node.yaml opt/containerd/cluster/gce/env 直接将压缩包解压到系统的各个目录中： ➜ ~ tar -C / -xzf cri-containerd-cni-1.5.5-linux-amd64.tar.gz 当然要记得将 /usr/local/bin 和 /usr/local/sbin 追加到 ~/.bashrc 文件的 PATH 环境变量中： export PATH=$PATH:/usr/local/bin:/usr/local/sbin 然后执行下面的命令使其立即生效： ➜ ~ source ~/.bashrc containerd 的默认配置文件为 /etc/containerd/config.toml，我们可以通过如下所示的命令生成一个默认的配置： ➜ ~ mkdir /etc/containerd ➜ ~ containerd config default \u003e /etc/containerd/config.toml 由于上面我们下在的 containerd 压缩包中包含一个 etc/systemd/system/containerd.service 的文件，这样我们就可以通过 systemd 来配置 containerd 作为守护进程运行了，内容如下所示： ➜ ~ cat /etc/systemd/system/containerd.service [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target local-fs.target [Service] ExecStartPre=-/sbin/modprobe overlay ExecStart=/usr/local/bin/containerd Type=notify Delegate=yes KillMode=process Restart=always RestartSec=5 # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNPROC=infinity LimitCORE=infinity LimitNOFILE=1048576 # Comment TasksMax if your systemd version does not supports it. # Only systemd 226 and above support this version. TasksMax=infinity OOMScoreAdjust=-999 [Install] WantedBy=multi-user.target 这里有两个重要的参数： Delegate: 这个选项允许 containerd 以及运行时自己管理自己创建容器的 cgroups。如果不设置这个选项，systemd 就会将进程移到自己的 cgroups 中，从而导致 containerd 无法正确获取容器的资源使用情况。 KillMode: 这个选项用来处理 containerd 进程被杀死的方式。默认情况下，systemd 会在进程的 cgroup 中查找并杀死 containerd 的所有子进程。KillMode 字段可以设置的值如下。 control-group（默认值）：当前控制组里面的所有子进程，都会被杀掉 process：只杀主进程 mixed：主进程将收到 SIGTERM 信号，子进程收到 SIGKILL 信号 none：没有进程会被杀掉，只是执行服务的 stop 命令 我们需要将 KillMode 的值设置为 process，这样可以确保升级或重启 containerd 时不杀死现有的容器。 现在我们就可以启动 containerd 了，直接执行下面的命令即可： ➜ ~ systemctl enable containerd --now 启动完成后就可以使用 containerd 的本地 CLI 工具 ctr 了，比如查看版本： ctr version ","date":"8085-08-846","objectID":"/containerd/:3:2","tags":["docker"],"title":"一文搞定Containerd","uri":"/containerd/"},{"categories":["docker"],"content":"配置 我们首先来查看下上面默认生成的配置文件 /etc/containerd/config.toml： disabled_plugins = [] imports = [] oom_score = 0 plugin_dir = \"\" required_plugins = [] root = \"/var/lib/containerd\" state = \"/run/containerd\" version = 2 [cgroup] path = \"\" [debug] address = \"\" format = \"\" gid = 0 level = \"\" uid = 0 [grpc] address = \"/run/containerd/containerd.sock\" gid = 0 max_recv_message_size = 16777216 max_send_message_size = 16777216 tcp_address = \"\" tcp_tls_cert = \"\" tcp_tls_key = \"\" uid = 0 [metrics] address = \"\" grpc_histogram = false [plugins] [plugins.\"io.containerd.gc.v1.scheduler\"] deletion_threshold = 0 mutation_threshold = 100 pause_threshold = 0.02 schedule_delay = \"0s\" startup_delay = \"100ms\" [plugins.\"io.containerd.grpc.v1.cri\"] disable_apparmor = false disable_cgroup = false disable_hugetlb_controller = true disable_proc_mount = false disable_tcp_service = true enable_selinux = false enable_tls_streaming = false ignore_image_defined_volumes = false max_concurrent_downloads = 3 max_container_log_line_size = 16384 netns_mounts_under_state_dir = false restrict_oom_score_adj = false sandbox_image = \"k8s.gcr.io/pause:3.5\" selinux_category_range = 1024 stats_collect_period = 10 stream_idle_timeout = \"4h0m0s\" stream_server_address = \"127.0.0.1\" stream_server_port = \"0\" systemd_cgroup = false tolerate_missing_hugetlb_controller = true unset_seccomp_profile = \"\" [plugins.\"io.containerd.grpc.v1.cri\".cni] bin_dir = \"/opt/cni/bin\" conf_dir = \"/etc/cni/net.d\" conf_template = \"\" max_conf_num = 1 [plugins.\"io.containerd.grpc.v1.cri\".containerd] default_runtime_name = \"runc\" disable_snapshot_annotations = true discard_unpacked_layers = false no_pivot = false snapshotter = \"overlayfs\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.default_runtime] base_runtime_spec = \"\" container_annotations = [] pod_annotations = [] privileged_without_host_devices = false runtime_engine = \"\" runtime_root = \"\" runtime_type = \"\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.default_runtime.options] [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes] [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc] base_runtime_spec = \"\" container_annotations = [] pod_annotations = [] privileged_without_host_devices = false runtime_engine = \"\" runtime_root = \"\" runtime_type = \"io.containerd.runc.v2\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options] BinaryName = \"\" CriuImagePath = \"\" CriuPath = \"\" CriuWorkPath = \"\" IoGid = 0 IoUid = 0 NoNewKeyring = false NoPivotRoot = false Root = \"\" ShimCgroup = \"\" SystemdCgroup = false [plugins.\"io.containerd.grpc.v1.cri\".containerd.untrusted_workload_runtime] base_runtime_spec = \"\" container_annotations = [] pod_annotations = [] privileged_without_host_devices = false runtime_engine = \"\" runtime_root = \"\" runtime_type = \"\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.untrusted_workload_runtime.options] [plugins.\"io.containerd.grpc.v1.cri\".image_decryption] key_model = \"node\" [plugins.\"io.containerd.grpc.v1.cri\".registry] config_path = \"\" [plugins.\"io.containerd.grpc.v1.cri\".registry.auths] [plugins.\"io.containerd.grpc.v1.cri\".registry.configs] [plugins.\"io.containerd.grpc.v1.cri\".registry.headers] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors] [plugins.\"io.containerd.grpc.v1.cri\".x509_key_pair_streaming] tls_cert_file = \"\" tls_key_file = \"\" [plugins.\"io.containerd.internal.v1.opt\"] path = \"/opt/containerd\" [plugins.\"io.containerd.internal.v1.restart\"] interval = \"10s\" [plugins.\"io.containerd.metadata.v1.bolt\"] content_sharing_policy = \"shared\" [plugins.\"io.containerd.monitor.v1.cgroups\"] no_prometheus = false [plugins.\"io.containerd.runtime.v1.linux\"] no_shim = false runtime = \"runc\" runtime_root = \"\" shim = \"containerd-shim\" shim_debug = false [plugins.\"io.containerd.runtime.v2.task\"] platforms = [\"linux/amd64\"] [plugins.\"io.containerd.service.v1.diff-service\"] default = [\"walking\"] [plugins.\"io.containerd.snapshotter.v1.aufs\"] root_path = \"\" [plugins.\"io.containerd.snapshotter.v1.btrfs\"] root_path = \"","date":"8085-08-846","objectID":"/containerd/:3:3","tags":["docker"],"title":"一文搞定Containerd","uri":"/containerd/"},{"categories":["docker"],"content":"使用 我们知道 Docker CLI 工具提供了需要增强用户体验的功能，containerd 同样也提供一个对应的 CLI 工具：ctr，不过 ctr 的功能没有 docker 完善，但是关于镜像和容器的基本功能都是有的。接下来我们就先简单介绍下 ctr 的使用。 帮助 直接输入 ctr 命令即可获得所有相关的操作命令使用方式： ➜ ~ ctr NAME: ctr - __ _____/ /______ / ___/ __/ ___/ / /__/ /_/ / \\___/\\__/_/ containerd CLI USAGE: ctr [global options] command [command options] [arguments...] VERSION: v1.5.5 DESCRIPTION: ctr is an unsupported debug and administrative client for interacting with the containerd daemon. Because it is unsupported, the commands, options, and operations are not guaranteed to be backward compatible or stable from release to release of the containerd project. COMMANDS: plugins, plugin provides information about containerd plugins version print the client and server versions containers, c, container manage containers content manage content events, event display containerd events images, image, i manage images leases manage leases namespaces, namespace, ns manage namespaces pprof provide golang pprof outputs for containerd run run a container snapshots, snapshot manage snapshots tasks, t, task manage tasks install install a new package oci OCI tools shim interact with a shim directly help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --debug enable debug output in logs --address value, -a value address for containerd's GRPC server (default: \"/run/containerd/containerd.sock\") [$CONTAINERD_ADDRESS] --timeout value total timeout for ctr commands (default: 0s) --connect-timeout value timeout for connecting to containerd (default: 0s) --namespace value, -n value namespace to use with commands (default: \"default\") [$CONTAINERD_NAMESPACE] --help, -h show help --version, -v print the version 镜像操作 拉取镜像 拉取镜像可以使用 ctr image pull 来完成，比如拉取 Docker Hub 官方镜像 nginx:alpine，需要注意的是镜像地址需要加上 docker.io Host 地址： ➜ ~ ctr image pull docker.io/library/nginx:alpine docker.io/library/nginx:alpine: resolved |++++++++++++++++++++++++++++++++++++++| index-sha256:bead42240255ae1485653a956ef41c9e458eb077fcb6dc664cbc3aa9701a05ce: exists |++++++++++++++++++++++++++++++++++++++| manifest-sha256:ce6ca11a3fa7e0e6b44813901e3289212fc2f327ee8b1366176666e8fb470f24: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:9a6ac07b84eb50935293bb185d0a8696d03247f74fd7d43ea6161dc0f293f81f: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:e82f830de071ebcda58148003698f32205b7970b01c58a197ac60d6bb79241b0: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:d7c9fa7589ae28cd3306b204d5dd9a539612593e35df70f7a1d69ff7548e74cf: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:bf2b3ee132db5b4c65432e53aca69da4e609c6cb154e0d0e14b2b02259e9c1e3: done |++++++++++++++++++++++++++++++++++++++| config-sha256:7ce0143dee376bfd2937b499a46fb110bda3c629c195b84b1cf6e19be1a9e23b: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:3c1eaf69ff492177c34bdbf1735b6f2e5400e417f8f11b98b0da878f4ecad5fb: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:29291e31a76a7e560b9b7ad3cada56e8c18d50a96cca8a2573e4f4689d7aca77: done |++++++++++++++++++++++++++++++++++++++| elapsed: 11.9s total: 8.7 Mi (748.1 KiB/s) unpacking linux/amd64 sha256:bead42240255ae1485653a956ef41c9e458eb077fcb6dc664cbc3aa9701a05ce... done: 410.86624ms 也可以使用 --platform 选项指定对应平台的镜像。当然对应的也有推送镜像的命令 ctr image push，如果是私有镜像则在推送的时候可以通过 --user 来自定义仓库的用户名和密码。 列出本地镜像 ➜ ~ ctr image ls REF TYPE DIGEST SIZE PLATFORMS LABELS docker.io/library/nginx:alpine application/vnd.docker.distribution.manifest.list.v2+json sha256:bead42240255ae1485653a956ef41c9e458eb077fcb6dc664cbc3aa9701a05ce 9.5 MiB linux/386,linux/amd64,linux/arm/v6,linux/arm/v7,linux/arm64/v8,linux/ppc64le,linux/s390x - ➜ ~ ctr image ls -q docker.io/library/nginx:alpine 使用 -q（--quiet） 选项可以只打印镜像名称。 检测本地镜像 ➜ ~ ctr image check REF TYPE DIGEST STATUS SIZE UNPACKED docker.io/library/nginx:alpine application/vnd.docker.distribution.manifest.list.v2+json sha256:bead42240255ae1485653a956ef41c9e458eb077fcb6dc664cbc3aa9701a05ce complete (7/7) 9.5 MiB/9.5 MiB true 主要查看其中的 STATUS，comp","date":"8085-08-846","objectID":"/containerd/:3:4","tags":["docker"],"title":"一文搞定Containerd","uri":"/containerd/"},{"categories":["docker"],"content":"runc containers","date":"8085-08-832","objectID":"/runcandcontainerd/","tags":["docker"],"title":"Runc和containerd概述","uri":"/runcandcontainerd/"},{"categories":["docker"],"content":"1. 概述 各个组件调用关系图如下： 图片来源：https://www.jianshu.com/p/62e71584d1cb 2. OCI. Open Container Initiative OCI（Open Container Initiative）即开放的容器运行时规范，目的在于定义一个容器运行时及镜像的相关标准和规范，其中包括 runtime-spec：容器的生命周期管理，具体参考runtime-spec。 image-spec：镜像的生命周期管理，具体参考image-spec。 实现OCI标准的容器运行时有runc，kata等。 3. RunC **runc（run container）**是一个基于OCI标准实现的一个轻量级容器运行工具，用来创建和运行容器。而Containerd是用来维持通过runc创建的容器的运行状态。即runc用来创建和运行容器，containerd作为常驻进程用来管理容器。 runc包含libcontainer，包括对namespace和cgroup的调用操作。 命令参数： To start a new instance of a container: # runc run [ -b bundle ] \u003ccontainer-id\u003e USAGE: runc [global options] command [command options] [arguments...] COMMANDS: checkpoint checkpoint a running container create create a container delete delete any resources held by the container often used with detached container events display container events such as OOM notifications, cpu, memory, and IO usage statistics exec execute new process inside the container init initialize the namespaces and launch the process (do not call it outside of runc) kill kill sends the specified signal (default: SIGTERM) to the container's init process list lists containers started by runc with the given root pause pause suspends all processes inside the container ps ps displays the processes running inside a container restore restore a container from a previous checkpoint resume resumes all processes that have been previously paused run create and run a container spec create a new specification file start executes the user defined process in a created container state output the state of a container update update container resource constraints help, h Shows a list of commands or help for one command 4. Containerd **containerd（container daemon）**是一个daemon进程用来管理和运行容器，可以用来拉取/推送镜像和管理容器的存储和网络。其中可以调用runc来创建和运行容器。 ","date":"8085-08-832","objectID":"/runcandcontainerd/:0:0","tags":["docker"],"title":"Runc和containerd概述","uri":"/runcandcontainerd/"},{"categories":["docker"],"content":"4.1. containerd的架构图 ","date":"8085-08-832","objectID":"/runcandcontainerd/:1:0","tags":["docker"],"title":"Runc和containerd概述","uri":"/runcandcontainerd/"},{"categories":["docker"],"content":"4.2. docker与containerd、runc的关系图 更具体的调用逻辑： 5. CRI Container Runtime Interface CRI即容器运行时接口，主要用来定义k8s与容器运行时的API调用，kubelet通过CRI来调用容器运行时，只要实现了CRI接口的容器运行时就可以对接到k8s的kubelet组件。 ","date":"8085-08-832","objectID":"/runcandcontainerd/:2:0","tags":["docker"],"title":"Runc和containerd概述","uri":"/runcandcontainerd/"},{"categories":["docker"],"content":"5.1. docker与k8s调用containerd的关系图 ","date":"8085-08-832","objectID":"/runcandcontainerd/:3:0","tags":["docker"],"title":"Runc和containerd概述","uri":"/runcandcontainerd/"},{"categories":["docker"],"content":"5.2. cri-api ","date":"8085-08-832","objectID":"/runcandcontainerd/:4:0","tags":["docker"],"title":"Runc和containerd概述","uri":"/runcandcontainerd/"},{"categories":["docker"],"content":"5.2.1. runtime service // Runtime service defines the public APIs for remote container runtimes service RuntimeService { // Version returns the runtime name, runtime version, and runtime API version. rpc Version(VersionRequest) returns (VersionResponse) {} // RunPodSandbox creates and starts a pod-level sandbox. Runtimes must ensure // the sandbox is in the ready state on success. rpc RunPodSandbox(RunPodSandboxRequest) returns (RunPodSandboxResponse) {} // StopPodSandbox stops any running process that is part of the sandbox and // reclaims network resources (e.g., IP addresses) allocated to the sandbox. // If there are any running containers in the sandbox, they must be forcibly // terminated. // This call is idempotent, and must not return an error if all relevant // resources have already been reclaimed. kubelet will call StopPodSandbox // at least once before calling RemovePodSandbox. It will also attempt to // reclaim resources eagerly, as soon as a sandbox is not needed. Hence, // multiple StopPodSandbox calls are expected. rpc StopPodSandbox(StopPodSandboxRequest) returns (StopPodSandboxResponse) {} // RemovePodSandbox removes the sandbox. If there are any running containers // in the sandbox, they must be forcibly terminated and removed. // This call is idempotent, and must not return an error if the sandbox has // already been removed. rpc RemovePodSandbox(RemovePodSandboxRequest) returns (RemovePodSandboxResponse) {} // PodSandboxStatus returns the status of the PodSandbox. If the PodSandbox is not // present, returns an error. rpc PodSandboxStatus(PodSandboxStatusRequest) returns (PodSandboxStatusResponse) {} // ListPodSandbox returns a list of PodSandboxes. rpc ListPodSandbox(ListPodSandboxRequest) returns (ListPodSandboxResponse) {} // CreateContainer creates a new container in specified PodSandbox rpc CreateContainer(CreateContainerRequest) returns (CreateContainerResponse) {} // StartContainer starts the container. rpc StartContainer(StartContainerRequest) returns (StartContainerResponse) {} // StopContainer stops a running container with a grace period (i.e., timeout). // This call is idempotent, and must not return an error if the container has // already been stopped. // The runtime must forcibly kill the container after the grace period is // reached. rpc StopContainer(StopContainerRequest) returns (StopContainerResponse) {} // RemoveContainer removes the container. If the container is running, the // container must be forcibly removed. // This call is idempotent, and must not return an error if the container has // already been removed. rpc RemoveContainer(RemoveContainerRequest) returns (RemoveContainerResponse) {} // ListContainers lists all containers by filters. rpc ListContainers(ListContainersRequest) returns (ListContainersResponse) {} // ContainerStatus returns status of the container. If the container is not // present, returns an error. rpc ContainerStatus(ContainerStatusRequest) returns (ContainerStatusResponse) {} // UpdateContainerResources updates ContainerConfig of the container. rpc UpdateContainerResources(UpdateContainerResourcesRequest) returns (UpdateContainerResourcesResponse) {} // ReopenContainerLog asks runtime to reopen the stdout/stderr log file // for the container. This is often called after the log file has been // rotated. If the container is not running, container runtime can choose // to either create a new log file and return nil, or return an error. // Once it returns error, new container log file MUST NOT be created. rpc ReopenContainerLog(ReopenContainerLogRequest) returns (ReopenContainerLogResponse) {} // ExecSync runs a command in a container synchronously. rpc ExecSync(ExecSyncRequest) returns (ExecSyncResponse) {} // Exec prepares a streaming endpoint to execute a command in the container. rpc Exec(ExecRequest) returns (ExecResponse) {} // Attach prepares a streaming endpoint to attach to a running container. rpc Attach(AttachRequest) returns (AttachResponse) {} // P","date":"8085-08-832","objectID":"/runcandcontainerd/:4:1","tags":["docker"],"title":"Runc和containerd概述","uri":"/runcandcontainerd/"},{"categories":["docker"],"content":"5.2.2. image service // ImageService defines the public APIs for managing images. service ImageService { // ListImages lists existing images. rpc ListImages(ListImagesRequest) returns (ListImagesResponse) {} // ImageStatus returns the status of the image. If the image is not // present, returns a response with ImageStatusResponse.Image set to // nil. rpc ImageStatus(ImageStatusRequest) returns (ImageStatusResponse) {} // PullImage pulls an image with authentication config. rpc PullImage(PullImageRequest) returns (PullImageResponse) {} // RemoveImage removes the image. // This call is idempotent, and must not return an error if the image has // already been removed. rpc RemoveImage(RemoveImageRequest) returns (RemoveImageResponse) {} // ImageFSInfo returns information of the filesystem that is used to store images. rpc ImageFsInfo(ImageFsInfoRequest) returns (ImageFsInfoResponse) {} } ","date":"8085-08-832","objectID":"/runcandcontainerd/:4:2","tags":["docker"],"title":"Runc和containerd概述","uri":"/runcandcontainerd/"},{"categories":["docker"],"content":"5.3. cri-containerd ","date":"8085-08-832","objectID":"/runcandcontainerd/:5:0","tags":["docker"],"title":"Runc和containerd概述","uri":"/runcandcontainerd/"},{"categories":["docker"],"content":"5.3.1. CRI Plugin调用流程 kubelet调用CRI插件，通过CRI Runtime Service接口创建pod cri通过CNI接口创建和配置pod的network namespace cri调用containerd创建sandbox container（pause container ）并将容器放入pod的cgroup和namespace中 kubelet调用CRI插件，通过image service接口拉取镜像，接着通过containerd来拉取镜像 kubelet调用CRI插件，通过runtime service接口运行拉取下来的镜像服务，最后通过containerd来运行业务容器，并将容器放入pod的cgroup和namespace中。 具体参考：https://github.com/containerd/cri/blob/release/1.4/docs/architecture.md ","date":"8085-08-832","objectID":"/runcandcontainerd/:5:1","tags":["docker"],"title":"Runc和containerd概述","uri":"/runcandcontainerd/"},{"categories":["docker"],"content":"5.3.2. k8s对runtime调用的演进 由原来通过dockershim调用docker再调用containerd，直接变成通过cri-containerd调用containerd，从而减少了一层docker调用逻辑。 具体参考：https://github.com/containerd/cri/blob/release/1.4/docs/proposal.md ","date":"8085-08-832","objectID":"/runcandcontainerd/:5:2","tags":["docker"],"title":"Runc和containerd概述","uri":"/runcandcontainerd/"},{"categories":["docker"],"content":"5.4. Dockershim 在旧版本的k8s中，由于docker没有实现CRI接口，因此增加一个Dockershim来实现k8s对docker的调用。（shim：垫片，一般用来表示对第三方组件API调用的适配插件，例如k8s使用Dockershim来实现对docker接口的适配调用） ","date":"8085-08-832","objectID":"/runcandcontainerd/:6:0","tags":["docker"],"title":"Runc和containerd概述","uri":"/runcandcontainerd/"},{"categories":["docker"],"content":"5.5. CRI-O cri-o与containerd类似，用来实现容器的管理，可替换containerd的使用。 参考： https://opencontainers.org/about/overview/ https://github.com/opencontainers/runtime-spec https://github.com/kubernetes/kubernetes/blob/242a97307b34076d5d8f5bbeb154fa4d97c9ef1d/docs/devel/container-runtime-interface.md https://github.com/containerd/containerd/blob/main/docs/cri/architecture.md https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/ https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/ ","date":"8085-08-832","objectID":"/runcandcontainerd/:7:0","tags":["docker"],"title":"Runc和containerd概述","uri":"/runcandcontainerd/"},{"categories":["istio"],"content":"istio 网易轻舟.","date":"262611-08-2651","objectID":"/%E7%BD%91%E6%98%93%E8%BD%BB%E8%88%9F/","tags":["istio"],"title":"istio - 网易轻舟（转载）","uri":"/%E7%BD%91%E6%98%93%E8%BD%BB%E8%88%9F/"},{"categories":["istio"],"content":"Envoy 架构及其在网易轻舟的落地实践 Envoy 是由 Lyft 开源的一款高性能数据和服务代理软件，后被捐赠给 CNCF，并成为 CNCF 继 Kubernetes 和 Prometheus 之后第三个毕业项目。如今，Envoy 已经被广泛应用于 API 网关（Gloo、Ambassador）与服务网格（Istio、AWS Mesh、Open Service Mesh）之中，作为核心数据面组件。本文将详细介绍 Envoy 整体架构以及 Envoy 在网易数帆轻舟团队中落地实践的一些经验，希望能够增加大家对 Envoy 本身及其实际使用的了解。 ","date":"262611-08-2651","objectID":"/%E7%BD%91%E6%98%93%E8%BD%BB%E8%88%9F/:0:0","tags":["istio"],"title":"istio - 网易轻舟（转载）","uri":"/%E7%BD%91%E6%98%93%E8%BD%BB%E8%88%9F/"},{"categories":["istio"],"content":"什么是 Envoy 首先解释什么是 Envoy。Envoy 社区的定义是：Envoy 是一个开源的边缘与服务代理，专为云原生应用而设计。此处只需要抓住最核心的词–代理。 代理，本质属性就是做两件事：第一，代替客户端向服务端发送请求，第二，代理服务端向客户端提供服务。前者隐藏客户端，后者隐藏服务端。在开端代理软件领域，比较著名有 HA Proxy 以及 Nginx，而 Envoy 则是该领域的的一个后起之秀。作为为云原生而设计的高性能网络代理，它具备以下的优点： 高性能：Envoy 具备非常优越的性能。尽管 Envoy 在设计之初没有将性能作为最终的目标，而是更加强调模块化、易测试、易开发等特性，可它仍旧拥有足可媲美 Nginx 等经典代理软件的超高性能。 可观察：相比于 Nginx 等传统代理，Envoy 具备更好的可观察性，包括灵活可定制的日志，丰富的指标监控，原生的多种分布式跟踪协议支持等等，在后文中会做更详细的介绍。 社区活跃：Envoy 的社区完全开放，不存在对应的商业版本，所以不用担心部分高级功能会被锁死在商业版当中。而且 Envoy 社区非常活跃，在 Envoy 使用过程中的问题或者新的功能需求向社区提出后都可以得到很快的反馈。 动态配置：xDS 协议的提出使得 Envoy 几乎所有的配置都可以动态的下发、加载和生效，而无需重载进程。并且 xDS 协议已经成为了构建通用数据面接口协议（UDPA）的基础。 易扩展：作为一款云原生网络代理软件，可扩展性自然也是必不可少。Envoy 提供了 L4/L7 Filter 机制，可以让开发者在不侵入 Envoy 主干的前提下在各个层级对 Envoy 进行扩展和增强。 多协议：最后 Envoy 支持代理多种协议数据，包括 HTTP，Kafka，Dubbo 等等。此类多协议治理能力其实也是构建在 Envoy 强大可扩展性上的。因为 Envoy 所有的协议解析和治理都是使用 Filter 来实现的。基本上每一种协议代理能力都对应一个 L4 Filter。 Envoy 具备很多优点，但它核心的概念却很少，只有四个。基本上，Envoy 中绝大部分的模块和功能都是围绕着这四个概念展开的： Listener（监听器）：监听器负责监听数据端口，接受下游的连接和请求。作为代理软件，无论是正向代理还是反向代理，肯定要接受来自下游的连接并进行数据处理。Envoy 把相关的功能都抽象在了名为监听器的资源当中。 Cluster（集群）：集群是对真实后端服务的抽象和封装，管理后端服务连接池、负责后端服务健康检查、实现服务级熔断等等。 Filter（过滤器）：过滤器主要负责将 Listener 接收的客户端二进制数据包解析为结构化的协议数据，比如 HTTP 二进制流解析为具体的 Header、Body、Trailer、Metadata 诸如此类并进行各种流量治理。Envoy 中 Filter 分为多种类型，覆盖不同的层级和场景，是 Envoy 强大功能的源泉。 Route（路由）：路由一般是作为某个协议解析 Filter 的一部分存在。筛选器解析出结构化数据后会根据路由中具体规则选择一个 Cluster，最终发数据转发给后端服务。 注：为了避免歧义，后文中 Envoy 中 Cluster 仍旧使用英文，而不是使用中文翻译“集群”。 以上的四个概念或者说资源类型，构成了 Envoy 架构最核心的骨架。此外，Envoy 将请求的来源方向称之为下游（Downstream），而请求转发的去向称之为上游（Upstream）。 总结来说：Downstream 请求自 Listener 进入 Envoy，流经 Filter 被解析、修改、记录然后根据 Route 选择 Cluster 将其发送给 Upstream 服务。 了解以上的核心概念，对于 Envoy 的使用者和学习者来说都非常有用，因为 Envoy 的功能模块和配置使用都是和这些核心概念紧密相关的。 ","date":"262611-08-2651","objectID":"/%E7%BD%91%E6%98%93%E8%BD%BB%E8%88%9F/:0:1","tags":["istio"],"title":"istio - 网易轻舟（转载）","uri":"/%E7%BD%91%E6%98%93%E8%BD%BB%E8%88%9F/"},{"categories":["istio"],"content":"Envoy 关键特性 在了解了 Envoy 的背景和最核心的概念之后，接下来介绍 Envoy 最核心的三个特性。分别是基于 xDS 协议的动态配置，基于核心概念 Filter 的可扩展性，以及可观察性。 Envoy xDS 协议 xDS 协议是 Envoy 带来的最大的改变之一。Envoy 使用 Protobuf 来定义几乎所有的配置，然后通过 xDS 协议实现配置项的动态化加载和生效。 xDS 全称是 x Discovery Service，x 表示某种资源。在 Envoy 当中，最为核心资源自然是 Listener、Cluster、Route 和 Filter。除了 Filter 之外，其他三种资源都有一个对应的 DS 与之关联，分别是 LDS、CDS、RDS： 通过 LDS，就可以动态的下发 Listener 的配置，可以在运行时打开新的监听端口，关闭旧的监听端口或者更新某个 Listener 的 Filter 配置等等。 通过 CDS，就可以动态控制 Envoy 可以访问哪些服务。比如说，新启动了一个 Service，就可以通过 CDS 向 Envoy 下发一个新的 Cluster。之后，来自客户端的流量就可以通过 Envoy 访问到对应的服务。 通过 RDS，可以实现路由规则的动态更新和加载，可以动态的对路由表进行删改和生效。 前面最核心的四种资源之中，Filter 比较特殊，它的配置一般来说都嵌入在 LDS/CDS/RDS 之内。比如 Listener 中除了监听端口之外，最关键的配置内容就是在该端口下接收的数据需要执行哪些 Filter 链以及相关 Filter本身的配置项。这些最终都是通过 LDS 来下发。而 Cluster 中也有对应的 Filter 链，可以通过 CDS 更新和下发。而 RDS 则提供了路由粒度的 Filter 配置能力。 此外，还有一类关键 DS：EDS。EDS 是对 CDS 的补充。很多时候，服务一旦创建，就不会经常变动。但是服务的实例可能会经常变动，比如 Deploy 滚动更新之类的。EDS，就是用于在 CDS 不变的前提下，动态更新每个 Cluster 后面可用的实例。 大部分情况下，如果要做 Envoy 相关的工作，LDS/CDS/RDS/EDS 四种 DS 协议是必须了解的。 目前 Envoy 支持 gRPC 服务、Restful 接口以及磁盘文件三种不同类型的 xDS 数据源。以最经典的gRPC 为例，Envoy 会定义了一个流式的 gPRC Service，xDS 服务提供方只需要实现对应的 gRPC Service。然后在需要配置更新时，向 gRPC 流推送配置数据即可。Envoy 侧会接受配置数据，然后加载更新。 Restful xDS 就是由 Envoy 开放一个 Post 接口由配置的提供方去调用；磁盘文件 xDS 则是由 Envoy 去 Watch 指定文件的变化并在文件更新时更新配置。这两种 xDS 配置方法在实践当中都很少使用。 本质上 xDS 协议并不复杂，真正困难的在于如何把各种资源抽象出来并通过一种通用的协议来封装和传输、如何管理数据的版本以及保证更新过程流量的平稳。这些都涉及到一些实现的细节，就不再赘述了。 下图是一个相对实际的例子。当使用 Isito Pilot 作为 xDS Server 时，如何利用 xDS 来动态更新 Envoy 配置。一般情况下，是用户修改了 K8s 集群中的一些 CRD 资源亦或者注册中心有配置更新才会触发配置更新；之后 Pilot watch 到相关变化变更将相关变化抽象成各种 Envoy 中对应的资源，如 Listener、Cluster，然后通过各个 xDS 将对应资源推送到 Envoy。 Envoy 可扩展性 可扩展性是 Envoy 最关键的特性之一。Envoy 提供了一个 Filter 机制，让开发者可以完全无侵入的在各个层级定义和扩展 Envoy 功能。 前文介绍过，下游的数据从 Listener 流入到 Envoy。到了 Envoy 之后，Listener 就会把数据交给 Filter 来处理。但是 Filter 也是很多种的。 最底层的，是 Listener Filter，在下游和 Envoy 连接建立之后，首先被执行处理，它主要用于获取连接协议相关信息并进行插件链匹配。在网关或者代理的场景中很少使用，因为端口和协议相对固定。可以缺省。 在 Listener Filter 处理完之后，就会由 Network Filter。Network Filter 是 Envoy 管理各种协议和流量的基础。它负责对二进制数据进行解析并根据解析后的数据以及路由配置选择合适的 Cluster 把请求发出去。Network Filter 一般是不可缺省的，比如 HTTP 协议就得要配置 HCM 才能处理，而 Dubbo 协议也只有 Dubbo proxy 可以处理。 最上层的就是 L7 Filter。Network Filter 把二进制数据解析完成之后，如果想要更复杂的流量治理怎么办？最简单是全部都让 Network Filter 自己做。但是这样显然不够灵活，扩展性不好。所以在这里又可以抽象出一层 L7 Filter，负责治理解析后的具体数据。L7 Filter 一般和 HTTP Filter 划等号，因为目前只有 HTTP 协议对应的 L7 Filter 做的最完善、功能支持最丰富。 总结来说就是：Listener Filter 处理连接、Network Filter 处理二进制数据、L7 Filter 处理解析后结构化数据。 其中，L7 Filter 都是作为某个 Network Filter 的子 Filter 存在。这也很好理解，L7 处理解析后结构化数据，总要有一个 Network Filter 来解析二进制数据然后把解析后数据传递给它。 下图是一个相对完整的 Envoy 插件链执行流程图。原本应该还有所谓的 encoder/decoder 层级的，它们才是真正负责协议解析的组件。但是在目前实现当中，encoder/decoder 一般都是嵌在 Network Filter 中作为某个 Network Filter 的一部分，所以这里干脆简化掉了。 Envoy 可扩展性方面还有最后一个问题：功能更新和升级带来的运维成本。Envoy 是使用 C++ 来实现的。每实现一个新的功能性的 Filter，无论是复杂的 L4 Network Filter，还是相对简单的 L7 HTTP Filter，都需要重新构建整个 Envoy。相比于配置的动态化，Envoy 功能上似乎没那么动态化。 为了解决这个问题，Envoy 社区提出了基于 WASM 的 Filter 扩展机制。WASM 是一种前端的技术，最初设计是用于加速 JS 脚本以及将 C++ 等语言带到 WEB 上。Envoy 内置了 WASM 虚拟机，开发者可以将自己的功能扩展使用如 C++、Go 、AS 等各种语言开发，然后编译成 WASM 字节码文件。之后，Envoy 动态的加载文件就可以实现功能的增强。 此外，Envoy 社区也提供了 Lua Filter，可以让 Envoy 通过动态的下发一个 Lua 脚本来实现功能扩展。举例来说，用户可以使用 Lua 来编写一段 Lua 逻辑，只要实现 envoy_on_request 和 envoy_on_response 两个函数，然后将 Lua 脚本通过 xDS 动态的下发给 Envoy，Envoy 在处理请求的过程中，就会执行对应的 Lua 脚本代码。 目前 WASM 还没有完全落地，但是 Gloo、Istio 社区都在力推，前景美好。而 Lua 相对来说，比较成熟，但是社区提供的功能较弱一些。所以轻舟团队也做了一些优化工作，后文当中也会讲到。 Envoy 可观察性 接下来需要介绍的 Envoy 关键特性是可观察性。可观察性其实是一个很大的主题，完全可以做一个单独的文章分享。但是这里就是主要介绍一下可观察性的概念以及 Envoy 的一些优势。希望有机会在其他文章中在对 Envoy 可观察性做更详细的介绍。 可观察性是指在软件程序运行过程中，获取软件程序内部状态或者发生的一个能力。如果程序执行起来之后，开发者和运维人员就对内部状态一无所知，那很多问题就根本无法定位。 按照侧重面的不同，Envoy 的可观察性主要依靠三个部分构成。分别是日志、指标、以及分布式追踪。 日志：日志是对 Envoy 内部事件（或者直白的说，就是一个请求处理过程）的详细记录。Envoy 提供了非常丰富的日志字段，而且可以灵活的配置。同时还提供了类似 L7 Filter 一样的 Access Log Filter 来自定义日志过滤和筛选、二次修改之类的能力。 指标监控数据是对 Envoy 内部事件的数值化统计，本质上，指标数据就是一个个计数器。记录诸如请求次数、正确请求次数、错误请求次数之类的统计数据。指标监控数据要结合 Prometheus 之类的时间数据库来使用，用于观察 Envoy 的整体流量趋势。 Envoy 提供了非常丰富的指标数据。包括 xDS 指标监控， Cluster 维度的请求统计、连接统计，Listener 维度请求统计、连接统计。而且对于连接上发生的一些事件也会进行记录。比如连接断开了，根据断开的原因不同，会提供不同的计数，这样就非常方便开发者去定位问题。 指标监控和日志描述的都是单个实例内部的状态。但是在微服务架构中，往往不同的服务实例内事件是有关联的。","date":"262611-08-2651","objectID":"/%E7%BD%91%E6%98%93%E8%BD%BB%E8%88%9F/:0:2","tags":["istio"],"title":"istio - 网易轻舟（转载）","uri":"/%E7%BD%91%E6%98%93%E8%BD%BB%E8%88%9F/"},{"categories":["istio"],"content":"Envoy 落地实践 最后分享从 Envoy 数据面的视角看 Envoy 如何在网易轻舟落地。通过这一部分的介绍，大家可以更好地理解 Envoy 的各个特性是如何在实践之中发挥效用的。 最简单的用法是把 Envoy 当作一个纯粹的七层网络代理，没有任何其他的依赖，只有单独的一个 Envoy，就像使用单体的 Nginx 一样。可以使用静态配置，或者基于磁盘文件作为数据源的 xDS 协议。 对于广大的个人开发者以及个人网站维护者而言，如果需要一个七层代理，不妨尝试一下 Envoy，立刻就能拥有一个高性能的网络代理，而且具备丰富的观察手段。同时对于希望深入学习 Envoy 的开发者来说，这种方法也是上手 Envoy 最快的方式。 此外，也可以将 Envoy 应用于 API 网关之中。使用 Envoy 作为核心数据面，结合控制台、日志分析系统、指标监控报警系统、APM 链路跟踪系统、外部注册中心等等来构建一个功能完善、可观察、易扩展的 API 网关。也可以将 Envoy 作为 Service Mesh 服务网格中数据面，接管微服务集群东西向流量，构建全新的微服务架构。 注：有兴趣的同学也可以了解一下 Envoy Mobile 项目，那是另一个更加有趣的场景。 接下来要着重介绍的，就是Envoy 在网关和网格两种场景下的落地和实践。 整体架构 Envoy 在网易轻舟实践中的整体架构如下。当然，图中简化了很多细节，只留下了核心的一些模块。 Envoy 会作为 API 网关以及服务网格数据面，承接整个微服务集群中东西向和南北向的流量，实现微服务集群的全流量接管。 在 Envoy 之上，就是所谓的控制面。前面也说了，Envoy 使用 xDS 协议来实现配置的动态化加载，这些配置本身就是各种各样的流量治理策略，比如如何路由请求、是否限流、是否熔断、如何健康检查等等，它们依赖一个 xDS 服务提供方来向 Envoy 推送。控制面主要的工作就是作为 Envoy 配置提供方来负责整个集群中的 Envoy 配置，并以此实现集群中流量的治理和控制。 在网易轻舟，控制面是基于开源的 Istio Pilot 做的增强和扩展。Istio 应该是目前开源社区发展最好的服务网格软件。轻舟服务网格控制面使用 Isito 是一个自然而然的选择。而网关控制面，则是直接复用的网格控制面中的核心组件 Pilot。这样，在轻舟 API 网关以及轻舟服务网格之中，控制面和数据面的组件都是完全共享的，实现了基础设施技术栈的统一。不用单独维护工程和代码，功能也可以通用，只是部署模式和配置分发略有差异。 而在 Istio Pilot 之上，则是轻舟单独抽象的一层 API Plane。它是对 Pilot 以及集群基础设施如 K8s API 的一个封装和聚合，屏蔽下层的细节。 在 API Plane 之上，就是轻舟控制台，用户操作和管理集群的入口。通过对接 API Plane，也可以实现自定义的控制台。 此外，Prometheus 组件会负责自动发现并采集集群中各个组件（尤其是 Envoy）的指标监控数据。轻舟控制台会把一些关键指标数据进行可视化展示。 APM 系统会实现全链路的跟踪和集群服务拓扑分析。轻舟原本是使用私有协议，目前将要切换到 SkyWalking，当然，如果业务方或者客户有需求，ZipKin 之类的也可以支持。 最后，Envoy 日志会被采集到 ELK 日志分析系统之中，轻舟以此来实现各种审计功能。 基本上，轻舟 API 网关和轻舟服务网格大体架构就是如此。接下来，将分别详细介绍一下轻舟 API 网关以及轻舟服务网格。 轻舟 API 网关 API 网关是整个微服务集群的流量入口，主要用于解决微服务架构中服务暴露的问题。相比于数据面纯粹的代理，API 网关更强调流量的治理。 在 API 网关之中，个人认为，从一个数据面开发的角度看，最重要的是五个点，分别是：性能、可观察性、治理能力、稳定性以及控制台。 性能其实是很模糊的一个点。因为性能的影响因素太多了，网络、CPU、业务复杂程度。一般来说能够满足业务或者客户需求的性能就是好性能。当然，考虑到需求会变，性能留下的余地当然是越大越好。 第二个，可观察性。作为流量的入口，在出现问题或者故障时，毋庸置疑的，网关必然会成为第一个怀疑对象，同时也往往是问题排查的入口。所以网关应该具备： 所有此类能力都依赖可观察性来保障，详细的日志、丰富的监控、及时的报警、准确的链路分析，这些才能撑起一个 API 网关所必须的可观察性。 快速自证清白的能力； 帮助业务方或者客户快速定位到问题的能力； 万一网关真的出现故障第一时间明确根因并做处理以减少损失的能力； 提前预见风险把问题扼杀在摇篮的能力。 第三个是治理能力，包括鉴权、限流、熔断等等对集群入口请求进行检查、过滤、筛选、修改的能力。而且由于业务方和客户的需求千变万化，所以这些能力必须很容易扩展甚至可以让客户自定义。 第四个是稳定性，这一方面非常重要，非常非常重要。网关出问题波及的是整个微服务集群。但是在这一方面实在没有花巧可言，它更多是依赖流程和规范的来保障。如果说要有一个判断标准的话，就是必须具备实际落地案例并经过大规模流量验证，那才可以说，它大致是稳定的网关。 最后是控制台，它是管理人员操作和使用的入口。上述的四项都是一个优秀 API 网关必须具备的基础，而控制台的易用程度则直接决定网关本身的易用性。 比上文概览图详细一些的轻舟 API 网关架构简图如下所示。在控制面：控制台作为操作的入口，通过 API Plane 操作 K8s 或者 Pilot。Pilot 发现服务和配置并抽象成 xDS 协议约定的数据推动给 Envoy。在数据面：Envoy 接收来自控制面的配置数据并利用可扩展的各种 Filter 完成流量的治理。这里不过多展开，接下来分别介绍轻舟 API 网关对于前述五个要点的实践。 性能 在性能方面，Envoy 本身就是保障，在都使用最简配置的情况下，它的性能与 Nginx 相差仿佛，比 HA Proxy 要稍差一些。根据实际测试结果，一个 8C8G 的轻舟 API 网关，在容器网络下，可以达到 8w+ QPS，而物理网络下，可以达到 10w+ QPS。 在 Envoy API 网关性能保障方面，有三个小的 Tips 分享一下： 第一，Envoy 提供了一个 virtual host 的概念，对应着域名。每个 virtual host 下都会有一组对应域名下的路由。virtual host 的搜索复杂度是 O(1)，而路由的搜索和匹配是线性的。所以一定要合理的规划域名和路由，使用 virtual host 来把路由表隔离开来。 第二，Envoy 日志提供了 JSON Access log。使用 JSON log，可以方便后续的日志分析和解析，但是性能极差。Envoy 可以把日志输出到 gRPC 服务，没有验证过它在这种情况下的性能如何。但是JSON Access Log 写入磁盘文件时，由于会使用 ProtoBuf 自身提供的 JSON 序列化方法，会严重影响性能。 第三，对于核心的治理功能扩展，使用原生的 C++ 实现，并且使用 PPROF 功能来定位性能瓶颈做优化。这里是最需要开发去下功夫打磨的地方。性能，大部分时候都是需要一点点挤压出来的。 可观察性 在可观察性方面，轻舟 API 网关构建了完整的监控体系。由前文可知，Envoy 本身就提供了非常丰富的指标监控数据，以及灵活的日志系统，所以重点是如何把这些 Envoy 本身提供的能力利用起来的问题。 首先是指标监控，前面也提到过，轻舟使用的是 Prometheus 数据库。指标监控就是一个个计数器，而把这些计数器在时间轴上排列出来时，就可以形成线和图，用于观察整个网关的流量趋势。对于一些核心的数据，我们会对接到控制台。同时，也提供了 Grafana 来把 Prometheus 中的数据做可视化。此外，Prometheus 还负责提供根据指标数据监控告警的能力。比如说，一段时间内，4xx/5xx 的请求数暴涨，这个时候就立刻会把相关告警推送给对应的负责人。这些告警能力，也是通过控制台来对外暴露，让业务方或者客户配置。 其次是日志，Envoy JSON 日志存在性能问题，普通的文本日志解析起来又差一些。所以这里有一个取巧的办法，使用文字字符串拼接成 JSON 格式字符串。同时，使用日志提供的 Log Filter 对包含特殊字符串的字段做转义，这样就可以保证性能的同时，输出标准的 JSON 字符串了。 而分布式跟踪，Envoy 原生支持了 Zipkin、OpenTracing、LightStep 等多种分布式跟踪系统，可以直接对接。而我们自己开发了 SkyWalking 的 Tracing 支持，还在持续的优化之中，而且也准备贡献给社区。 治理能力 前文之中花了不小的篇幅来介绍 Envoy 的可扩展性，实际上都是为了介绍 API 网关的治理能力做铺垫。得益于 Envoy 强大的 L4/L7 Filter 扩展机制，开发者可以很轻松地在不侵入 Envoy 源码的前提下，扩展出各种流量治理能力。Envoy 提供了全局/本地限流、黑白名单、服务/路由熔断、动静态降级、流量染色等等流量治理功能，基本已经覆盖了 API 网关的绝大部分需求了。 而考虑到各个方业务的特殊性，也考虑到 Envoy C++ 扩展的开发门槛，以及运维部署的成本，所以轻舟网关也做了一些其他方面的工作。首先是 WASM 扩展，它肯定是社区的发展方向，而且兼具性能、安全、动态分发、多语言支持多种","date":"262611-08-2651","objectID":"/%E7%BD%91%E6%98%93%E8%BD%BB%E8%88%9F/:0:3","tags":["istio"],"title":"istio - 网易轻舟（转载）","uri":"/%E7%BD%91%E6%98%93%E8%BD%BB%E8%88%9F/"},{"categories":["istio"],"content":"That Is All 本文整体介绍了 Envoy 以及构筑在 Envoy 之上的网关网格技术。服务网格、API 网关本身都是非常宏大的主题，涉及知识面很广，其中的某一个小点都有很多可以挖掘的地方。限于作者目前的知识和理解，本文难免存在不够全面或错谬之处，敬请读者指正。 作者简介：王佰平，网易数帆轻舟事业部研发工程师，负责轻舟 Envoy 网关与轻舟Service Mesh 数据面开发、功能增强、性能优化等工作。对于 Envoy 数据面开发、增强、落地具有较为丰富的经验。 原文链接：https://mp.weixin.qq.com/s/zjIjGOSRk6nIiI8zWjNysQ ","date":"262611-08-2651","objectID":"/%E7%BD%91%E6%98%93%E8%BD%BB%E8%88%9F/:0:4","tags":["istio"],"title":"istio - 网易轻舟（转载）","uri":"/%E7%BD%91%E6%98%93%E8%BD%BB%E8%88%9F/"},{"categories":["istio"],"content":"istio 问题排查流程.","date":"262611-08-2651","objectID":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/","tags":["istio"],"title":"istio - 问题排查","uri":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["istio"],"content":"istio 问题排查 ⚠️ 用例部署的 yaml 会在文末给出 ","date":"262611-08-2651","objectID":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:0:0","tags":["istio"],"title":"istio - 问题排查","uri":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["istio"],"content":"前言 问题排查分类如下： istio CR 错误的配置如何排查 如何使用 istioctl 检测和防止错误的资源配置 如何使用 istioctl 检查 envoy 配置 如何理解 envoy 的日志以帮助了解envoy的行为 利用收集的遥测技术深入了解应用程序 istio可以有效地帮助你了解网络通讯，诸如：超时、重试和断路等服务弹性功能，以便应用程序能够自动响应网络问题，但是，如果 envoy本身的意外行为会发生什么呢？ 上图中展示了istio中路由请求参与的组件： istiod 确保数据面同步到所期望的状态 Ingress Gateway 允许流量进入集群的入口网关 istio-proxy提供访问控制并处理从 downstream到 Application的流量 Application 服务于请求的应用程序本身。应用程序可能会请求另一个服务，继续连接到另一个 UpStream服务 当检查 istio中问题的时候，可能会与上述组件中的任意组件有关。调试每个组件可能会花费大量的时间。因此，本文将讲述如何使用一些useful工具通过检查proxy及其相关config来排除一些问题与错误。 ","date":"262611-08-2651","objectID":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:1:0","tags":["istio"],"title":"istio - 问题排查","uri":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["istio"],"content":"最常见的问题：数据面 misconfigured istio 中 有许多的CR，诸如：VirtualService、DestinationRule 等等。这些资源会被转换成Envoy配置并应用于数据面中。如果在应用新资源后，数据面的行为和我们预期的不一致，最常见的问题就是我们可能错误地配置了数据面。 为了展示如何在 misconfigured数据面后解决故障，我们将设置如下：使用 Gateway资源允许流量通过istio入口网关和VirtualService 将 20% 请求路由到 subset catalog vesion-v1 其余 80% 的请求路由到subset catalog version-v2，如下图所示。 $ kubectl apply -f ./catalog.yaml # yaml 放在文末给出 $ kubectl apply -f ./catalog-deployment-v2.yaml # yaml 放在文末给出 $ kubectl apply -f ./catalog-gateway.yaml # 创建 Gateway 来配置入口网关以允许 HTTP 流量 $ kubectl apply -f ./catalog-virtualservice-subsets-v1-v2.yaml # 创建 VirtualService 将流量路由到 catalog # catalog-gateway.yamlapiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:catalog-gatewaynamespace:istioinactionspec:selector:istio:ingressgatewayservers:- hosts:- \"catalog.istioinaction.io\"port:number:80name:httpprotocol:HTTP # catalog-virtualservice-subsets-v1-v2.yamlapiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:catalog-v1-v2namespace:istioinactionspec:hosts:- \"catalog.istioinaction.io\"gateways:- \"catalog-gateway\"http:- route:- destination:host:catalog.istioinaction.svc.cluster.localsubset:version-v1port:number:80weight:20- destination:host:catalog.istioinaction.svc.cluster.localsubset:version-v2port:number:80weight:80 但是，你会发现 subset 并没有定义，那么 subset是在哪里定义的呢？ 答案：在 DestinationRule中 因为上述并没有通过 DestinationRule定义 subset，入口网关就没有针对 version-v1和 version-v2子集的集群定义，因此按照上述的部署模式下，所有的请求都会失败。 # 创建好资源后，打开一个新的终端并执行一个持续运行的测试来生成“catalog”workload的流量。 $ ./bin/query-catalog.sh get-items-cont # curl -s -H \"Host: catalog.istioinaction.io\" #-w \"\\nStatus Code %{http_code}\" localhost/items Status Code 000 # infinite loop CTRL-C to terminate 在输出中，我们看到由于缺少子集，响应代码是“503 Service Unavailable”。 如何修复❓ 配置上 DestinationRule 即可 ---apiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:catalognamespace:istioinactionspec:host:catalog.istioinaction.svc.cluster.localsubsets:- name:version-v1labels:version:v1- name:version-v2labels:version:v2 ","date":"262611-08-2651","objectID":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:2:0","tags":["istio"],"title":"istio - 问题排查","uri":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["istio"],"content":"数据面和控制面配置同步问题 在与控制面发生同步之前，环境（services、endpoints、health）以及配置的更改不会立即反馈到 数据面。 控制面会将特定服务的每个 endpoint IP 发送到数据面（就是service每个 POD IP），如果这些 endpoints中任何一个变得不健康，kubernetes需要一段时间来识别并标记 pod unhealth。在某一时刻，控制面也会识别出这一点（控制面通过 api-server 实现 service 和 endpoint 的配置发现和更新），并将 endpoint从数据面中删除。由此让数据面和控制面保持一致。 ​ 对于工作负载和事件数量增加的较大集群，数据平面同步所需的时间会按比例增加。我之后会单独起一篇文章阐述如何“优化控制平面的性能”。 ","date":"262611-08-2651","objectID":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:3:0","tags":["istio"],"title":"istio - 问题排查","uri":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["istio"],"content":"istioctl proxy-status # 让我们检查数据平面是否与最新配置同步 $ istioctl proxy-status NAME CDS LDS RDS catalog.\u003c...\u003e.istioinaction SYNCED SYNCED SYNCED catalog.\u003c...\u003e.istioinaction SYNCED SYNCED SYNCED catalog.\u003c...\u003e.istioinaction SYNCED SYNCED SYNCED istio-egressgateway.\u003c...\u003e.istio-system SYNCED SYNCED NOT SENT istio-ingressgateway.\u003c...\u003e.istio-system SYNCED SYNCED SYNCED 输出列出了所有 workload 的每个 xDS API 的同步状态 。有如下状态： SYNCED : Envoy已经确认了控制平面发送的最后一个配置。 NOT SENT : 控制面没有发送任何东西给 Envoy。这通常是因为控制平面没有什么可发送的。例如RDS的istio-egressgateway。(出口网关不会在服务网格内路由请求，因此不需要路由配置) STALE : istiod 控制平面已经发送了一个更新，但它没有被确认。这表示以下情况之一: 控制平面过载; envoy 与控制平面之间缺乏或失去连接; 或者可能是Istio本身的一个bug。 ","date":"262611-08-2651","objectID":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:3:1","tags":["istio"],"title":"istio - 问题排查","uri":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["istio"],"content":"可视化问题排查 kiali 使用 Kiali 可以快速发现配置错误的服务。 $ istioctl dashboard kiali http://localhost:20001/kiali 这里我以外文原文的图做演示： 仪表板在istioinaction名称空间中显示一个警告。单击它会将您重定向到Istio Config视图，该视图列出了在所选名称空间中应用的所有Istio配置。 提示了 Virtualservice 有问题，点击后，所有Istio配置都被列出，并伴随着错误配置的通知，就像catalog-v1-v2 `` VirtualService 的情况一样。单击警告图标会将您重定向到virtualService的YAML视图，在该视图中，嵌入式编辑器突出显示了配置错误的部分。 将鼠标悬停在警告图标上，会显示警告消息“KIA1107子集未找到”。有关此警告的更多信息，请查看Kiali文档的Kiali validation (https://kiali.io/documentation/latest/validations/)页面。此页面提供了已识别错误的描述、严重性和解决方案。例如，下面是我们面临的“KIA1107” warning部分: 修正指向不存在的子集的路由。它可能是修复子集名称中的拼写错误，或者在DestinationRule 中定义缺失的子集。 该描述帮助我们识别并修复问题，因为它正确地指出了哪些配置错误。在这种情况下，子集是缺失的，我们应该创建一个DestinationRule 来定义它们。 ","date":"262611-08-2651","objectID":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:4:0","tags":["istio"],"title":"istio - 问题排查","uri":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["istio"],"content":"使用 istioctl 发现错误配置 最有用的两个指令： istioctl analyze istioctl describe ","date":"262611-08-2651","objectID":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:5:0","tags":["istio"],"title":"istio - 问题排查","uri":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["istio"],"content":"istioctl analyze (可以分析 yaml 和 namespace) istioctl analyze 命令是一个强大的分析Istio配置的诊断工具。它可以在已经遇到问题的集群上运行，甚至可以在将这些问题应用到集群之前验证配置，以便首先防止错误配置资源. “analyze”命令运行一组分析器，其中每个分析器都专门用于检测特定的一组问题。 让我们 analyze下 istioinaction namespace : $ istioctl analyze -n istioinaction Error [IST0101] (VirtualService catalog-v1-v2.istioinaction) Referenced host+subset in destinationrule not found: \"catalog.istioinaction.svc.cluster.local+version-v1\" Error [IST0101] (VirtualService catalog-v1-v2.istioinaction) Referenced host+subset in destinationrule not found: \"catalog.istioinaction.svc.cluster.local+version-v2\" Error: Analyzers found issues when analyzing namespace: istioinaction. See https://istio.io/v1.10/docs/reference/config/analysis for more information about causes and resolutions. 输出显示没有找到子集，除了错误消息Referenced host+subset in destinationrule not found外，它还给我们提供了错误代码“IST0101”，我们可以在Istio的文档 (https://istio.io/latest/docs/reference/config/analysis/)中找到关于这个问题的更多细节。 ","date":"262611-08-2651","objectID":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:5:1","tags":["istio"],"title":"istio - 问题排查","uri":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["istio"],"content":"istioctl describe (workload-specific 的配置分析) describe命令用于描述特定于工作负载的配置。它分析直接或间接影响一个工作负载的Istio配置并打印摘要。这个摘要回答了关于工作负载的问题，例如:它是服务网格的一部分吗?应用于它的虚拟服务和目标规则是什么?它是否需要相互验证的请求等。 选择任何catalog工作负载的名称，并执行下面的描述命令: $ istioctl x describe pod catalog-68666d4988-vqhmb Pod: catalog-68666d4988-q6w42 Pod Ports: 3000 (catalog), 15090 (istio-proxy) --------------- Service: catalog Port: http 80/HTTP targets pod port 3000 Exposed on Ingress Gateway http://13.91.21.16 VirtualService: catalog-v1-v2 WARNING: No destinations match pod subsets (checked 1 HTTP routes) Warning: Route to subset version-v1 but NO DESTINATION RULE defining subsets! Warning: Route to subset version-v2 but NO DESTINATION RULE defining subsets! description命令的输出显示警告消息Route to subset version-v1 but NO DESTINATION RULE defining subset。这意味着路由是为不存在的子集配置的。为了完整起见，如果正确配置了工作负载，将显示istioctl describe 输出如下： Pod: catalog-68666d4988-q6w42 Pod Ports: 3000 (catalog), 15090 (istio-proxy) --------------- Service: catalog Port: http 80/HTTP targets pod port 3000 DestinationRule: catalog for \"catalog.istioinaction.svc.cluster.local\" Matching subsets: version-v1 (Non-matching subsets version-v2) No Traffic Policy Exposed on Ingress Gateway http:// VirtualService: catalog-v1-v2 Weight 20% analyze和describe子命令都有助于识别配置中的常见错误，通常足以提出修复建议。对于这些命令无法解决的问题，或者对于如何解决这些问题没有提供足够的指导，您需要深入挖掘!这就是我们下一节要做的。 ","date":"262611-08-2651","objectID":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:5:2","tags":["istio"],"title":"istio - 问题排查","uri":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["istio"],"content":"通过拉取 envoy 配置查找错误配置 当前面提到的一些自动分析器达不到要求时，可以使用手动调查 envoy 配置的方式查找问题。 ","date":"262611-08-2651","objectID":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:6:0","tags":["istio"],"title":"istio - 问题排查","uri":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["istio"],"content":"envoy dashboard envoy 的 dashboard 公开envoy 配置和其他功能，以修改 envoy 的某些方面比如提高日志记录级别。 $ istioctl dashboard envoy deploy/catalog -n istioinaction http://localhost:15000 我们将使用config_dump在代理中打印当前加载的Envoy配置 ⚠️ config_dump 出来的结果 数据量非常大 以为，istioctl 提供了将输出过滤成更小块的工具，这有助于提高可读性和理解性。那就是 istioctl proxy-config ","date":"262611-08-2651","objectID":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:6:1","tags":["istio"],"title":"istio - 问题排查","uri":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["istio"],"content":"istioctl proxy-config istioctl proxy-config 命令使我们能够基于Envoy xDS api检索和过滤工作负载的代理配置，其中每个子命令都被适当命名为: cluster -获取集群配置 endpoint -获取端点配置 listener -检索侦听器配置 route -获取路由配置信息 secret -获取秘密配置 Envoy Listeners定义一个网络配置，例如IP Address和Port，它允许downstream流量进入envoy。为允许的连接创建HTTP Filter chain。链中最重要的过滤器是路由器过滤器，它执行高级路由任务。 Envoy Routes是一组将virtual hots匹配到集群的规则。路由按列出的顺序处理。第一个匹配的用于将流量路由到工作负载集群。路由可以静态配置，但Istio使用路由发现服务动态配置。 Envoy Clusters是一组集群，其中每个集群都有一组指向类似工作负载的endpoints。subsets用于在集群中进一步划分工作负载，从而支持细粒度的流量管理。 Envoy Endpoints是一组endpoints，它们代表服务请求的工作负载的IP地址。 查询 envoy listener 配置 ​ 首先确保允许到达本地主机端口80上的入口网关的流量进入集群。如前所述，接收流量是envoy监听器(Envoy Listeners)的职责，它们在Istio中使用Gateway资源配置。让我们查询一下网关的监听器配置，并验证流量是否被80端口所允许: # 查询ingress pod的侦听器配置 $ istioctl proxy-config listeners deploy/istio-ingressgateway -n istio-system ADDRESS PORT MATCH DESTINATION 0.0.0.0 8080 ALL Route: http.80 # 在 8080 端口上配置了 listener，流量根据该侦听器名为“http.80”的路由进行路由 0.0.0.0 15021 ALL Inline Route: /healthz/ready* 0.0.0.0 15090 ALL Inline Route: /stats/prometheus* # Ports of the istio-ingressgateway service printed in yaml format $ kubectl -n istio-system get svc istio-ingressgateway -o yaml \\ | grep \"ports:\" -A 10 ports: - name: status-port nodePort: 30618 port: 15021 protocol: TCP targetPort: 15021 - name: http2 nodePort: 32589 port: 80 # Traffic in port 80 targets pods on port 8080 protocol: TCP | targetPort: 8080 \u003c--+ 因此，我们验证了流量到达端口 8080 并且存在一个侦听器以允许它进入入口网关。 此外，我们看到此侦听器的路由是由路由“http.80”完成的，这是我们的下一个检查点。 查询 envoy Route 配置 envoy Route 配置定义了一组规则，这些规则决定将通信路由到哪个集群。Istio使用VirtualService 资源配置envoy Routes，同时，clusters要么自动发现，要么使用DestinationRule 资源定义。 # 要找出“http.80”路由的流量路由到哪些集群，让我们查询其配置： $ istioctl pc routes deploy/istio-ingressgateway -n istio-system --name http.80 NOTE: This output only contains routes loaded via RDS. NAME DOMAINS MATCH VIRTUAL SERVICE http.80 * /productpage bookinfo.default http.80 * /static* bookinfo.default http.80 * /login bookinfo.default http.80 * /logout bookinfo.default http.80 * /api/v1/products* bookinfo.default http.80 bookinfo.10.20.144.36.nip.io /* productpage.bookinfo http.80 catalog.istioinaction.io /* catalog.istioinaction # 其 URL 与路径前缀 `/*` 匹配的 host `catalog.istioinaction.io` 的流量被路由到 catalog `VirtualService`，该 catalog 服务 位于 `istioinaction` 命名空间中的 `catalog` service 中。 当路由配置以 JSON 格式打印时，会显示有关 catalog.istioinaction VirtualService 后面的集群的详细信息： # 打印路由配置 $ istioctl pc routes deploy/istio-ingressgateway -n istio-system \\ --name http.80 -o json \u003committed\u003e \"routes\": [ { \"match\": { \"prefix\": \"/\" # Route rule that has to match }, \"route\": { \"weightedClusters\": { \"clusters\": [ # Clusters to which traffic is routed when the rule is matched { \"name\": \"outbound|80|version-v2|catalog.istioinaction.svc.cluster.local\", \"weight\": 80 }, { \"name\": \"outbound|80|version-v1|catalog.istioinaction.svc.cluster.local\", \"weight\": 20 } ] }, \u003committed\u003e } 由输出可知，路由匹配时，两个clusters接收流量，分别为: outbound|80|version-v2|catalog.istioinaction.svc.cluster.local outbound|80|version-v1|catalog.istioinaction.svc.cluster.local 让我们研究每个管道分离部分的含义，并研究如何将工作负载作为成员分配给这些集群。 查询 envoy Clutsers 配置 Envoy Clusters配置定义了请求可以路由到的后端服务。 跨实例或端点的集群负载平衡。 这些端点（通常是 IP 地址）代表为最终用户流量提供服务的各个工作负载实例。 使用 istioctl 我们可以查询入口网关知道的集群，但是，有很多集群，因为每个后端可路由服务都配置了一个集群。 我们可以使用以下 istioctl proxy-config clusters 配合 flags: direction、fqdn、port 和 subset打印我们感兴趣的集群. 所有flags的信息都包含在我们在前面部分检索到的集群名称中： 让我们查询其中一个集群，例如配置了入口网关的 version-v1子集的集群。 我们可以通过在查询中指定所有集群属性。 $ istioctl proxy-config clusters \\ deploy/istio-ingressgateway.istio-system \\ --fqdn catalog.istioinaction.svc.cluster.local \\ --port 80 \\ --subset version-v1 SERVICE FQDN PORT SUBSET DIRECTION TYPE DESTINATION RULE 会发现，子集version-v1没有集群! 如果没有这些子集的集群，请求将失败，因为 virtual service 路由到了不存在的集群。 显然，这是一个配置错误的情况，我们可以通过创建一个定义这些子集的集群的DestinationRule来解决这个问题。 # catalog-destinationrule-v1-v2.yaml---apiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:catalognamespace:istioinactionspec:host:catalog.istioinaction.svc.cluster.localsubsets:- name:version-v1labels:version:v1- name:version-v2labels:version:v2 但是在将它应用到集群之前，让我们使用 istioctl","date":"262611-08-2651","objectID":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:6:2","tags":["istio"],"title":"istio - 问题排查","uri":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["istio"],"content":"使用日志查找问题 对于基于微服务的应用程序，服务代理生成的日志和指标有助于解决许多问题，如发现导致性能瓶颈的服务、识别经常失败的端点、检测性能下降等。我们将使用Envoy访问日志和度量来排除应用程序弹性问题。 # 首先，更新我们的服务，以解决我们可以解决的问题。 # 设置超时的间歇慢工作负载，目录工作负载可以配置为间歇性地返回慢响应，使用以下脚本: $ ./query-catalog.sh delayed-responses blowups=[object Object] ------------------------------------------------------------------------------------------------------ # 现在让我们继续并将catalog-v1-v2 VirtualService配置为当请求需要超过半秒才能得到服务时超时: $ kubectl patch vs catalog-v1-v2 -n istioinaction --type json \\ -p '[{\"op\": \"add\", \"path\": \"/spec/http/0/timeout\", \"value\": \"0.5s\"}]' # 配置的可视化结果如下所示： 让我们在一个单独的终端中生成到“目录”工作负载的连续通信。这将产生我们在接下来的章节中所需要的日志和遥测技术。 $ ./bin/query-catalog.sh get-items-cont 在连续触发的请求中，我们看到一些请求被路由到较慢的工作负载，结果那些请求以超时结束。输出如下: upstream request timeout Status Code 504 状态码“504网关超时”是我们可以用来查询Envoy访问日志的一条信息。 Envoy访问日志记录了Envoy代理处理的所有请求，这有助于调试和排除故障。默认情况下，Istio将代理配置为使用TEXT格式的日志。这是简明但难以阅读，如下所示: # 只查询和打印状态码为504的日志 $ kubectl -n istio-system logs deploy/istio-ingressgateway \\ | grep 504 # output is truncated to a single failing request [2020-08-22T16:20:20.049Z] \"GET /items HTTP/1.1\" 504 UT \"-\" \"-\" 0 24 501 - \"192.168.65.3\" \"curl/7.64.1\" \"6f780bed-9996-9c95-a899-a5e293cd9fe4\" \"catalog.istioinaction.io\" \"10.1.0.68:3000\" outbound|80|version-v2|catalog.istioinaction.svc.cluster.local 10.1.0.69:34488 10.1.0.69:8080 192.168.65.3:55962 - - 为了易于读取日志，将服务代理配置为使用JSON格式： # 服务代理访问日志是可配置的，默认情况下，只有Istio的“demo”安装配置文件将访问日志打印到标准输出。如果您正在使用任何其他配置文件，那么您需要设置以下属性' meshConfig.accessLogFile=\"/dev/stdout\" '在Istio安装过程中。如下所示: $ istioctl install --set meshConfig.accessLogFile=\"/dev/stdout\" --set meshConfig.accessLogEncoding=\"JSON\" 上述的部署方式是对整个网格日志做了修改，如果要对特定的workload启动访问日志记录，可以使用EnvoyFilter： apiVersion:networking.istio.io/v1alpha3kind:EnvoyFiltermetadata:name:webapp-access-loggingnamespace:istioinactionspec:workloadSelector:labels:app:webapp # 这个配置允许对“webapp”工作负载进行访问日志记录configPatches:- applyTo:NETWORK_FILTERmatch:context:ANYlistener:filterChain:filter:name:\"envoy.filters.network.http_connection_manager\"patch:operation:MERGEvalue:typed_config:\"@type\": \"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\"access_log:- name:envoy.access_loggers.filetyped_config:\"@type\": \"type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\"path:/dev/stdoutformat:\"[%START_TIME%] \\\"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\\\" %RESPONSE_CODE% %RESPONSE_FLAGS% \\\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\\\" %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \\\"%REQ(X-FORWARDED-FOR)%\\\" \\\"%REQ(USER-AGENT)%\\\" \\\"%REQ(X-REQUEST-ID)%\\\" \\\"%REQ(:AUTHORITY)%\\\" \\\"%UPSTREAM_HOST%\\\" %UPSTREAM_CLUSTER% %UPSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_REMOTE_ADDRESS% %REQUESTED_SERVER_NAME% %ROUTE_NAME%\\n\" Istio安装更新后，我们可以再次查询访问日志，这一次考虑到它是JSON，我们可以将输出管道到jq，以提高可读性。 # 查询最近的日志并使用jq格式化它 $ kubectl -n istio-system logs deploy/istio-ingressgateway \\ | grep 504 | tail -n 1 | jq { \"user_agent\":\"curl/7.64.1\", \"Response_code\":\"504\", \"response_flags\":\"UT\", # Envoy response flag 'UT'表示“上游请求超时” \"start_time\":\"2020-08-22T16:35:27.125Z\", \"method\":\"GET\", \"request_id\":\"e65a3ea0-60dd-9f9c-8ef5-42611138ba07\", \"upstream_host\":\"10.1.0.68:3000\", # Upstream host receiving the request 表示处理请求的工作负载的实际IP地址 \"x_forwarded_for\":\"192.168.65.3\", \"requested_server_name\":\"-\", \"bytes_received\":\"0\", \"istio_policy_status\":\"-\", \"bytes_sent\":\"24\", \"upstream_cluster\": \"outbound|80|version-v2|catalog.istioinaction.svc.cluster.local\", \"downstream_remote_address\":\"192.168.65.3:41260\", \"authority\":\"catalog.istioinaction.io\", \"path\":\"/items\", \"protocol\":\"HTTP/1.1\", \"upstream_service_time\":\"-\", \"upstream_local_address\":\"10.1.0.69:48016\", \"duration\":\"503\", # Exceeding duration of 500 milliseconds \"upstream_transport_failure_reason\":\"-\", \"route_name\":\"-\", \"downstream_local_address\":\"10.1.0.69:8080\" } 将response_flagsUT与此请求关联是很重要的，因为它使我们能够区分超时决策是由代理而不是应用程序做出的。您将经常面对的一些响应标志是: UH:没有正常的上游，即集群没有工作负载 NR:未配置路由 UC:上行连接终止 DC:下行连接端子 The entire list can be found in the Envoy documentat","date":"262611-08-2651","objectID":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:7:0","tags":["istio"],"title":"istio - 问题排查","uri":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["istio"],"content":"通过抓包查找问题 抓包用到两个工具： ksniff 一个kubectl插件，可以通过tcpdump抓取pod的网络流量，并将其重定向到Wireshark** Wireshark 网络报文分析工具 ","date":"262611-08-2651","objectID":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:8:0","tags":["istio"],"title":"istio - 问题排查","uri":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["istio"],"content":"Installing Krew, Ksniff, and Wireshark 要安装ksniff , 我们需要Krew的kubectl插件管理器。Krew的安装过程在其官方文件documentation (https://krew.sigs.k8s.io/docs/user-guide/setup/install/)中进行了记录。 安装完Krew后，再安装Ksniff ： $ kubectl krew install sniff Wireshark安装直接参考网上别的教程。 ","date":"262611-08-2651","objectID":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:8:1","tags":["istio"],"title":"istio - 问题排查","uri":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["istio"],"content":"Inspecting network traffic # 捕获slow pod的本地主机网络接口上的流量 $ kubectl sniff -n istioinaction $SLOW_POD -i lo # 在一个成功的连接上，' ksniff'将使用tcpdump从本地主机网络接口捕获网络流量，并将输出重定向到本地的Wireshark实例以进行可视化。如果您仍然运行生成流量的脚本，那么在短时间内将捕获足够的流量。如果不是，那么在一个单独的终端窗口中执行该脚本。 $ ./query-catalog.sh get-items-cont 停止捕获额外的网络数据包 为了更好地了解，让我们只显示路径为' /items ‘的’ get ‘方法的HTTP协议数据包。这可以使用Wireshark显示过滤器，使用查询’ http contains “GET /items”，如下图所示。 ​ 这减少了输出到我们感兴趣的请求，我们可以得到更多关于TCP连接的开始和它被取消的时间的细节，通过跟踪它的TCP流。右键单击第一行，选择菜单项“Follow”，然后选择“TCP Stream”。这将打开“跟随TCP流窗口”，它显示了一个易于理解的TCP流格式。请随意关闭此窗口，因为在主Wireshark窗口中过滤的输出就足够了，如下图所示。 Point 1: The TCP three-way handshake was performed to set up a TCP connection. Indicated by the TCP flags [SYN], [SYN, ACK], and [ACK] Point 2: After the connection is set up we can see that the connection is reused for multiple requests from the client, and all of those are successfully served. Point 3: Another request comes in from the client, which is acknowledged by the server, but the response takes longer than half a second. This can be seen from the time difference from the packet no. 95 to the packet no. 102. Point 4: The client initiates a TCP Connection Termination by sending a FIN flag, due to the request taking too long, this is acknowledged by the server-side, and the connection is terminated. ","date":"262611-08-2651","objectID":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:8:2","tags":["istio"],"title":"istio - 问题排查","uri":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["istio"],"content":"总结 本章向您展示了如何排除数据平面故障，并让您练习使用调试工具，并告诉您如何: 使用istioctl命令了解服务网格和服务代理，例如: proxy-status 用于查看数据平面的同步状态 analyze 用于分析服务网格配置 description 用于获取摘要和验证服务代理配置 proxy-config 用于查询和修改业务代理配置 在应用到集群之前，使用istioctl analyze来验证配置 使用Kiali及其提供的验证功能来检测常见的配置错误 使用ksniff捕获受影响 pod 的网络流量 解释envoy logs的含义，以及如何使用istioctl提高日志级别 ","date":"262611-08-2651","objectID":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:9:0","tags":["istio"],"title":"istio - 问题排查","uri":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["istio"],"content":"附录 catalog.yaml apiVersion:v1kind:ServiceAccountmetadata:name:catalog---apiVersion:v1kind:Servicemetadata:labels:app:catalogname:catalogspec:ports:- name:httpport:80protocol:TCPtargetPort:3000selector:app:catalog---apiVersion:apps/v1kind:Deploymentmetadata:labels:app:catalogversion:v1name:catalogspec:replicas:1selector:matchLabels:app:catalogversion:v1template:metadata:labels:app:catalogversion:v1spec:serviceAccountName:catalogcontainers:- env:- name:KUBERNETES_NAMESPACEvalueFrom:fieldRef:fieldPath:metadata.namespaceimage:istioinaction/catalog:latestimagePullPolicy:IfNotPresentname:catalogports:- containerPort:3000name:httpprotocol:TCPsecurityContext:privileged:false Catalog-deployment-v2.yaml apiVersion:apps/v1kind:Deploymentmetadata:labels:app:catalogversion:v2name:catalog-v2spec:replicas:2selector:matchLabels:app:catalogversion:v2template:metadata:labels:app:catalogversion:v2# annotations:# sidecar.istio.io/extraStatTags: destination_ip,source_ip,source_port# readiness.status.sidecar.istio.io/applicationPorts: \"\"spec:serviceAccountName:catalogcontainers:- env:- name:KUBERNETES_NAMESPACEvalueFrom:fieldRef:fieldPath:metadata.namespaceimage:istioinaction/catalog:latencyimagePullPolicy:IfNotPresentname:catalogports:- containerPort:3000name:httpprotocol:TCPsecurityContext:privileged:false query-catalog.sh #!/usr/bin/env bash help () { cat \u003c\u003cEOF This script is a collection of request that are used in the book. Below is a list of arguments and the requests that those make: - 'get-items' Continuous requests that print the response status code - 'random-agent' Adds either chrome or firefox in the request header. Usage: ./bin/query-catalog.sh status-code EOF exit 1 } TYPE=$1 case ${TYPE} in get-items-cont) echo \"#\" curl -s -H \\\"Host: catalog.istioinaction.io\\\" -w \\\"\\\\nStatus Code %{http_code}\\\" localhost/items echo sleep 2 while : do curl -s -H \"Host: catalog.istioinaction.io\" -w \"\\nStatus Code %{http_code}\\n\\n\" localhost/items sleep .5 done ;; get-items) echo \"#\" curl -s -H \\\"Host: catalog.istioinaction.io\\\" -w \\\"\\\\nStatus Code %{http_code}\\\" localhost/items echo curl -s -H \"Host: catalog.istioinaction.io\" -w \"\\nStatus Code %{http_code}\" localhost/items ;; random-agent) echo \"== REQUEST EXECUTED ==\" echo curl -s -H \"Host: catalog.istioinaction.io\" -H \"User-Agent: RANDOM_AGENT\" -w \"\\nStatus Code %{http_code}\\n\\n\" localhost/items echo while : do useragents=(chrome firefox) agent=${useragents[ ($RANDOM% 2) ]} curl -s -H \"Host: catalog.istioinaction.io\" -H \"User-Agent: $agent\" -w \"\\nStatus Code %{http_code}\\n\\n\" localhost/items sleep .5 done ;; delayed-responses) CATALOG_POD=$(kubectl get pods -l version=v2 -n istioinaction -o jsonpath={.items..metadata.name} | cut -d ' ' -f1) if [ -z \"$CATALOG_POD\" ]; then echo \"No pods found with the following query:\" echo \"-\u003e kubectl get pods -l version=v2 -n istioinaction\" exit 1 fi kubectl -n istioinaction exec -c catalog $CATALOG_POD \\ -- curl -s -X POST -H \"Content-Type: application/json\" \\ -d '{\"active\": true, \"type\": \"latency\", \"latencyMs\": 1000, \"volatile\": true}' \\ localhost:3000/blowup ;; *) help ;; esac ","date":"262611-08-2651","objectID":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:10:0","tags":["istio"],"title":"istio - 问题排查","uri":"/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["calico"],"content":"calico 网络是怎么流通的，抓包如何，一点点带你走进 calico.","date":"212111-08-2151","objectID":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/","tags":["kubernetes","calico"],"title":"calico - 网络流通以及抓包实战","uri":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/"},{"categories":["calico"],"content":"前言 本文主要分析k8s中网络组件calico的 IPIP网络模式。旨在理解IPIP网络模式下产生的calixxxx，tunl0等设备以及跨节点网络通信方式。可能看着有点枯燥，但是请花几分钟时间坚持看完，如果看到后面忘了前面，请反复看两遍，这几分钟时间一定你会花的很值。 ","date":"212111-08-2151","objectID":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/:0:1","tags":["kubernetes","calico"],"title":"calico - 网络流通以及抓包实战","uri":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/"},{"categories":["calico"],"content":"一、calico介绍 Calico是Kubernetes生态系统中另一种流行的网络选择。虽然Flannel被公认为是最简单的选择，但Calico以其性能、灵活性而闻名。Calico的功能更为全面，不仅提供主机和pod之间的网络连接，还涉及网络安全和管理。Calico CNI插件在CNI框架内封装了Calico的功能。 Calico是一个基于BGP的纯三层的网络方案，与OpenStack、Kubernetes、AWS、GCE等云平台都能够良好地集成。Calico在每个计算节点(every node)都利用Linux Kernel实现了一个高效的虚拟路由器vRouter来负责数据转发。每个vRouter都通过BGP协议把在本节点上运行的容器的路由信息向整个Calico网络广播，并自动设置到达其他节点的路由转发规则。Calico保证所有容器之间的数据流量都是通过IP路由的方式完成互联互通的。Calico节点组网时可以直接利用数据中心的网络结构(L2或者L3)，不需要额外的NAT、隧道或者Overlay Network，没有额外的封包解包，能够节约CPU运算，提高网络效率。 此外，Calico基于iptables还提供了丰富的网络策略，实现了Kubernetes的Network Policy策略，提供容器间网络可达性限制的功能。 **calico官网：**https://www.projectcalico.org/ ","date":"212111-08-2151","objectID":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/:0:2","tags":["kubernetes","calico"],"title":"calico - 网络流通以及抓包实战","uri":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/"},{"categories":["calico"],"content":"二、calico架构及核心组件 架构图如下： calico核心组件： Felix (菲利克斯)：运行在每个需要运行workload的节点上的agent进程。主要负责配置路由及 ACLs(访问控制列表) 等信息来确保 endpoint 的连通状态，保证跨主机容器的网络互通; etcd：强一致性、高可用的键值存储，持久存储calico数据的存储管理系统。主要负责网络元数据一致性，确保Calico网络状态的准确性; BGP Client(BIRD)：读取Felix设置的内核路由状态，在数据中心分发状态。 BGP Route Reflector(BIRD)：BGP路由反射器，在较大规模部署时使用。如果仅使用BGP Client形成mesh全网互联就会导致规模限制，因为所有BGP client节点之间两两互联，需要建立N^2个连接，拓扑也会变得复杂。因此使用reflector来负责client之间的连接，防止节点两两相连。 ","date":"212111-08-2151","objectID":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/:0:3","tags":["kubernetes","calico"],"title":"calico - 网络流通以及抓包实战","uri":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/"},{"categories":["calico"],"content":"三、calico工作原理 ​ Calico把每个操作系统的协议栈认为是一个路由器，然后把所有的容器认为是连在这个路由器上的网络终端，在路由器之间跑标准的路由协议——BGP的协议，然后让它们自己去学习这个网络拓扑该如何转发。所以Calico方案其实是一个纯三层的方案，也就是说让每台机器的协议栈的三层去确保两个容器，跨主机容器之间的三层连通性。 ","date":"212111-08-2151","objectID":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/:0:4","tags":["kubernetes","calico"],"title":"calico - 网络流通以及抓包实战","uri":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/"},{"categories":["calico"],"content":"四、calico的两种网络方式 ","date":"212111-08-2151","objectID":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/:0:5","tags":["kubernetes","calico"],"title":"calico - 网络流通以及抓包实战","uri":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/"},{"categories":["calico"],"content":"1)IPIP ​ 把 IP 层封装到 IP 层的一个 tunnel。它的作用其实基本上就相当于一个基于IP层的网桥!一般来说，普通的网桥是基于mac层的，根本不需 IP，而这个 ipip 则是通过两端的路由做一个 tunnel，把两个本来不通的网络通过点对点连接起来。ipip 的源代码在内核 net/ipv4/ipip.c 中可以找到。 ","date":"212111-08-2151","objectID":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/:0:6","tags":["kubernetes","calico"],"title":"calico - 网络流通以及抓包实战","uri":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/"},{"categories":["calico"],"content":"2)BGP 边界网关协议(Border Gateway Protocol, BGP)是互联网上一个核心的去中心化自治路由协议。它通过维护IP路由表或‘前缀’表来实现自治系统(AS)之间的可达性，属于矢量路由协议。BGP不使用传统的内部网关协议(IGP)的指标，而使用基于路径、网络策略或规则集来决定路由。因此，它更适合被称为矢量性协议，而不是路由协议。 ","date":"212111-08-2151","objectID":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/:0:7","tags":["kubernetes","calico"],"title":"calico - 网络流通以及抓包实战","uri":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/"},{"categories":["calico"],"content":"五、IPIP网络模式分析 由于个人环境中使用的是IPIP模式，因此接下来这里分析一下这种模式。 # kubectl get po -o wide -n paas | grep hello demo-hello-perf-d84bffcb8-7fxqj 1/1 Running 0 9d 10.20.105.215 node2.perf \u003cnone\u003e \u003cnone\u003e demo-hello-sit-6d5c9f44bc-ncpql 1/1 Running 0 9d 10.20.42.31 node1.sit \u003cnone\u003e \u003cnone\u003e 进行ping测试 这里在demo-hello-perf这个pod中ping demo-hello-sit这个pod。 root@demo-hello-perf-d84bffcb8-7fxqj:/# ping 10.20.42.31 PING 10.20.42.31 (10.20.42.31) 56(84) bytes of data. 64 bytes from 10.20.42.31: icmp_seq=1 ttl=62 time=5.60 ms 64 bytes from 10.20.42.31: icmp_seq=2 ttl=62 time=1.66 ms 64 bytes from 10.20.42.31: icmp_seq=3 ttl=62 time=1.79 ms ^C --- 10.20.42.31 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 6ms rtt min/avg/max/mdev = 1.662/3.015/5.595/1.825 ms 进入pod demo-hello-perf中查看这个pod中的路由信息 root@demo-hello-perf-d84bffcb8-7fxqj:/# route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 169.254.1.1 0.0.0.0 UG 0 0 0 eth0 169.254.1.1 0.0.0.0 255.255.255.255 UH 0 0 0 eth0 根据路由信息，ping 10.20.42.31，会匹配到第一条。 **第一条路由的意思是：**去往任何网段的数据包都发往网关169.254.1.1，然后从eth0网卡发送出去。 demo-hello-perf所在的node node2.perf 宿主机上路由信息如下： # node2 # route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 172.16.36.1 0.0.0.0 UG 100 0 0 eth0 10.20.42.0 172.16.35.4 255.255.255.192 UG 0 0 0 tunl0 10.20.105.196 0.0.0.0 255.255.255.255 UH 0 0 0 cali4bb1efe70a2 169.254.169.254 172.16.36.2 255.255.255.255 UGH 100 0 0 eth0 172.16.36.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 可以看到一条Destination为 10.20.42.0的路由。 该路由的意思是：去往10.20.42.0/26的网段的数据包都发往网关172.16.35.4(其实 这个 ip 就是 对端 node 的 ip)。因为demo-hello-perf的pod在172.16.36.5上，demo-hello-sit的pod在172.16.35.4上。所以数据包就通过设备tunl0发往到node节点上。 demo-hello-sit所在的node node1.sit 宿主机上路由信息如下： # node1 # route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 172.16.35.1 0.0.0.0 UG 100 0 0 eth0 10.20.15.64 172.16.36.4 255.255.255.192 UG 0 0 0 tunl0 10.20.42.31 0.0.0.0 255.255.255.255 UH 0 0 0 cali04736ec14ce 10.20.105.192 172.16.36.5 255.255.255.192 UG 0 0 0 tunl0 当node节点网卡收到数据包之后，发现发往的目的ip为10.20.42.31，于是匹配到Destination为10.20.42.31的路由。 该路由的意思是：10.20.42.31是本机直连设备，去往设备的数据包发往cali04736ec14ce 为什么这么奇怪会有一个名为cali04736ec14ce的设备呢?这是个啥玩意儿呢? 其实这个设备就是veth pair的一端。在创建demo-hello-sit 时calico会给demo-hello-sit创建一个veth pair设备。一端是demo-hello-sit 的网卡，另一端就是我们看到的cali04736ec14ce 接着验证一下。我们进入demo-hello-sit 的pod，查看到 4 号设备后面的编号是：122964 root@demo-hello-sit--6d5c9f44bc-ncpql:/# ip a 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: tunl0@NONE: \u003cNOARP\u003e mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 4: eth0@if122964: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1380 qdisc noqueue state UP group default link/ether 9a:7d:b2:26:9b:17 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.20.42.31/32 brd 10.20.42.31 scope global eth0 # 这个 ip 就是 pod ip valid_lft forever preferred_lft forever 然后我们登录到demo-hello-sit这个pod所在的宿主机查看 # node1 宿主机 # ip a | grep -A 5 \"cali04736ec14ce\" 122964: cali04736ec14ce@if4: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1380 qdisc noqueue state UP group default link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 16 inet6 fe80::ecee:eeff:feee:eeee/64 scope link valid_lft forever preferred_lft forever 120918: calidd1cafcd275@if4: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1380 qdisc noqueue state UP group default link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 2 发现pod demo-hello-sit中 的另一端设备编号和这里在node上看到的cali04736ec14ce编号122964是一样的 所以，node上的路由，发送cali04736ec14ce网卡设备的数据其实就是发送到了demo-hello-sit的这个pod中去了。到这里ping包就到了目的地。 注意看 demo-hello-sit这个pod所在的宿主机的路由，有一条 Destination为10.20.105.192的路由 # node1 # route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface ... 0.0.0.0 172.16.35.1 0.0.0.0 UG 100 0","date":"212111-08-2151","objectID":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/:0:8","tags":["kubernetes","calico"],"title":"calico - 网络流通以及抓包实战","uri":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/"},{"categories":["calico"],"content":"六、抓包分析 这里我使用了httpbin 服务和 sleep 服务来演示抓包过程 在 node1 sleep这个 pod 中去 curl node2 httpbin 服务，接着在 httpbin 所在node2 上tcpdump抓包 # 先在 nodes2 上 启动抓包 $ tcpdump -i ens160 -nn -w httpbin_ens160.cap tcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes # 然后在 node1 上发起 curl 请求 $ kubectl exec sleep-76c9b748f4-p964r -c sleep -n zhangji -- curl httpbin.httpbin:8000/headers % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 565 100 565 0 0 137k 0 --:--:-- --:--:-- --:--:-- 137k { \"headers\": { \"Accept\": \"*/*\", \"Content-Length\": \"0\", \"Host\": \"httpbin.httpbin:8000\", \"User-Agent\": \"curl/7.76.1-DEV\", \"X-B3-Parentspanid\": \"365864bb263d7b45\", \"X-B3-Sampled\": \"0\", \"X-B3-Spanid\": \"65ee1633796944f9\", \"X-B3-Traceid\": \"a36a2e412ab0d22b365864bb263d7b45\", \"X-Envoy-Attempt-Count\": \"1\", \"X-Forwarded-Client-Cert\": \"By=spiffe://cluster.local/ns/httpbin/sa/httpbin;Hash=cdd5655c5735fa86c8ad8636fdd91027fa288a2c31b6cb18d7feae0d563b009f;Subject=\\\"\\\";URI=spiffe://cluster.local/ns/zhangji/sa/default\" } } 结束抓包后下载httpbin_ens160.cap到本地wireshark进行抓包分析 能看到该数据包一共5层，其中IP(Internet Protocol)所在的网络层有两个，分别是pod之间的网络和主机之间的网络封装。 红色框选的是两个pod所在的宿主机，黄色框选的是两个pod的ip，src表示发起ping操作的pod所在的宿主机ip以及发起curl操作的pod的ip，dst表示被curl的pod所在的宿主机ip及被curl的pod的ip 封包顺序如下所示： 可以看到每个数据报文共有两个IP网络层,内层是Pod容器之间的IP网络报文,外层是宿主机节点的网络报文(2个node节点)。之所以要这样做是因为tunl0是一个隧道端点设备，在数据到达时要加上一层封装，便于发送到对端隧道设备中。 两层封包的具体内容如下： Pod间的通信经由IPIP的三层隧道转发,相比较VxLAN的二层隧道来说，IPIP隧道的开销较小，但其安全性也更差一些。 ","date":"212111-08-2151","objectID":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/:0:9","tags":["kubernetes","calico"],"title":"calico - 网络流通以及抓包实战","uri":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/"},{"categories":["docker"],"content":"Dockerfile 手把手教你怎么写.","date":"212111-08-2151","objectID":"/docker_dockerbuilder/","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"[TOC] 一 使用 Dockerfile 定制镜像 镜像的定制实际上就是定制每一层所添加的配置、文件。如果我们可以把每一层修改、安装、构建、操作的命令都写入一个脚本，用这个脚本来构建、定制镜像，那么之前提及的无法重复的问题、镜像构建透明性的问题、体积的问题就都会解决。这个脚本就是 Dockerfile。 Dockerfile 是一个文本文件，其内包含了一条条的 指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。 以定制 nginx 镜像为例，这次我们使用 Dockerfile 来定制。 在一个空白目录中，建立一个文本文件，并命名为 Dockerfile： $ mkdir mynginx $ cd mynginx $ touch Dockerfile 其内容为： FROMnginxRUN echo '\u003ch1\u003eHello, Docker!\u003c/h1\u003e' \u003e /usr/share/nginx/html/index.html 这个 Dockerfile 很简单，一共就两行。涉及到了两条指令，FROM 和 RUN。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:0:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"FROM 指定基础镜像 所谓定制镜像，那一定是以一个镜像为基础，在其上进行定制。基础镜像是必须指定的。而 FROM 就是指定 基础镜像，因此一个 Dockerfile 中 FROM 是必备的指令，并且必须是第一条指令。 在 Docker Hub 上有非常多的高质量的官方镜像，有可以直接拿来使用的服务类的镜像，如 nginx、redis、mongo、mysql、httpd、php、tomcat 等；也有一些方便开发、构建、运行各种语言应用的镜像，如 node、openjdk、python、ruby、golang 等。可以在其中寻找一个最符合我们最终目标的镜像为基础镜像进行定制。 如果没有找到对应服务的镜像，官方镜像中还提供了一些更为基础的操作系统镜像，如 ubuntu、debian、centos、fedora、alpine 等，这些操作系统的软件库为我们提供了更广阔的扩展空间。 除了选择现有镜像为基础镜像外，Docker 还存在一个特殊的镜像，名为 scratch。这个镜像是虚拟的概念，并不实际存在，它表示一个空白的镜像。 FROMscratch... 如果你以 scratch 为基础镜像的话，意味着你不以任何镜像为基础，接下来所写的指令将作为镜像第一层开始存在。 不以任何系统为基础，直接将可执行文件复制进镜像的做法并不罕见. 对于 Linux 下静态编译的程序来说，并不需要有操作系统提供运行时支持，所需的一切库都已经在可执行文件里了，因此直接 FROM scratch 会让镜像体积更加小巧。 使用 Go 语言 开发的应用很多会使用这种方式来制作镜像，这也是为什么有人认为 Go 是特别适合容器微服务架构的语言的原因之一。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:1:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"RUN 执行命令 RUN 指令是用来执行命令行命令的。由于命令行的强大能力，RUN 指令在定制镜像时是最常用的指令之一。其格式有两种： shell 格式：RUN \u003c命令\u003e，就像直接在命令行中输入的命令一样。刚才写的 Dockerfile 中的 RUN 指令就是这种格式。 RUN echo '\u003ch1\u003eHello, Docker!\u003c/h1\u003e' \u003e /usr/share/nginx/html/index.html exec 格式：RUN [\"可执行文件\", \"参数1\", \"参数2\"]，这更像是函数调用中的格式。 既然 RUN 就像 Shell 脚本一样可以执行命令，那么我们是否就可以像 Shell 脚本一样把每个命令对应一个 RUN 呢？比如这样： FROMdebian:stretchRUN apt-get updateRUN apt-get install -y gcc libc6-dev make wgetRUN wget -O redis.tar.gz \"http://download.redis.io/releases/redis-5.0.3.tar.gz\"RUN mkdir -p /usr/src/redisRUN tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1RUN make -C /usr/src/redisRUN make -C /usr/src/redis install 之前说过，Dockerfile 中每一个指令都会建立一层，RUN 也不例外。每一个 RUN 的行为： 新建立一层，在其上执行这些命令 执行结束后，commit 这一层的修改，构成新的镜像。 而上面的这种写法，创建了 7 层镜像。这是完全没有意义的，而且很多运行时不需要的东西，都被装进了镜像里，比如编译环境、更新的软件包等等。结果就是产生非常臃肿、非常多层的镜像，不仅仅增加了构建部署的时间，也很容易出错。 这是很多初学 Docker 的人常犯的一个错误。 Union FS 是有最大层数限制的，比如 AUFS，曾经是最大不得超过 42 层，现在是不得超过 127 层。 上面的 Dockerfile 正确的写法应该是这样： FROMdebian:stretchRUN set -x; buildDeps='gcc libc6-dev make wget' \\ \u0026\u0026 apt-get update \\ \u0026\u0026 apt-get install -y $buildDeps \\ \u0026\u0026 wget -O redis.tar.gz \"http://download.redis.io/releases/redis-5.0.3.tar.gz\" \\ \u0026\u0026 mkdir -p /usr/src/redis \\ \u0026\u0026 tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \\ \u0026\u0026 make -C /usr/src/redis \\ \u0026\u0026 make -C /usr/src/redis install \\ \u0026\u0026 rm -rf /var/lib/apt/lists/* \\ \u0026\u0026 rm redis.tar.gz \\ \u0026\u0026 rm -r /usr/src/redis \\ \u0026\u0026 apt-get purge -y --auto-remove $buildDeps 首先，之前所有的命令只有一个目的，就是编译、安装 redis 可执行文件。因此没有必要建立很多层，这只是一层的事情。因此，这里没有使用很多个 RUN 一一对应不同的命令，而是仅仅使用一个 RUN 指令，并使用 \u0026\u0026 将各个所需命令串联起来。将之前的 7 层，简化为了 1 层。在撰写 Dockerfile 的时候，要经常提醒自己，这并不是在写 Shell 脚本，而是在定义每一层该如何构建。 并且，这里为了格式化还进行了换行。Dockerfile 支持 Shell 类的行尾添加 \\ 的命令换行方式，以及行首 # 进行注释的格式。良好的格式，比如换行、缩进、注释等，会让维护、排障更为容易，这是一个比较好的习惯。 此外，还可以看到这一组命令的最后添加了清理工作的命令，删除了为了编译构建所需要的软件，清理了所有下载、展开的文件，并且还清理了 apt 缓存文件。这是很重要的一步，我们之前说过，镜像是多层存储，每一层的东西并不会在下一层被删除，会一直跟随着镜像。因此镜像构建时，一定要确保每一层只添加真正需要添加的东西，任何无关的东西都应该清理掉。 很多人初学 Docker 制作出了很臃肿的镜像的原因之一，就是忘记了每一层构建的最后一定要清理掉无关文件。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:2:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"构建镜像 好了，让我们再回到之前定制的 nginx 镜像的 Dockerfile 来。现在我们明白了这个 Dockerfile 的内容，那么让我们来构建这个镜像吧。 在 Dockerfile 文件所在目录执行： $ docker build -t nginx:v3 . Sending build context to Docker daemon 2.048 kB Step 1 : FROM nginx ---\u003e e43d811ce2f4 Step 2 : RUN echo '\u003ch1\u003eHello, Docker!\u003c/h1\u003e' \u003e /usr/share/nginx/html/index.html ---\u003e Running in 9cdc27646c7b ---\u003e 44aa4490ce2c Removing intermediate container 9cdc27646c7b Successfully built 44aa4490ce2c 从命令的输出结果中，我们可以清晰的看到镜像的构建过程。在 Step 2 中，如同我们之前所说的那样，RUN 指令启动了一个容器 9cdc27646c7b，执行了所要求的命令，并最后提交了这一层 44aa4490ce2c，随后删除了所用到的这个容器 9cdc27646c7b。 这里我们使用了 docker build 命令进行镜像构建。其格式为： docker build [选项] \u003c上下文路径/URL/-\u003e 在这里我们指定了最终镜像的名称 -t nginx:v3，构建成功后，会生成一个新的nginx镜像。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:3:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"镜像构建上下文（Context） 如果注意，会看到 docker build 命令最后有一个 .。. 表示当前目录，而 Dockerfile 就在当前目录，因此不少初学者以为这个路径是在指定 Dockerfile 所在路径，这么理解其实是不准确的。如果对应上面的命令格式，你可能会发现，这是在指定 上下文路径。那么什么是上下文呢？ 首先我们要理解 docker build 的工作原理。Docker 在运行时分为 Docker 引擎（也就是服务端守护进程）和客户端工具。Docker 的引擎提供了一组 REST API，被称为 Docker Remote API，而如 docker 命令这样的客户端工具，则是通过这组 API 与 Docker 引擎交互，从而完成各种功能。因此，虽然表面上我们好像是在本机执行各种 docker 功能，但实际上，一切都是使用的远程调用形式在服务端（Docker 引擎）完成。也因为这种 C/S 设计，让我们操作远程服务器的 Docker 引擎变得轻而易举。 当我们进行镜像构建的时候，并非所有定制都会通过 RUN 指令完成，经常会需要将一些本地文件复制进镜像，比如通过 COPY 指令、ADD 指令等。而 docker build 命令构建镜像，其实并非在本地构建，而是在服务端，也就是 Docker 引擎中构建的。那么在这种客户端/服务端的架构中，如何才能让服务端获得本地文件呢？ 这就引入了上下文的概念。当构建的时候，用户会指定构建镜像上下文的路径，docker build 命令得知这个路径后，会将路径下的所有内容打包，然后上传给 Docker 引擎。这样 Docker 引擎收到这个上下文包后，展开就会获得构建镜像所需的一切文件。 如果在 Dockerfile 中这么写： COPY ./package.json /app/ 这并不是要复制执行 docker build 命令所在的目录下的 package.json，也不是复制 Dockerfile 所在目录下的 package.json，而是复制 上下文（context） 目录下的 package.json。 因此，COPY 这类指令中的源文件的路径都是相对路径。这也是初学者经常会问的为什么 COPY ../package.json /app 或者 COPY /opt/xxxx /app 无法工作的原因，因为这些路径已经超出了上下文的范围，Docker 引擎无法获得这些位置的文件。如果真的需要那些文件，应该将它们复制到上下文目录中去。 现在就可以理解刚才的命令 docker build -t nginx:v3 . 中的这个 .，实际上是在指定上下文的目录，docker build 命令会将该目录下的内容打包交给 Docker 引擎以帮助构建镜像。 如果观察 docker build 输出，我们其实已经看到了这个发送上下文的过程： $ docker build -t nginx:v3 . Sending build context to Docker daemon 2.048 kB ... 理解构建上下文对于镜像构建是很重要的，避免犯一些不应该的错误。比如有些初学者在发现 COPY /opt/xxxx /app 不工作后，于是干脆将 Dockerfile 放到了硬盘根目录去构建，结果发现 docker build 执行后，在发送一个几十 GB 的东西，极为缓慢而且很容易构建失败。那是因为这种做法是在让 docker build 打包整个硬盘，这显然是使用错误。 一般来说，应该会将 Dockerfile 置于一个空目录下，或者项目根目录下。如果该目录下没有所需文件，那么应该把所需文件复制一份过来。如果目录下有些东西确实不希望构建时传给 Docker 引擎，那么可以用 .gitignore 一样的语法写一个 .dockerignore，该文件是用于剔除不需要作为上下文传递给 Docker 引擎的。 那么为什么会有人误以为 . 是指定 Dockerfile 所在目录呢？这是因为在默认情况下，如果不额外指定 Dockerfile 的话，会将上下文目录下的名为 Dockerfile 的文件作为 Dockerfile。 这只是默认行为，实际上 Dockerfile 的文件名并不要求必须为 Dockerfile，而且并不要求必须位于上下文目录中，比如可以用 -f ../Dockerfile.php 参数指定某个文件作为 Dockerfile。 当然，一般大家习惯性的会使用默认的文件名 Dockerfile，以及会将其置于镜像构建上下文目录中。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:4:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"其它 docker build 的用法 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:5:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"直接用 Git repo 进行构建 或许你已经注意到了，docker build 还支持从 URL 构建，比如可以直接从 Git repo 中构建： # $env:DOCKER_BUILDKIT=0 # export DOCKER_BUILDKIT=0 $ docker build -t hello-world https://github.com/docker-library/hello-world.git#master:amd64/hello-world Step 1/3 : FROM scratch ---\u003e Step 2/3 : COPY hello / ---\u003e ac779757d46e Step 3/3 : CMD [\"/hello\"] ---\u003e Running in d2a513a760ed Removing intermediate container d2a513a760ed ---\u003e 038ad4142d2b Successfully built 038ad4142d2b 这行命令指定了构建所需的 Git repo，并且指定分支为 master，构建目录为 /amd64/hello-world/，然后 Docker 就会自己去 git clone 这个项目、切换到指定分支、并进入到指定目录后开始构建。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:5:1","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"用给定的 tar 压缩包构建 $ docker build http://server/context.tar.gz 如果所给出的 URL 不是个 Git repo，而是个 tar 压缩包，那么 Docker 引擎会下载这个包，并自动解压缩，以其作为上下文，开始构建。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:5:2","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"从标准输入中读取 Dockerfile 进行构建 docker build - \u003c Dockerfile 或 cat Dockerfile | docker build - 如果标准输入传入的是文本文件，则将其视为 Dockerfile，并开始构建。这种形式由于直接从标准输入中读取 Dockerfile 的内容，它没有上下文，因此不可以像其他方法那样可以将本地文件 COPY 进镜像之类的事情。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:5:3","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"从标准输入中读取上下文压缩包进行构建 $ docker build - \u003c context.tar.gz 如果发现标准输入的文件格式是 gzip、bzip2 以及 xz 的话，将会使其为上下文压缩包，直接将其展开，将里面视为上下文，并开始构建。 二 指令详解 我们已经介绍了 FROM，RUN，还提及了 COPY, ADD，其实 Dockerfile 功能很强大，它提供了十多个指令。下面我们继续讲解其他的指令。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:5:4","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"COPY 复制文件 格式： COPY [--chown=\u003cuser\u003e:\u003cgroup\u003e] \u003c源路径\u003e... \u003c目标路径\u003e COPY [--chown=\u003cuser\u003e:\u003cgroup\u003e] [\"\u003c源路径1\u003e\",... \"\u003c目标路径\u003e\"] 和 RUN 指令一样，也有两种格式，一种类似于命令行，一种类似于函数调用。 COPY 指令将从构建上下文目录中 \u003c源路径\u003e 的文件/目录复制到新的一层的镜像内的 \u003c目标路径\u003e 位置。比如： COPY package.json /usr/src/app/ \u003c源路径\u003e 可以是多个，甚至可以是通配符，其通配符规则要满足 Go 的 filepath.Match 规则，如： COPY hom* /mydir/COPY hom?.txt /mydir/ \u003c目标路径\u003e 可以是容器内的绝对路径，也可以是相对于工作目录的相对路径（工作目录可以用 WORKDIR 指令来指定）。目标路径不需要事先创建，如果目录不存在会在复制文件前先行创建缺失目录。 此外，还需要注意一点，使用 COPY 指令，源文件的各种元数据都会保留。比如读、写、执行权限、文件变更时间等。这个特性对于镜像定制很有用。特别是构建相关文件都在使用 Git 进行管理的时候。 在使用该指令的时候还可以加上 --chown=\u003cuser\u003e:\u003cgroup\u003e 选项来改变文件的所属用户及所属组。 COPY --chown=55:mygroup files* /mydir/COPY --chown=bin files* /mydir/COPY --chown=1 files* /mydir/COPY --chown=10:11 files* /mydir/ 注意事项：如果源路径为文件夹，复制的时候不是直接复制该文件夹，而是将文件夹中的内容复制到目标路径。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:6:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"ADD 更高级的复制文件 ADD 指令和 COPY 的格式和性质基本一致。但是在 COPY 基础上增加了一些功能。 比如 \u003c源路径\u003e 可以是一个 URL，这种情况下，Docker 引擎会试图去下载这个链接的文件放到 \u003c目标路径\u003e 去。下载后的文件权限自动设置为 600，如果这并不是想要的权限，那么还需要增加额外的一层 RUN 进行权限调整，另外，如果下载的是个压缩包，需要解压缩，也一样还需要额外的一层 RUN 指令进行解压缩。所以不如直接使用 RUN 指令，然后使用 wget 或者 curl 工具下载，处理权限、解压缩、然后清理无用文件更合理。因此，这个功能其实并不实用，而且不推荐使用。 如果 \u003c源路径\u003e 为一个 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，ADD 指令将会自动解压缩这个压缩文件到 \u003c目标路径\u003e 去。 在某些情况下，这个自动解压缩的功能非常有用，比如官方镜像 ubuntu 中： FROMscratchADD ubuntu-xenial-core-cloudimg-amd64-root.tar.gz /... 但在某些情况下，如果我们真的是希望复制个压缩文件进去，而不解压缩，这时就不可以使用 ADD 命令了。 在 Docker 官方的 Dockerfile 最佳实践文档 中要求，尽可能的使用 COPY，因为 COPY 的语义很明确，就是复制文件而已，而 ADD 则包含了更复杂的功能，其行为也不一定很清晰。最适合使用 ADD 的场合，就是所提及的需要自动解压缩的场合。 另外需要注意的是，ADD 指令会令镜像构建缓存失效，从而可能会令镜像构建变得比较缓慢。 因此在 COPY 和 ADD 指令中选择的时候，可以遵循这样的原则： 所有的文件复制均使用 COPY 指令，仅在需要自动解压缩的场合使用 ADD。 在使用该指令的时候还可以加上 --chown=\u003cuser\u003e:\u003cgroup\u003e 选项来改变文件的所属用户及所属组。 ADD --chown=55:mygroup files* /mydir/ADD --chown=bin files* /mydir/ADD --chown=1 files* /mydir/ADD --chown=10:11 files* /mydir/ ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:7:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"CMD 容器启动命令 CMD 指令的格式和 RUN 相似，也是两种格式： shell 格式：CMD \u003c命令\u003e exec 格式：CMD [\"可执行文件\", \"参数1\", \"参数2\"...] 参数列表格式：CMD [\"参数1\", \"参数2\"...]。在指定了 ENTRYPOINT 指令后，用 CMD 指定具体的参数。 Docker 不是虚拟机，容器就是进程。既然是进程，那么在启动容器的时候，需要指定所运行的程序及参数。 功能：CMD 指令就是用于指定默认的容器主进程的启动命令的。 在运行时可以指定新的命令来替代镜像设置中的这个默认命令，比如，ubuntu 镜像默认的 CMD 是 /bin/bash，如果我们直接 docker run -it ubuntu 的话，会直接进入 bash。我们也可以在运行时指定运行别的命令，如 docker run -it ubuntu cat /etc/os-release。这就是用 cat /etc/os-release 命令替换了默认的 /bin/bash 命令了，输出了系统版本信息。 在指令格式上，一般推荐使用 exec 格式，这类格式在解析时会被解析为 JSON 数组，因此一定要使用双引号 \"，而不要使用单引号。 如果使用 shell 格式的话，实际的命令会被包装为 sh -c 的参数的形式进行执行。比如： CMD echo $HOME 在实际执行中，会将其变更为： CMD [ \"sh\", \"-c\", \"echo $HOME\" ] 这就是为什么我们可以使用环境变量的原因，因为这些环境变量会被 shell 进行解析处理。 提到 CMD 就不得不提容器中应用在前台执行和后台执行的问题。这是初学者常出现的一个混淆。 Docker 不是虚拟机，容器中的应用都应该以前台执行，而不是像虚拟机、物理机里面那样，用 systemd 去启动后台服务，容器内没有后台服务的概念。 一些初学者将 CMD 写为： CMD service nginx start 然后发现容器执行后就立即退出了。甚至在容器内去使用 systemctl 命令结果却发现根本执行不了。这就是因为没有搞明白前台、后台的概念，没有区分容器和虚拟机的差异，依旧在以传统虚拟机的角度去理解容器。 对于容器而言，其启动程序就是容器应用进程，容器就是为了主进程而存在的，主进程退出，容器就失去了存在的意义，从而退出，其它辅助进程不是它需要关心的东西。 而使用 service nginx start 命令，则是希望 upstart 来以后台守护进程形式启动 nginx 服务。而刚才说了 CMD service nginx start 会被理解为 CMD [ \"sh\", \"-c\", \"service nginx start\"]，因此主进程实际上是 sh。那么当 service nginx start 命令结束后，sh 也就结束了，sh 作为主进程退出了，自然就会令容器退出。 正确的做法是直接执行 nginx 可执行文件，并且要求以前台形式运行。比如： CMD [\"nginx\", \"-g\", \"daemon off;\"] ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:8:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"ENTRYPOINT 入口点 ENTRYPOINT 的格式和 RUN 指令格式一样，分为 exec 格式和 shell 格式。 ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数。ENTRYPOINT 在运行时也可以替代，不过比 CMD 要略显繁琐，需要通过 docker run 的参数 --entrypoint 来指定。 当指定了 ENTRYPOINT 后，CMD 的含义就发生了改变，不再是直接的运行其命令，而是将 CMD 的内容作为参数传给 ENTRYPOINT 指令，换句话说实际执行时，将变为： \u003cENTRYPOINT\u003e \"\u003cCMD\u003e\" 那么有了 CMD 后，为什么还要有 ENTRYPOINT 呢？这种 \u003cENTRYPOINT\u003e \"\u003cCMD\u003e\" 有什么好处么？让我们来看几个场景。 场景一：让镜像变成像命令一样使用 假设我们需要一个得知自己当前公网 IP 的镜像，那么可以先用 CMD 来实现： FROMubuntu:18.04RUN apt-get update \\ \u0026\u0026 apt-get install -y curl \\ \u0026\u0026 rm -rf /var/lib/apt/lists/*CMD [ \"curl\", \"-s\", \"http://myip.ipip.net\" ] 假如我们使用 docker build -t myip . 来构建镜像的话，如果我们需要查询当前公网 IP，只需要执行： $ docker run myip 当前 IP：61.148.226.66 来自：北京市 联通 嗯，这么看起来好像可以直接把镜像当做命令使用了，不过命令总有参数，如果我们希望加参数呢？比如从上面的 CMD 中可以看到实质的命令是 curl，那么如果我们希望显示 HTTP 头信息，就需要加上 -i 参数。那么我们可以直接加 -i 参数给 docker run myip 么？ $ docker run myip -i docker: Error response from daemon: invalid header field value \"oci runtime error: container_linux.go:247: starting container process caused \\\"exec: \\\\\\\"-i\\\\\\\": executable file not found in $PATH\\\"\\n\". 我们可以看到可执行文件找不到的报错，executable file not found。之前我们说过，跟在镜像名后面的是 command，运行时会替换 CMD 的默认值。因此这里的 -i 替换了原来的 CMD，而不是添加在原来的 curl -s http://myip.ipip.net 后面。而 -i 根本不是命令，所以自然找不到。 那么如果我们希望加入 -i 这参数，我们就必须重新完整的输入这个命令： $ docker run myip curl -s http://myip.ipip.net -i 这显然不是很好的解决方案，而使用 ENTRYPOINT 就可以解决这个问题。现在我们重新用 ENTRYPOINT 来实现这个镜像： FROMubuntu:18.04RUN apt-get update \\ \u0026\u0026 apt-get install -y curl \\ \u0026\u0026 rm -rf /var/lib/apt/lists/*ENTRYPOINT [ \"curl\", \"-s\", \"http://myip.ipip.net\" ] 这次我们再来尝试直接使用 docker run myip -i： $ docker run myip 当前 IP：61.148.226.66 来自：北京市 联通 $ docker run myip -i HTTP/1.1 200 OK Server: nginx/1.8.0 Date: Tue, 22 Nov 2016 05:12:40 GMT Content-Type: text/html; charset=UTF-8 Vary: Accept-Encoding X-Powered-By: PHP/5.6.24-1~dotdeb+7.1 X-Cache: MISS from cache-2 X-Cache-Lookup: MISS from cache-2:80 X-Cache: MISS from proxy-2_6 Transfer-Encoding: chunked Via: 1.1 cache-2:80, 1.1 proxy-2_6:8006 Connection: keep-alive 当前 IP：61.148.226.66 来自：北京市 联通 可以看到，这次成功了。这是因为当存在 ENTRYPOINT 后，CMD 的内容将会作为参数传给 ENTRYPOINT，而这里 -i 就是新的 CMD，因此会作为参数传给 curl，从而达到了我们预期的效果。 场景二：应用运行前的准备工作 启动容器就是启动主进程，但有些时候，启动主进程前，需要一些准备工作。 比如 mysql 类的数据库，可能需要一些数据库配置、初始化的工作，这些工作要在最终的 mysql 服务器运行之前解决。 此外，可能希望避免使用 root 用户去启动服务，从而提高安全性，而在启动服务前还需要以 root 身份执行一些必要的准备工作，最后切换到服务用户身份启动服务。或者除了服务外，其它命令依旧可以使用 root 身份执行，方便调试等。 这些准备工作是和容器 CMD 无关的，无论 CMD 为什么，都需要事先进行一个预处理的工作。这种情况下，可以写一个脚本，然后放入 ENTRYPOINT 中去执行，而这个脚本会将接到的参数（也就是 \u003cCMD\u003e）作为命令，在脚本最后执行。比如官方镜像 redis 中就是这么做的： FROMalpine:3.4...RUN addgroup -S redis \u0026\u0026 adduser -S -G redis redis...ENTRYPOINT [\"docker-entrypoint.sh\"]EXPOSE6379CMD [ \"redis-server\" ] 可以看到其中为了 redis 服务创建了 redis 用户，并在最后指定了 ENTRYPOINT 为 docker-entrypoint.sh 脚本。 #!/bin/sh ... # allow the container to be started with `--user` if [ \"$1\" = 'redis-server' -a \"$(id -u)\" = '0' ]; then find . \\! -user redis -exec chown redis '{}' + exec gosu redis \"$0\" \"$@\" fi exec \"$@\" 该脚本的内容就是根据 CMD 的内容来判断，如果是 redis-server 的话，则切换到 redis 用户身份启动服务器，否则依旧使用 root 身份执行。比如： $ docker run -it redis id uid=0(root) gid=0(root) groups=0(root) ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:9:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"ENV 设置环境变量 格式有两种： ENV \u003ckey\u003e \u003cvalue\u003e ENV \u003ckey1\u003e=\u003cvalue1\u003e \u003ckey2\u003e=\u003cvalue2\u003e... 这个指令很简单，就是设置环境变量而已，无论是后面的其它指令，如 RUN，还是运行时的应用，都可以直接使用这里定义的环境变量。 ENV VERSION=1.0 DEBUG=on \\ NAME=\"Happy Feet\" 这个例子中演示了如何换行，以及对含有空格的值用双引号括起来的办法，这和 Shell 下的行为是一致的。 定义了环境变量，那么在后续的指令中，就可以使用这个环境变量。比如在官方 node 镜像 Dockerfile 中，就有类似这样的代码： ENV NODE_VERSION 7.2.0RUN curl -SLO \"https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION-linux-x64.tar.xz\" \\ \u0026\u0026 curl -SLO \"https://nodejs.org/dist/v$NODE_VERSION/SHASUMS256.txt.asc\" \\ \u0026\u0026 gpg --batch --decrypt --output SHASUMS256.txt SHASUMS256.txt.asc \\ \u0026\u0026 grep \" node-v$NODE_VERSION-linux-x64.tar.xz\\$\" SHASUMS256.txt | sha256sum -c - \\ \u0026\u0026 tar -xJf \"node-v$NODE_VERSION-linux-x64.tar.xz\" -C /usr/local --strip-components=1 \\ \u0026\u0026 rm \"node-v$NODE_VERSION-linux-x64.tar.xz\" SHASUMS256.txt.asc SHASUMS256.txt \\ \u0026\u0026 ln -s /usr/local/bin/node /usr/local/bin/nodejs 在这里先定义了环境变量 NODE_VERSION，其后的 RUN 这层里，多次使用 $NODE_VERSION 来进行操作定制。可以看到，将来升级镜像构建版本的时候，只需要更新 7.2.0 即可，Dockerfile 构建维护变得更轻松了。 下列指令可以支持环境变量展开： ADD、COPY、ENV、EXPOSE、FROM、LABEL、USER、WORKDIR、VOLUME、STOPSIGNAL、ONBUILD、RUN。 可以从这个指令列表里感觉到，环境变量可以使用的地方很多，很强大。通过环境变量，我们可以让一份 Dockerfile 制作更多的镜像，只需使用不同的环境变量即可。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:10:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"ARG 构建参数 格式：ARG \u003c参数名\u003e[=\u003c默认值\u003e] 构建参数和 ENV 的效果一样，都是设置环境变量。所不同的是，ARG 所设置的构建环境的环境变量，在将来容器运行时是不会存在这些环境变量的。但是不要因此就使用 ARG 保存密码之类的信息，因为 docker history 还是可以看到所有值的。 Dockerfile 中的 ARG 指令是定义参数名称，以及定义其默认值。该默认值可以在构建命令 docker build 中用 --build-arg \u003c参数名\u003e=\u003c值\u003e 来覆盖。 灵活的使用 ARG 指令，能够在不修改 Dockerfile 的情况下，构建出不同的镜像。 ARG 指令有生效范围，如果在 FROM 指令之前指定，那么只能用于 FROM 指令中。 ARG DOCKER_USERNAME=library FROM${DOCKER_USERNAME}/alpineRUN set -x ; echo ${DOCKER_USERNAME} 使用上述 Dockerfile 会发现无法输出 ${DOCKER_USERNAME} 变量的值，要想正常输出，你必须在 FROM 之后再次指定 ARG # 只在 FROM 中生效ARG DOCKER_USERNAME=library FROM${DOCKER_USERNAME}/alpine# 要想在 FROM 之后使用，必须再次指定ARG DOCKER_USERNAME=library RUN set -x ; echo ${DOCKER_USERNAME} 对于多阶段构建，尤其要注意这个问题 # 这个变量在每个 FROM 中都生效ARG DOCKER_USERNAME=library FROM${DOCKER_USERNAME}/alpineRUN set -x ; echo 1FROM${DOCKER_USERNAME}/alpineRUN set -x ; echo 2 对于上述 Dockerfile 两个 FROM 指令都可以使用 ${DOCKER_USERNAME}，对于在各个阶段中使用的变量都必须在每个阶段分别指定： ARG DOCKER_USERNAME=library FROM${DOCKER_USERNAME}/alpine# 在FROM 之后使用变量，必须在每个阶段分别指定ARG DOCKER_USERNAME=library RUN set -x ; echo ${DOCKER_USERNAME}FROM${DOCKER_USERNAME}/alpine# 在FROM 之后使用变量，必须在每个阶段分别指定ARG DOCKER_USERNAME=library RUN set -x ; echo ${DOCKER_USERNAME} ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:11:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"VOLUME 定义匿名卷 格式为： VOLUME [\"\u003c路径1\u003e\", \"\u003c路径2\u003e\"...] VOLUME \u003c路径\u003e 之前我们说过，容器运行时应该尽量保持容器存储层不发生写操作，对于数据库类需要保存动态数据的应用，其数据库文件应该保存于卷(volume)中，后面的章节我们会进一步介绍 Docker 卷的概念。为了防止运行时用户忘记将动态文件所保存目录挂载为卷，在 Dockerfile 中，我们可以事先指定某些目录挂载为匿名卷，这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据。 VOLUME/data 这里的 /data 目录就会在容器运行时自动挂载为匿名卷，任何向 /data 中写入的信息都不会记录进容器存储层，从而保证了容器存储层的无状态化。当然，运行容器时可以覆盖这个挂载设置。比如： $ docker run -d -v mydata:/data xxxx 在这行命令中，就使用了 mydata 这个命名卷挂载到了 /data 这个位置，替代了 Dockerfile 中定义的匿名卷的挂载配置。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:12:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"EXPOSE 暴露端口 格式为 EXPOSE \u003c端口1\u003e [\u003c端口2\u003e...]。 EXPOSE 指令是声明容器运行时提供服务的端口，这只是一个声明，在容器运行时并不会因为这个声明应用就会开启这个端口的服务。在 Dockerfile 中写入这样的声明有两个好处： 一个是帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射； 另一个用处则是在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口。 要将 EXPOSE 和在运行时使用 -p \u003c宿主端口\u003e:\u003c容器端口\u003e 区分开来。-p，是映射宿主端口和容器端口，换句话说，就是将容器的对应端口服务公开给外界访问，而 EXPOSE 仅仅是声明容器打算使用什么端口而已，并不会自动在宿主进行端口映射。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:13:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"WORKDIR 指定工作目录 格式为 WORKDIR \u003c工作目录路径\u003e。 使用 WORKDIR 指令可以来指定工作目录（或者称为当前目录），以后各层的当前目录就被改为指定的目录，如该目录不存在，WORKDIR 会帮你建立目录。 之前提到一些初学者常犯的错误是把 Dockerfile 等同于 Shell 脚本来书写，这种错误的理解还可能会导致出现下面这样的错误： RUN cd /appRUN echo \"hello\" \u003e world.txt 如果将这个 Dockerfile 进行构建镜像运行后，会发现找不到 /app/world.txt 文件，或者其内容不是 hello。原因其实很简单： 在 Shell 中，连续两行是同一个进程执行环境，因此前一个命令修改的内存状态，会直接影响后一个命令； 而在 Dockerfile 中，这两行 RUN 命令的执行环境根本不同，是两个完全不同的容器。这就是对 Dockerfile 构建分层存储的概念不了解所导致的错误。 之前说过每一个 RUN 都是启动一个容器、执行命令、然后提交存储层文件变更。第一层 RUN cd /app 的执行仅仅是当前进程的工作目录变更，一个内存上的变化而已，其结果不会造成任何文件变更。而到第二层的时候，启动的是一个全新的容器，跟第一层的容器更完全没关系，自然不可能继承前一层构建过程中的内存变化。 因此如果需要改变以后各层的工作目录的位置，那么应该使用 WORKDIR 指令。 WORKDIR/appRUN echo \"hello\" \u003e world.txt 如果你的 WORKDIR 指令使用的相对路径，那么所切换的路径与之前的 WORKDIR 有关： WORKDIR/aWORKDIRbWORKDIRcRUN pwd RUN pwd 的工作目录为 /a/b/c。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:14:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"USER 指定当前用户 格式：USER \u003c用户名\u003e[:\u003c用户组\u003e] USER 指令和 WORKDIR 相似，都是改变环境状态并影响以后的层。WORKDIR 是改变工作目录，USER 则是改变之后层的执行 RUN, CMD 以及 ENTRYPOINT 这类命令的身份。 注意，USER 只是帮助你切换到指定用户而已，这个用户必须是事先建立好的，否则无法切换。 RUN groupadd -r redis \u0026\u0026 useradd -r -g redis redisUSERredisRUN [ \"redis-server\" ] 如果以 root 执行的脚本，在执行期间希望改变身份，比如希望以某个已经建立好的用户来运行某个服务进程，不要使用 su 或者 sudo，这些都需要比较麻烦的配置，而且在 TTY 缺失的环境下经常出错。建议使用 gosu。 # 建立 redis 用户，并使用 gosu 换另一个用户执行命令RUN groupadd -r redis \u0026\u0026 useradd -r -g redis redis# 下载 gosuRUN wget -O /usr/local/bin/gosu \"https://github.com/tianon/gosu/releases/download/1.12/gosu-amd64\" \\ \u0026\u0026 chmod +x /usr/local/bin/gosu \\ \u0026\u0026 gosu nobody true# 设置 CMD，并以另外的用户执行CMD [ \"exec\", \"gosu\", \"redis\", \"redis-server\" ] ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:15:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"HEALTHCHECK 健康检查 格式： HEALTHCHECK [选项] CMD \u003c命令\u003e：设置检查容器健康状况的命令 HEALTHCHECK NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉其健康检查指令 HEALTHCHECK 指令是告诉 Docker 应该如何进行判断容器的状态是否正常，这是 Docker 1.12 引入的新指令。 在没有 HEALTHCHECK 指令前，Docker 引擎只可以通过容器内主进程是否退出来判断容器是否状态异常。很多情况下这没问题，但是如果程序进入死锁状态，或者死循环状态，应用进程并不退出，但是该容器已经无法提供服务了。在 1.12 以前，Docker 不会检测到容器的这种状态，从而不会重新调度，导致可能会有部分容器已经无法提供服务了却还在接受用户请求。 而自 1.12 之后，Docker 提供了 HEALTHCHECK 指令，通过该指令指定一行命令，用这行命令来判断容器主进程的服务状态是否还正常，从而比较真实的反应容器实际状态。 当在一个镜像指定了 HEALTHCHECK 指令后，用其启动容器，初始状态会为 starting，在 HEALTHCHECK 指令检查成功后变为 healthy，如果连续一定次数失败，则会变为 unhealthy。 HEALTHCHECK 支持下列选项： --interval=\u003c间隔\u003e：两次健康检查的间隔，默认为 30 秒； --timeout=\u003c时长\u003e：健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认 30 秒； --retries=\u003c次数\u003e：当连续失败指定次数后，则将容器状态视为 unhealthy，默认 3 次。 和 CMD, ENTRYPOINT 一样，HEALTHCHECK 只可以出现一次，如果写了多个，只有最后一个生效。 在 HEALTHCHECK [选项] CMD 后面的命令，格式和 ENTRYPOINT 一样，分为 shell 格式，和 exec 格式。命令的返回值决定了该次健康检查的成功与否：0：成功；1：失败；2：保留，不要使用这个值。 假设我们有个镜像是个最简单的 Web 服务，我们希望增加健康检查来判断其 Web 服务是否在正常工作，我们可以用 curl 来帮助判断，其 Dockerfile 的 HEALTHCHECK 可以这么写： FROMnginxRUN apt-get update \u0026\u0026 apt-get install -y curl \u0026\u0026 rm -rf /var/lib/apt/lists/*HEALTHCHECK --interval=5s --timeout=3s \\ CMD curl -fs http://localhost/ || exit 1 这里我们设置了每 5 秒检查一次（这里为了试验所以间隔非常短，实际应该相对较长），如果健康检查命令超过 3 秒没响应就视为失败，并且使用 curl -fs http://localhost/ || exit 1 作为健康检查命令。 使用 docker build 来构建这个镜像： $ docker build -t myweb:v1 . 构建好了后，我们启动一个容器： $ docker run -d --name web -p 80:80 myweb:v1 当运行该镜像后，可以通过 docker container ls 看到最初的状态为 (health: starting)： $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 03e28eb00bd0 myweb:v1 \"nginx -g 'daemon off\" 3 seconds ago Up 2 seconds (health: starting) 80/tcp, 443/tcp web 在等待几秒钟后，再次 docker container ls，就会看到健康状态变化为了 (healthy)： $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 03e28eb00bd0 myweb:v1 \"nginx -g 'daemon off\" 18 seconds ago Up 16 seconds (healthy) 80/tcp, 443/tcp web 如果健康检查连续失败超过了重试次数，状态就会变为 (unhealthy)。 为了帮助排障，健康检查命令的输出（包括 stdout 以及 stderr）都会被存储于健康状态里，可以用 docker inspect 来查看。 $ docker inspect --format '{{json .State.Health}}' web | python -m json.tool { \"FailingStreak\": 0, \"Log\": [ { \"End\": \"2016-11-25T14:35:37.940957051Z\", \"ExitCode\": 0, \"Output\": \"\u003c!DOCTYPE html\u003e\\n\u003chtml\u003e\\n\u003chead\u003e\\n\u003ctitle\u003eWelcome to nginx!\u003c/title\u003e\\n\u003cstyle\u003e\\n body {\\n width: 35em;\\n margin: 0 auto;\\n font-family: Tahoma, Verdana, Arial, sans-serif;\\n }\\n\u003c/style\u003e\\n\u003c/head\u003e\\n\u003cbody\u003e\\n\u003ch1\u003eWelcome to nginx!\u003c/h1\u003e\\n\u003cp\u003eIf you see this page, the nginx web server is successfully installed and\\nworking. Further configuration is required.\u003c/p\u003e\\n\\n\u003cp\u003eFor online documentation and support please refer to\\n\u003ca href=\\\"http://nginx.org/\\\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e\\nCommercial support is available at\\n\u003ca href=\\\"http://nginx.com/\\\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e\\n\\n\u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e\\n\u003c/body\u003e\\n\u003c/html\u003e\\n\", \"Start\": \"2016-11-25T14:35:37.780192565Z\" } ], \"Status\": \"healthy\" } ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:16:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"ONBUILD 为他人作嫁衣裳 格式：ONBUILD \u003c其它指令\u003e。 ONBUILD 是一个特殊的指令，它后面跟的是其它指令，比如 RUN, COPY 等，而这些指令，在当前镜像构建时并不会被执行。只有当以当前镜像为基础镜像，去构建下一级镜像的时候才会被执行。 Dockerfile 中的其它指令都是为了定制当前镜像而准备的，唯有 ONBUILD 是为了帮助别人定制自己而准备的。 假设我们要制作 Node.js 所写的应用的镜像。我们都知道 Node.js 使用 npm 进行包管理，所有依赖、配置、启动信息等会放到 package.json 文件里。在拿到程序代码后，需要先进行 npm install 才可以获得所有需要的依赖。然后就可以通过 npm start 来启动应用。因此，一般来说会这样写 Dockerfile： FROMnode:slimRUN mkdir /appWORKDIR/appCOPY ./package.json /appRUN [ \"npm\", \"install\" ]COPY . /app/CMD [ \"npm\", \"start\" ] 把这个 Dockerfile 放到 Node.js 项目的根目录，构建好镜像后，就可以直接拿来启动容器运行。但是如果我们还有第二个 Node.js 项目也差不多呢？好吧，那就再把这个 Dockerfile 复制到第二个项目里。那如果有第三个项目呢？再复制么？文件的副本越多，版本控制就越困难，让我们继续看这样的场景维护的问题。 如果第一个 Node.js 项目在开发过程中，发现这个 Dockerfile 里存在问题，比如敲错字了、或者需要安装额外的包，然后开发人员修复了这个 Dockerfile，再次构建，问题解决。\u0008第一个项目没问题了，但是第二个项目呢？虽然最初 Dockerfile 是复制、粘贴自第一个项目的，但是并不会因为第一个项目修复了他们的 Dockerfile，而第二个项目的 Dockerfile 就会被自动修复。 那么我们可不可以做一个基础镜像，然后各个项目使用这个基础镜像呢？这样基础镜像更新，各个项目不用同步 Dockerfile 的变化，重新构建后就继承了基础镜像的更新？好吧，可以，让我们看看这样的结果。那么上面的这个 Dockerfile 就会变为： FROMnode:slimRUN mkdir /appWORKDIR/appCMD [ \"npm\", \"start\" ] 这里我们把项目相关的构建指令拿出来，放到子项目里去。假设这个基础镜像的名字为 my-node 的话，各个项目内的自己的 Dockerfile 就变为： FROMmy-nodeCOPY ./package.json /appRUN [ \"npm\", \"install\" ]COPY . /app/ 基础镜像变化后，各个项目都用这个 Dockerfile 重新构建镜像，会继承基础镜像的更新。 那么，问题解决了么？没有。准确说，只解决了一半。如果这个 Dockerfile 里面有些东西需要调整呢？比如 npm install 都需要加一些参数，那怎么办？这一行 RUN 是不可能放入基础镜像的，因为涉及到了当前项目的 ./package.json，难道又要一个个修改么？所以说，这样制作基础镜像，只解决了原来的 Dockerfile 的前4条指令的变化问题，而后面三条指令的变化则完全没办法处理。 ONBUILD 可以解决这个问题。让我们用 ONBUILD 重新写一下基础镜像的 Dockerfile: FROMnode:slimRUN mkdir /appWORKDIR/appONBUILD COPY ./package.json /appONBUILD RUN [ \"npm\", \"install\" ]ONBUILD COPY . /app/CMD [ \"npm\", \"start\" ] 这次我们回到原始的 Dockerfile，但是这次将项目相关的指令加上 ONBUILD，这样在构建基础镜像的时候，这三行并不会被执行。然后各个项目的 Dockerfile 就变成了简单地： FROMmy-node 是的，只有这么一行。当在各个项目目录中，用这个只有一行的 Dockerfile 构建镜像时，之前基础镜像的那三行 ONBUILD 就会开始执行，成功的将当前项目的代码复制进镜像、并且针对本项目执行 npm install，生成应用镜像。 LABEL 为镜像添加元数据 LABEL 指令用来给镜像以键值对的形式添加一些元数据（metadata）。 LABEL \u003ckey\u003e=\u003cvalue\u003e \u003ckey\u003e=\u003cvalue\u003e \u003ckey\u003e=\u003cvalue\u003e ... 我们还可以用一些标签来申明镜像的作者、文档地址等： LABEL org.opencontainers.image.authors=\"yeasy\"LABEL org.opencontainers.image.documentation=\"https://yeasy.gitbooks.io\" 具体可以参考 https://github.com/opencontainers/image-spec/blob/master/annotations.md ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:17:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"SHELL 指令 格式：SHELL [\"executable\", \"parameters\"] SHELL` 指令可以指定 `RUN` `ENTRYPOINT` `CMD` 指令的 shell，Linux 中默认为 `[\"/bin/sh\", \"-c\"] SHELL [\"/bin/sh\", \"-c\"]RUN lll ; lsSHELL [\"/bin/sh\", \"-cex\"]RUN lll ; ls 两个 RUN 运行同一命令，第二个 RUN 运行的命令会打印出每条命令并当遇到错误时退出。 当 ENTRYPOINT CMD 以 shell 格式指定时，SHELL 指令所指定的 shell 也会成为这两个指令的 shell SHELL [\"/bin/sh\", \"-cex\"]# /bin/sh -cex \"nginx\"ENTRYPOINT nginx SHELL [\"/bin/sh\", \"-cex\"]# /bin/sh -cex \"nginx\"CMD nginx 参考文档 Dockerfie 官方文档：https://docs.docker.com/engine/reference/builder/ Dockerfile 最佳实践文档：https://docs.docker.com/develop/develop-images/dockerfile_best-practices/ Docker 官方镜像 Dockerfile：https://github.com/docker-library/docs docker优质博客教程 https://yeasy.gitbook.io/docker_practice/image ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:18:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"Docker容器运行时引擎-runC分析","date":"212111-08-2151","objectID":"/runc_analyse/","tags":["docker"],"title":"docker - Docker容器运行时引擎-runC分析","uri":"/runc_analyse/"},{"categories":["docker"],"content":"Docker、containerd、runC、Kubernetes ","date":"212111-08-2151","objectID":"/runc_analyse/:1:0","tags":["docker"],"title":"docker - Docker容器运行时引擎-runC分析","uri":"/runc_analyse/"},{"categories":["docker"],"content":"从Docker到containerd、runC Docker 是一个开源工具，它可以将你的应用打包成一个标准格式的镜像，并且以容器的方式运行。Docker 容器将一系列软件包装在一个完整的文件系统中（代码、运行时工具、系统工具、系统依赖…)，保证了容器内应用程序运行环境的稳定性，不会被容器外的系统环境所影响。 2013年Docker宣布开源，初始阶段的Docker容器化基于LXC实现，镜像分层文件系统基于AUFS，核心功能都是基于当时的现成技术组装实现，但由于其工具生态完善、公共镜像仓库、组建重用、多版本支持、以应用为中心的容器封装等特性，这也使得Docker火速地流行了起来。 可以参考Docker相关的FAQ：https://docs.docker.com/engine/faq 在2014年开源了基于Golang开发的libcontainer，越过LXC直接操作系统内核模块，不必依赖LXC来提供容器化隔离能力了，自身架构也不断发展拆分成了如下几个模块： 2015年，Docker联合多家公司制定了开放容器交互标准（Open Container Initiative），即OCI；制定了关于容器相关的规范，包含运行时标准（runtime-spec）、容器镜像标准（image-spec）和镜像分发标准（distribution-spec）。 2016年，Docker为了符合OCI标准，将原本libcontainer模块独立出来，封装重构成runC项目，捐献给了Linux基金会管理，而后为了能够兼容所有符合标准的OCI Runtime实现，Docker重构了Docker Daemon子系统，将其中与运行时交互的部分抽象为containerd项目捐献给了CNCF基金会管理，内部为每个容器运行时创建一个containerd-shim适配进程，与runC搭配工作。 ","date":"212111-08-2151","objectID":"/runc_analyse/:1:1","tags":["docker"],"title":"docker - Docker容器运行时引擎-runC分析","uri":"/runc_analyse/"},{"categories":["docker"],"content":"Docker Daemon、containerd与runC containerd提供了镜像管理（镜像、元信息等）、容器运行（调用最终运行时组件执行）的功能，向上为 Docker Daemon 提供了 gRPC 接口，使得 Docker Daemon 屏蔽下面的结构变化，确保原有接口向下兼容；向下运行容器由符合OCI规范的容器支持，如runC，使得引擎可以独立升级。 ","date":"212111-08-2151","objectID":"/runc_analyse/:1:2","tags":["docker"],"title":"docker - Docker容器运行时引擎-runC分析","uri":"/runc_analyse/"},{"categories":["docker"],"content":"Kubernetes与Docker Kubernetes 是一个自动化部署，缩放，以及容器化管理应用程序的开源系统，前身是Google内部运行多年的集群管理系统Borg。 2014年Google使用Golang完全重写后开源，诞生后备受云计算相关的业界巨头追捧，成为云原生时代的操作系统，让复杂软件在云计算下获得韧性（Resilience）、弹性（Elasticity）、可观测性（Observability）的最佳路径。 2015年Kubernetes发布了第一个正式版本，Google宣布与Linux基金会共同筹建云原生基金会（Cloud Native Computing Foundation，CNCF），并且将Kubernetes托管到CNCF，成为其第一个项目。 随着Kubernetes的火速流行，自身的架构也在渐进演化，演进路线如下： Kubernetes 1.5之前与Docker是强绑定的关系，Kubernetes 管理容器的方式都是通过内部的DockerManager向Docker Engine发送指令（HTTP方式），通过Docker来操作镜像；调用链路如下所示 Kubernetes Master → kubelet → DockerManager → Docker Engine → containerd → runC Kubernetes 1.5版本开始引入容器运行时接口（Container Runtime Interface，CRI），CRI为定义容器运行时应该如何接入到kubelet的规范标准，内部的DockerManager也被KubeGenericRuntimeManager替代，已提供更通用的实现；kubelet与KubeGenericRuntimeManager之间通过gRPC协议通信。由于Docker不能直接支持CRI，所以Kubernetes提供了DockerShim服务作为Docker与CRI的适配层，实现了DockerManager的功能，演化到这里Docker Engine对Kubernetes来说只是一项默认依赖；调用链路如下所示： Kubernetes Master → kubelet → KubeGenericRuntimeManager → DockerShim → Docker Engine → containerd → runC 2017年，由Google、RedHat、Intel、SUSE、IBM联合发起的CRI-O（Container Runtime Interface Orchestrator）项目发布了首个正式版本。CRI-O是完全遵循CRI规范进行实现的，而且可以支持所有符合OCI运行时标准的容器引擎，默认使用runC搭配工作也可换成其他OCI运行时。此时Kubernetes可以选择使用CRI-O、cri-containerd、DockerShim作为CRI实现，演化到这里Docker Engine对Kubernetes来说只是一个可选项；调用链路如下所示： Kubernetes Master → kubelet → KubeGenericRuntimeManager → CRI-O→ runC 2018年，由Docker捐献给CNCF的containerd发布了1.1版，与1.0的区别是完全支持CRI标准，意味着原本用作CRI适配器的cri-containerd不再需要了，Kubernetes也在1.10版本宣布开始支持containerd 1.1，演化到这里Kubernetes已经可以完全脱开Docker Engine；调用链路如下所示： Kubernetes Master → kubelet → KubeGenericRuntimeManager → containerd → runC 2020年，Kubernetes官方宣布从1.20版本起放弃对Docker的支持，即不在维护DockerShim这一Docker与CRI的适配层；往后CRI适配器可以选择CRI-O或者containerd。 ","date":"212111-08-2151","objectID":"/runc_analyse/:1:3","tags":["docker"],"title":"docker - Docker容器运行时引擎-runC分析","uri":"/runc_analyse/"},{"categories":["docker"],"content":"容器运行时-runC分析 runc是一个CLI工具，用于根据OCI规范生成和运行容器，这里会通过runC源码简单分析其运行原理与流程。runC可以说是Docker容器中最为核心的部分，容器的创建，运行，销毁等等操作最终都将通过调用runC完成。 ","date":"212111-08-2151","objectID":"/runc_analyse/:2:0","tags":["docker"],"title":"docker - Docker容器运行时引擎-runC分析","uri":"/runc_analyse/"},{"categories":["docker"],"content":"OCI标准 image spec OCI 容器镜像标准包含如下几个模块： config.json：包含容器必需的元信息 rootfs：容器root文件系统，保存在一个文件夹里 config.json包含的元素如下所示，具体可以参考runtime-spec相关文档 ociVersion：指定OCI容器的版本号 root：配置容器的root文件系统 process：配置容器进程信息 hostname：配置容器的主机名 mounts：配置额外的挂载点 hook：配置容器生命周期触发钩子 runtime spec OCI 容器运行时标准主要是指定容器的运行状态，和 runtime 需要提供的命令，如下状态流转图所示： 基于busybox创建OCI runtime bundle相关文件 docker pull busybox mkdir mycontainer \u0026\u0026 cd mycontainer mkdir rootfs # 创建rootfs，基于busybox镜像 docker export $(docker create busybox) | tar -C rootfs -xvf - # 根据OCI规范来生成配置文件 runc spec 文件目录如下所示 └── mycontainer ├── config.json └── rootfs ├── bin ├── dev ├── etc ├── home ├── proc ├── root ├── sys ├── tmp ├── usr └── var ","date":"212111-08-2151","objectID":"/runc_analyse/:2:1","tags":["docker"],"title":"docker - Docker容器运行时引擎-runC分析","uri":"/runc_analyse/"},{"categories":["docker"],"content":"runC相关命令 通过runc -h可以看到runc支持的命令如下所示： checkpoint checkpoint正在运行的容器，配合restore使用 create 创建容器 delete 删除容器及容器所拥有的任何资源 events 显示容器事件，如OOM通知，cpu，内存和IO使用情况统计信息 exec 在容器内执行新的进程 init 初始化namespace并启动进程 kill 发送指定的kill信号（默认值：SIGTERM）到容器的init进程 list 列出容器 pause 暂停容器内的所有进程 ps 显示在容器内运行的进程 restore 从之前的checkpoint还原容器 resume 恢复容器内暂停的所有进程 run 创建并运行容器 spec 创建specification文件 start 在创建的容器中执行用户定义的进程 state 输出容器的状态 update 更新容器资源约束，如cgroup参数 实践操作： # 1. 修改config文件中的process元素的terminal与args \"process\": { \"terminal\": false, \"args\": [ \"/bin/sleep\", \"3600\" ] # 2. runc create创建容器（此时状态为created,容器内进程为init进程） sudo runc create mycontainer # 3. runc start启动容器（此时状态为running，容器内进程为我们指定的进程） sudo runc start mycontainer # 4. runc 暂停/恢复容器（暂停时状态为pause） sudo runc pause mycontainer sudo runc resume mycontainer # 5. runc kill关闭容器（此时状态为stopped） sudo runc kill mycontainer # 6. runc delete删除容器 sudo runc delete mycontainer # 7. runc run组合命令，组合了runc create创建，runc start启动以及在退出之后runc delete的删除命令 sudo runc run mycontainer ","date":"212111-08-2151","objectID":"/runc_analyse/:2:2","tags":["docker"],"title":"docker - Docker容器运行时引擎-runC分析","uri":"/runc_analyse/"},{"categories":["docker"],"content":"runC-run runC源码在github runc项目可以获取到，项目结构如下所示，其中如create.go、delete.go等文件为runC命令的实现；libcontainer目录为核心包（参考上文runC的由来），可见runC本质上是对libcontainer的一层封装，在libcontainer上加了一层OCI的适配操作与hook；main.go是入口文件，使用 github.com/urfave/cli 库进行命令行解析与执行函数： $ tree -L 1 -F --dirsfirst . ├── contrib/ ├── docs/ ├── libcontainer/ ├── man/ ├── script/ ├── tests/ ├── types/ ├── vendor/ ├── checkpoint.go ├── create.go ├── delete.go ├── events.go ├── exec.go ├── init.go ├── kill.go ├── list.go ├── main.go ├── pause.go ├── ps.go ├── restore.go ├── run.go ├── spec.go ├── start.go ├── state.go ... 通过run.go我们可以看到runCommand相关的执行逻辑： Action: func(context *cli.Context) error { if err := checkArgs(context, 1, exactArgs); err != nil { return err } if err := revisePidFile(context); err != nil { return err } spec, err := setupSpec(context) if err != nil { return err } status, err := startContainer(context, spec, CT_ACT_RUN, nil) if err == nil { os.Exit(status) } return err } 这里主要涉及四步操作： 参数校验 若指定了pidFile，进行路径转换 读取 config.json 文件转换成 spec 结构对象 根据配置启动容器 第四步是操作调用了utils_linux.go下startContainer方法进行容器创建、运行，具体实现如下： func startContainer(context *cli.Context, spec *specs.Spec, action CtAct, criuOpts *libcontainer.CriuOpts) (int, error) { id := context.Args().First() ... container, err := createContainer(context, id, spec) if err != nil { return -1, err } ... r := \u0026runner{ enableSubreaper: !context.Bool(\"no-subreaper\"), shouldDestroy: true, container: container, listenFDs: listenFDs, notifySocket: notifySocket, consoleSocket: context.String(\"console-socket\"), detach: context.Bool(\"detach\"), pidFile: context.String(\"pid-file\"), preserveFDs: context.Int(\"preserve-fds\"), action: action, criuOpts: criuOpts, init: true, logLevel: logLevel, } return r.run(spec.Process) } 通过调用createContainer创建出逻辑容器（包含保存了namespace、cgroups、mounts…容器配置） 创建runner对象，通过调用runner的run方法运行容器进程，并进行对应的设置 Factory创建逻辑容器 func createContainer(context *cli.Context, id string, spec *specs.Spec) (libcontainer.Container, error) { ... config, err := specconv.CreateLibcontainerConfig(\u0026specconv.CreateOpts{ CgroupName: id, UseSystemdCgroup: context.GlobalBool(\"systemd-cgroup\"), NoPivotRoot: context.Bool(\"no-pivot\"), NoNewKeyring: context.Bool(\"no-new-keyring\"), Spec: spec, RootlessEUID: os.Geteuid() != 0, RootlessCgroups: rootlessCg, }) if err != nil { return nil, err } factory, err := loadFactory(context) if err != nil { return nil, err } return factory.Create(id, config) } 通过specconv.CreateLibcontainerConfig把spec转换成libcontainer内部config对象 通过factory 模式获取对应平台的factory实现（目前仅linux平台支持）,如libcontainer/factory_linux.go，并调用其Create方法构建返回libcontainer.Container 对象（包含容器命令的实现，如Run、Start、State…） 注意libcontainer/factory_linux.goNew方法的InitPath为/proc/self/exe,InitArgs为init func (l *LinuxFactory) Create(id string, config *configs.Config) (Container, error) { ... c := \u0026linuxContainer{ id: id, root: containerRoot, config: config, initPath: l.InitPath, initArgs: l.InitArgs, criuPath: l.CriuPath, newuidmapPath: l.NewuidmapPath, newgidmapPath: l.NewgidmapPath, cgroupManager: l.NewCgroupsManager(config.Cgroups, nil), } if l.NewIntelRdtManager != nil { c.intelRdtManager = l.NewIntelRdtManager(config, id, \"\") } c.state = \u0026stoppedState{c: c} return c, nil } func New(root string, options ...func(*LinuxFactory) error) (Factory, error) { ... l := \u0026LinuxFactory{ Root: root, InitPath: \"/proc/self/exe\", InitArgs: []string{os.Args[0], \"init\"}, Validator: validate.New(), CriuPath: \"criu\", } Cgroupfs(l) for _, opt := range options { if opt == nil { continue } if err := opt(l); err != nil { return nil, err } } return l, nil } Runner运行容器-container_linux.start runner的run方法会根据OCI specs.Process生成 libcontainer.Process逻辑process对象（包含进程的命令、参数、环境变量、用户…）；根据对应的command如run会调用container的Run方法，在linux平台下最终会调用container_linux的start方法与exec方法，start方法如下所示： func (c *linuxContainer) start(process *Process) (retErr error) { parent, err := c.newParentProcess(process) if err != nil { return newSystemErrorWithCause(err, \"creating new parent process\") } // if err := parent.start(); err != nil { return ","date":"212111-08-2151","objectID":"/runc_analyse/:2:3","tags":["docker"],"title":"docker - Docker容器运行时引擎-runC分析","uri":"/runc_analyse/"},{"categories":["docker"],"content":"runC-init 上文分析runc run流程，在最后通过/proc/self/exe - init启动容器init进程执行init command操作，这里直接追溯到init操作的核心方法standard_init_linux.go下的Init方法，如下所示： func (l *linuxStandardInit) Init() error { //配置容器网络与路由 if err := setupNetwork(l.config); err != nil { return err } if err := setupRoute(l.config.Config); err != nil { return err } //设置rootfs，挂载文件系统，chroot if err := prepareRootfs(l.pipe, l.config); err != nil { return err } //配置console, hostname, apparmor, sysctl，seccomp，user namespace... ... //同步父进程 init进程准备执行exec if err := syncParentReady(l.pipe); err != nil { return errors.Wrap(err, \"sync ready\") } ... //已完成初始化，关闭管道 l.pipe.Close() ... //向exec.fifo管道写入数据，阻塞到exec执行，读取管道中的数据 fd, err := unix.Open(\"/proc/self/fd/\"+strconv.Itoa(l.fifoFd), unix.O_WRONLY|unix.O_CLOEXEC, 0) if err != nil { return newSystemErrorWithCause(err, \"open exec fifo\") } if _, err := unix.Write(fd, []byte(\"0\")); err != nil { return newSystemErrorWithCause(err, \"write 0 exec fifo\") } //调用Exec系统命令，执行用户进程 if err := unix.Exec(name, l.config.Args[0:], os.Environ()); err != nil { return newSystemErrorWithCause(err, \"exec user process\") } return nil } 这里涉及如下几步操作： 配置容器网络与路由 设置rootfs，挂载文件系统，chroot 配置console, hostname, apparmor, sysctl，seccomp，user namespace 同步父进程，让父进程执行hook等操作 调用Exec系统命令，执行用户进程，此时容器的init进程会替换成用户进程 最后，在Init执行之前，会被/runc/libcontainer/nsenter劫持，在Go的runtime之前执行对应的C代码，从init管道中读取容器的配置，设置namespace，调用setns系统调用，将容器init进程加入到对应的namespace中。 ","date":"212111-08-2151","objectID":"/runc_analyse/:2:4","tags":["docker"],"title":"docker - Docker容器运行时引擎-runC分析","uri":"/runc_analyse/"},{"categories":["docker"],"content":"runC-run-init-运行流程 runC-run运行流程 runC-init运行流程 ","date":"212111-08-2151","objectID":"/runc_analyse/:2:5","tags":["docker"],"title":"docker - Docker容器运行时引擎-runC分析","uri":"/runc_analyse/"},{"categories":["docker"],"content":"docker 怎么走代理.","date":"212111-08-2151","objectID":"/docker_proxy/","tags":["docker"],"title":"docker - 代理","uri":"/docker_proxy/"},{"categories":["docker"],"content":"docker三种网络代理模式 有时因为网络原因，比如公司NAT，或其它啥的，需要使用代理。 Docker的代理配置，略显复杂，因为有三种场景。 但基本原理都是一致的，都是利用Linux的http_proxy等环境变量。 ","date":"212111-08-2151","objectID":"/docker_proxy/:0:0","tags":["docker"],"title":"docker - 代理","uri":"/docker_proxy/"},{"categories":["docker"],"content":"dockerd代理 在执行docker pull时，是由守护进程dockerd来执行。 因此，代理需要配在dockerd的环境中。 而这个环境，则是受systemd所管控，因此实际是systemd的配置。 sudo mkdir -p /etc/systemd/system/docker.service.d sudo touch /etc/systemd/system/docker.service.d/proxy.conf 在这个proxy.conf文件（可以是任意*.conf的形式）中，添加以下内容： [Service] Environment=\"HTTP_PROXY=http://proxy.example.com:8080/\" Environment=\"HTTPS_PROXY=http://proxy.example.com:8080/\" Environment=\"NO_PROXY=localhost,127.0.0.1,.example.com\" 其中，proxy.example.com:8080要换成可用的免密代理。 通常使用cntlm在本机自建免密代理，去对接公司的代理。 可参考《Linux下安装配置Cntlm代理》。 ","date":"212111-08-2151","objectID":"/docker_proxy/:1:0","tags":["docker"],"title":"docker - 代理","uri":"/docker_proxy/"},{"categories":["docker"],"content":"Container代理 在容器运行阶段，如果需要代理上网，则需要配置~/.docker/config.json。 以下配置，只在Docker 17.07及以上版本生效。 { \"proxies\": { \"default\": { \"httpProxy\": \"http://proxy.example.com:8080\", \"httpsProxy\": \"http://proxy.example.com:8080\", \"noProxy\": \"localhost,127.0.0.1,.example.com\" } } } 这个是用户级的配置，除了proxies，docker login等相关信息也会在其中。 而且还可以配置信息展示的格式、插件参数等。 此外，容器的网络代理，也可以直接在其运行时通过-e注入http_proxy等环境变量。 这两种方法分别适合不同场景。 config.json非常方便，默认在所有配置修改后启动的容器生效，适合个人开发环境。 在CI/CD的自动构建环境、或者实际上线运行的环境中，这种方法就不太合适，用-e注入这种显式配置会更好，减轻对构建、部署环境的依赖。 当然，在这些环境中，最好用良好的设计避免配置代理上网。 ","date":"212111-08-2151","objectID":"/docker_proxy/:2:0","tags":["docker"],"title":"docker - 代理","uri":"/docker_proxy/"},{"categories":["docker"],"content":"docker build代理 虽然docker build的本质，也是启动一个容器，但是环境会略有不同，用户级配置无效。 在构建时，需要注入http_proxy等参数。 docker build . \\ --build-arg \"HTTP_PROXY=http://proxy.example.com:8080/\" \\ --build-arg \"HTTPS_PROXY=http://proxy.example.com:8080/\" \\ --build-arg \"NO_PROXY=localhost,127.0.0.1,.example.com\" \\ -t your/image:tag 注意：无论是docker run还是docker build，默认是网络隔绝的。 如果代理使用的是localhost:3128这类，则会无效。 这类仅限本地的代理，必须加上--network host才能正常使用。 而一般则需要配置代理的外部IP，而且代理本身要开启gateway模式。 ","date":"212111-08-2151","objectID":"/docker_proxy/:3:0","tags":["docker"],"title":"docker - 代理","uri":"/docker_proxy/"},{"categories":["docker"],"content":"重启生效 代理配置完成后，reboot重启当然可以生效，但不重启也行。 docker build代理是在执行前设置的，所以修改后，下次执行立即生效。 Container代理的修改也是立即生效的，但是只针对以后启动的Container，对已经启动的Container无效。 dockerd代理的修改比较特殊，它实际上是改systemd的配置，因此需要重载systemd并重启dockerd才能生效。 sudo systemctl daemon-reload sudo systemctl restart docker ","date":"212111-08-2151","objectID":"/docker_proxy/:4:0","tags":["docker"],"title":"docker - 代理","uri":"/docker_proxy/"},{"categories":["docker"],"content":"参考 ¶ Control Docker with systemd | Docker Documentation Configure Docker to use a proxy server | Docker Documentation Use the Docker command line | Docker Documentation ","date":"212111-08-2151","objectID":"/docker_proxy/:5:0","tags":["docker"],"title":"docker - 代理","uri":"/docker_proxy/"},{"categories":["docker"],"content":"docker 常用指令介绍.","date":"212111-08-2151","objectID":"/docker_cmd/","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"docker 常用指令 ","date":"212111-08-2151","objectID":"/docker_cmd/:0:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"验证是否安装成功 $ docker version # 查看docker版本 # 或者 $ docker info # 查看docker系统的信息 ","date":"212111-08-2151","objectID":"/docker_cmd/:1:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"启动docker $ systemctl start docker ","date":"212111-08-2151","objectID":"/docker_cmd/:2:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"查看docker 镜像 $ docker image ls 或者 $ docker images ","date":"212111-08-2151","objectID":"/docker_cmd/:3:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"拉取镜像 $ docker pull \u003cimage\u003e:\u003ctag\u003e ","date":"212111-08-2151","objectID":"/docker_cmd/:4:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"删除镜像 $ docker rmi [image-id] # 删除镜像 $ docker rmi $(docker images -q) # 删除所有镜像 $ docker rmi $(sudo docker images --filter \"dangling=true\" -q --no-trunc) # 删除无用镜像 ","date":"212111-08-2151","objectID":"/docker_cmd/:5:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"列出正在运行的容器(containers) $ docker ps ","date":"212111-08-2151","objectID":"/docker_cmd/:6:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"列出所有的容器(包括不运行的容器) $ docker ps -a ","date":"212111-08-2151","objectID":"/docker_cmd/:7:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"容器相关操作 docker exec -it # 容器ID sh 进入容器 docker stop ‘container’ # 停止一个正在运行的容器，‘container’可以是容器ID或名称 docker start ‘container’ # 启动一个已经停止的容器 docker restart ‘container’ # 重启容器 docker rm ‘container’ # 删除容器 docker run -i -t -p :80 LAMP /bin/bash # 运行容器并做http端口转发 docker exec -it ‘container’ /bin/bash # 进入ubuntu类容器的bash docker exec -it \u003ccontainer\u003e /bin/sh # 进入alpine类容器的sh docker rm docker ps -a -q # 删除所有已经停止的容器 docker kill $(docker ps -a -q) # 杀死所有正在运行的容器，$()功能同`` ","date":"212111-08-2151","objectID":"/docker_cmd/:8:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"查看容器日志 查看容器日志 docker logs \u003cid/container_name\u003e 实时查看日志输出 docker logs -f \u003cid/container_name\u003e (类似 tail -f) (带上时间戳-t） ","date":"212111-08-2151","objectID":"/docker_cmd/:9:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"docker镜像的导出和导入 $ docker save -o \u003c保存路径\u003e \u003c镜像名称:标签\u003e # 导出镜像$ docker save -o ./ubuntu18.tar ubuntu:18.04 # 导出镜像$ docker load --input ./ubuntu18.tar # 导入镜像 ","date":"212111-08-2151","objectID":"/docker_cmd/:10:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"制作自己的镜像 **docker commit :**从容器创建一个新的镜像。 $ docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]] Dockerfile:命令用于使用 Dockerfile 创建镜像。 $ docker build [OPTIONS] PATH | URL | -例如：$ docker build -t ztc/ubuntu:v1 . ","date":"212111-08-2151","objectID":"/docker_cmd/:11:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"从容器中导出文件 # 从主机复制到容器 $ docker cp host_path containerID:container_path # 从容器复制到主机 $ docker cp containerID:container_path host_path ","date":"212111-08-2151","objectID":"/docker_cmd/:12:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"容器启动参数 $ docker run ：创建一个新的容器并运行一个命令 语法 docker run [OPTIONS] IMAGE [COMMAND] [ARG...] OPTIONS说明： -a stdin: 指定标准输入输出内容类型，可选 STDIN/STDOUT/STDERR 三项； -d: 后台运行容器，并返回容器ID； -i: 以交互模式运行容器，通常与 -t 同时使用； -P: 随机端口映射，容器内部端口随机映射到主机的端口 -p: 指定端口映射，格式为：主机(宿主)端口:容器端口 -t: 为容器重新分配一个伪输入终端，通常与 -i 同时使用； --name=\"nginx-lb\": 为容器指定一个名称； --dns 8.8.8.8: 指定容器使用的DNS服务器，默认和宿主一致； --dns-search example.com: 指定容器DNS搜索域名，默认和宿主一致； -h \"mars\": 指定容器的hostname； -e username=\"ritchie\": 设置环境变量； --env-file=[]: 从指定文件读入环境变量； --cpuset=\"0-2\" or --cpuset=\"0,1,2\": 绑定容器到指定CPU运行； -m :设置容器使用内存最大值； --net=\"bridge\": 指定容器的网络连接类型，支持 bridge/host/none/container: 四种类型； --link=[]: 添加链接到另一个容器； --expose=[]: 开放一个端口或一组端口； --volume , -v: 绑定一个卷 # 实例 # 使用docker镜像nginx:latest以后台模式启动一个容器,并将容器命名为mynginx。 $ docker run --name mynginx -d nginx:latest #使用镜像nginx:latest以后台模式启动一个容器,并将容器的80端口映射到主机随机端口。 $ docker run -P -d nginx:latest #使用镜像 nginx:latest，以后台模式启动一个容器,将容器的 80 端口映射到主机的 80 端口,主机的目录 /data 映射到容器的 /data。 $docker run -p 80:80 -v /data:/data -d nginx:latest #绑定容器的 8080 端口，并将其映射到本地主机 127.0.0.1 的 80 端口上。 $ docker run -p 127.0.0.1:80:8080/tcp ubuntu bash #使用镜像nginx:latest以交互模式启动一个容器,在容器内执行/bin/bash命令。 $ @ztc:~$ docker run -it nginx:latest /bin/bash ","date":"212111-08-2151","objectID":"/docker_cmd/:13:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"镜像仓库操作 docker login : 登陆到一个Docker镜像仓库，如果未指定镜像仓库地址，默认为官方仓库 Docker Hub docker logout : 登出一个Docker镜像仓库，如果未指定镜像仓库地址，默认为官方仓库 Docker Hub $ docker login -u 用户名 -p 密码$ docker logout ","date":"212111-08-2151","objectID":"/docker_cmd/:14:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["istio"],"content":"istio 多集群管控流程介绍.","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"多云管理 Multi-cluster service mesh 多集群服务网格，配置工作负载来路由跨集群的流量。所有这些都符合应用Istio配置，如' VirtualServices ‘， ' DestinationRules ‘， ' Sidecar ‘等。 Mesh Federation 网格联邦，表示公开并支持两个独立服务网格的工作负载的通信 对于网格联邦，您可以查看Istio文档multiple mesh 或一个名为Gloo mesh的开源项目，以帮助自动化配置以支持它。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:0:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"多集群服务网格 多集群服务网格需要跨集群发现、连接和共同信任 跨集群工作负载发现 -控制平面必须发现对等集群中的工作负载，以便配置服务代理 (例如，集群的API server 必须可以访问对面集群中的Istio控制平面)。 跨集群工作负载连接性 -工作负载之间必须具有连接性。除非您可以初始化到工作负载端点的连接，否则对工作负载端点的感知是没有用的。 集群之间的共同信任 -跨集群的工作负载必须相互认证以使Istio的安全特性成为可能的PeerAuthentication和AuthorizationPolicy。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:1:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"多集群部署模型 集群分类： 主集群 （Primary Cluster） : 安装Istio控制平面的集群 远端集群（Remote Cluster）: 安装控制平面的远端集群 部署模型： Primary-Remote(共享控制平面) Primary-Primary(复用控制平面) External控制平面 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:2:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"Primary-Remote 部署模型 Primary-Remote部署模型有一个管理网格的单一控制平面，因此，它经常被称为单一控制平面或共享控制平面部署模型。 这种模型使用更少的资源，然而，主集群的中断会影响整个网格，因此它的可用性很低。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:2:1","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"Primary-Primary部署模型 Primary-Primary 部署模型有多个控制平面，这确保了更高的可用性，因为中断的范围仅限于发生中断的集群，但也需要更多的资源。我们将此模型称为复制控制平面部署模型。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:2:2","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"External控制平面 External控制平面 为所有集群远程到控制平面的部署模型。这种部署模型使云提供商能够将Istio作为托管服务提供。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:2:3","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"多集群下的服务发现 Istio的控制平面需要与Kubernetes API服务器通信，以收集相关信息来配置服务代理，比如 services 和 endpoints。 但是对 kubernetes 的 api server 的访问，存在安全的问题，因为您可以查找资源细节、查询敏感信息，并更新或删除资源，从而使集群处于糟糕且不可逆的状态。 可以参考 RBAC 来解决这个问题 为istiod 提供remote cluster的 service account token , istiod使用 这个token 对 remote cluster 进行身份验证，并发现运行在其中的工作负载。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:3:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"跨集群工作负载连接 两种情况： 集群处于同一网络平面，工作负载使用 IP 连接，天生满足条件。 集群处于不同网络，必须使用位于 网格边缘的特殊 istio 入口网关来代理跨集群网络 在“多网络”网格中桥接集群的入口网关 称为 “东西向网关”， 东西向网关将 作为一个 反向代理，将请求发送到个字集群中的工作负载。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:4:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"集群之间的相互信任 拥有公共信任可以确保不同集群的工作负载可以相互进行身份验证。 实现集群之间相互信任的方法： 第一种方法是使用我们称为Plug-In CA Certificates的东西，它是由一个公共根证书颁发机构颁发的用户定义的证书。 第二种方法是集成一个外部证书颁发机构，两个集群都使用它来签署证书。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:5:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"Plug-in CA Certificates 默认情况下，Istio CA生成自签名根证书和密钥，并使用它们对工作负载证书进行签名。要保护根CA密匙，应该使用在安全机器上脱机运行的根CA，并使用根CA向在每个集群中运行的Istio CA颁发中间证书。 Istio CA可以使用管理员指定的证书和密钥对工作负载证书进行签名，并将管理员指定的根证书作为信任根分发到工作负载。 因为，不是让 istio 生成中间证书颁发机构，而是通过在Istio安装名称空间上提供秘密证书来指定要使用的证书（就是创建一个secret 持久化到 etcd 中）。您可以对两个集群都这样做，并使用由公共根CA签名的中间CA。 上图为 使用由同一根签名的中间CA证书 但是上述方法存在安全风险： 如果暴露中间证书颁发机构，攻击者可以使用这些签名来签署证书，然后这些证书将被信任，直到检测到暴露和中间CA的证书被撤销。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:5:1","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"External certificate authority integration Cert-manager (待补充，见原文) Custom development （待补充，见原文） 多集群、多网络、多控制平面服务网格概述 West-cluster: Kubernetes cluster，该cluster在us-west区域拥有私有网络。我们将运行webapp服务。 East-cluster: Kubernetes cluster，它在us-east区域拥有自己的私有网络。在那里我们将运行catalog服务。 本文将建立一个一个多集群、多网络、多控制平面的服务网格，使用东西网关连接网络，并使用primary-primary部署模型 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:5:2","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"第一步 创建集群 创建两个 kubernetes 集群，每个集群都位于不同的网络上。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:6:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"第二步 配置 plug-in CA certificates 建立相互信任 Istio 默认会生成一个在 Istio-system namespace 下的 istio-ca-secret的 secret。 通过插入我们自己的证书颁发机构，可以覆盖这个 secret。 ​ cacert密钥由根CA的公钥和中间CA的公钥和私钥组成。根CA的私钥在集群外“安全”存储 创建一个cacerts secret，包含如下输入文件ca-cert.pem, ca-key.pem, root-cert.pem 和cert-chain.pem。需要注意的是创建出来的secret的名称必须是cacerts，这样才能被istio正确挂载。 ca-cert.pem - 中间CA的证书 ca-key.pem - 中间CA的私钥 root-cert.pem - 颁发中间CA的根CA的证书，用于验证由它颁发的任何中间CA颁发的证书 cert-chain.pem - 中间CA证书和根CA证书的连接，形成信任链 # 通过创建命名空间' istio-system '，然后将证书作为名为' cacerts '的秘密文件应用，在每个集群中配置中间CA。 # setting up certificates for the west-cluster （在 west-cluster 下操作） $ kubectl create namespace istio-system $ kubectl create secret generic cacerts -n istio-system \\ --from-file=ch12/certs/west-cluster/ca-cert.pem \\ --from-file=ch12/certs/west-cluster/ca-key.pem \\ --from-file=ch12/certs/root-cert.pem \\ --from-file=ch12/certs/west-cluster/cert-chain.pem # setting up certificates for the east-cluster (在 east-cluster 下操作) $ kubectl create namespace istio-system $ kubectl create secret generic cacerts -n istio-system \\ --from-file=ch12/certs/east-cluster/ca-cert.pem \\ --from-file=ch12/certs/east-cluster/ca-key.pem \\ --from-file=ch12/certs/root-cert.pem \\ --from-file=ch12/certs/east-cluster/cert-chain.pem ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:7:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"第三步 安装各个集群的控制面 继续安装Istio控制面，该控制面将挑选插件CA证书(即用户定义的中间证书)来签署工作负载证书。 标记集群的网络拓扑 # 标记 west-cluster的 网络拓扑 $ kubectl label namespace istio-system topology.istio.io/network=west-network # 标记 east-cluster的 网络拓扑 $ kubectl label namespace istio-system topology.istio.io/network=east-network 通过这些标签，Istio形成了对网络拓扑的理解，并使用它来决定如何配置工作负载。 部署控制面 部署 west 集群 # 方式一 $ istioctl install --set profile=demo \\ --set values.global.meshID=usmesh \\ --set values.global.multiCluster.clusterName=west-cluster \\ --set values.global.network=west-network # 方式二 使用 istioOperator 部署 apiVersion: install.istio.io/v1alpha1 metadata: name: istio-controlplane namespace: istio-system kind: IstioOperator spec: profile: demo components: egressGateways: - name: istio-egressgateway enabled: false values: global: meshID: usmesh # 属性' meshID '使我们能够识别这个安装属于哪个网格。 Istio提供了在集群中安装多个网格的选项，允许团队分别管理他们网格的操作。 multiCluster: clusterName: west-cluster network: west-network # 上述 yaml 使用如下命令安装： $ istioctl install -f xxx.yaml 使用同样的方式部署 east 集群， 和 west 集群的不同在于 clusterName 和 network 不同 apiVersion:install.istio.io/v1alpha1metadata:name:istio-controlplanenamespace:istio-systemkind:IstioOperatorspec:profile:democomponents:egressGateways:- name:istio-egressgatewayenabled:falsevalues:global:meshID:usmeshmultiCluster:clusterName:east-clusternetwork:east-network 在 west 和 east 集群 安装完控制面后，我们有了两个独立的网格，两个网格都运行着 istiod ，但是只发现本地的服务。当前网格的状态如下图所示： 当前网格缺乏跨集群工作负载发现和连接，会在 第四步 和 第五步 讨论。 此时可以在集群中创建一些工作负载，分别在 west 和 east 集群中，运行如下： # 在 west 集群安装应用和 网关 $ kubectl create ns istioinaction $ kubectl label namespace istioinaction istio-injection=enabled $ kubectl -n istioinaction apply -f ./webapp-deployment-svc.yaml $ kubectl -n istioinaction apply -f ./webapp-gw-vs.yaml $ kubectl -n istioinaction apply -f ./catalog-svc.yaml # 在 east 集群安装 catalog $ kubectl create ns istioinaction $ kubectl label namespace istioinaction istio-injection=enabled $ kubectl -n istioinaction apply -f ./catalog.yaml 测试用用例如上所示。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:8:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"第四步 启动跨集群的服务发现 为了从远程集群中查询信息，istio 需要使用 service account 进行身份验证。 istio 在安装时会创建一个名为 istio-reader-service-account 的 service account，具有可被另一个控制平面使用的最小权限集。 $ kubectl get sa -n istio-system NAME SECRETS AGE default 1 4d20h istio-egressgateway-service-account 1 4d20h istio-ingressgateway-service-account 1 4d20h istio-reader-service-account 1 4d20h istiod-service-account 1 4d20h 但是，我们需要使服务帐户令牌对对端的集群可用，以及证书来启动到远程集群的安全连接。 创建对端集群访问的secret # 在 east-cluster上执行如下： (创建时，务必指明集群名称 --name) $ istioctl x create-remote-secret --name=\"east-cluster\" # 得到如下结果 # This file is autogenerated, do not edit. apiVersion: v1 kind: Secret metadata: annotations: networking.istio.io/cluster: east-cluster creationTimestamp: null labels: istio/multiCluster: \"true\" name: istio-remote-secret-east-cluster namespace: istio-system stringData: east-cluster: | apiVersion: v1 clusters: # 包含集群地址和验证API服务器提供的连接的证书颁发机构数据的集群列表。 - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkF... server: https://10.20.144.83:6443 name: east-cluster contexts: # 上下文列表，每组一个用户和一个集群，这简化了切换集群(与我们的用例无关)。 - context: cluster: east-cluster user: east-cluster name: east-cluster current-context: east-cluster kind: Config preferences: {} users: # 一个列表，定义了包含要对API服务器进行身份验证的令牌的用户 - name: east-cluster user: token: eyJhbGciOiJSUzI1NiIsImtpZCI6.... --- # 该命令使用默认的 istio-reader-service-account service account 为远程集群访问创建secret。 # 上述的 secret 就是“kubectl”需要启动到Kubernetes API服务器的安全连接并对其进行身份验证的全部内容。 # 需要将 上述内容 手动 执行 kubectl apply -f xxx.yaml 应到到 west-cluster ⚠️ 注意 上述的内容，先在一个集群中(east-cluster)生成 secret 的 yaml ，然后将生成的 yaml 放到 remote 集群中(west-cluster) 中 执行 apply 一旦在 remote 集群中创建了secret，istiod就会获取它，并在新添加的 remote 集群中查询工作负载。通过查看日志可以更详细看到细节： # 在 remote 集群中(west-cluster) 查看 istiod 的日志 $ kubectl logs deploy/istiod -n istio-system | grep 'Adding cluster' 2021-10-08T08:47:32.408052Z info Adding cluster_id=east-cluster from secret=istio-system/istio-remote-secret-east-cluster # 通过日志可以验证集群是否初始化完成，并且west-cluster的控制面可以发现 east-cluster 的 workload 为了配置成 primary-primary 部署模式，同样要对 对端集群做同样的操作，使得 east-cluster 的控制面可以发现 west-cluster 的 workload。 # 在west-cluster 中执行如下： $ kubectl x create-remote-secret --name=\"west-cluster\" # 将生成的secret 放到 east-cluster中apply,配置 east-cluster查看west-cluster $ kubectl apply -f 刚刚生成的secret.yaml secret/istio-remote-secret-west-cluster created ❤️ 现在，双方的控制面都可以查询到对端集群的 workload，接下来要去设置跨集群连接。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:9:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"第五步 建立跨集群连接 不同内部网络(在我们的实例中是集群网络)之间的流量称为“东西流量” 将内部服务通过网关定向到内部网络的流量称为“南北向流量” 南北向流量管理：使用Ingress将Kubernetes中的应用暴露成对外提供的服务，针对这个对外暴露的服务可以实现灰度发布、A/B测试、全量发布、流量管理等。我们把这种流量管理称为南北向流量管理，也就是入口请求到集群服务的流量管理。 东西向流量管理：Istio还有侧重于集群服务网格之间的流量管理，我们把这类管理称为东西向流量管理。 对于不同云服务商的集群，或者网络并非对等连接的地方，istio 提供了东西向网关作为解决方案。 Istio 东西向网关 东西向网关的目标初了成为跨集群东西向流量的入口点之外，还在于使这对运营服务的团队透明。为了实现这一目标，它必须: 开启跨集群的细粒度流量管理 通过路由加密流量，实现负载之间的双向认证 istio 有两个特性： SNI cluster SNI Auto Passthrough 东西向网关配置 SNI cluster # cluster-east-eastwest-gateway.yamlapiVersion:install.istio.io/v1alpha1kind:IstioOperatormetadata:name:istio-eastwestgateway # 一定不能和前面的 istioOperator 资源相同，如果相同，将会覆盖之前的istioOpeatornamespace:istio-systemspec:profile:emptycomponents:ingressGateways:- name:istio-eastwestgatewaylabel:istio:eastwestgatewayapp:istio-eastwestgatewayenabled:truek8s:env:- name:ISTIO_META_ROUTER_MODE # SNI集群的配置是一个可选特性value:\"sni-dnat\"# 网关路由器模式设置为' snii -dnat '来启用, 自动配置SNI集群- name:ISTIO_META_REQUESTED_NETWORK_VIEW# The network to which traffic is routedvalue:east-networkservice:ports:# redacted for brevityvalues:global:meshID:usmeshmultiCluster:clusterName:east-cluster # 注意：这是 east-clusternetwork:east-network # 在 east 集群中执行如下，apply 上述的yaml $ istioctl install -y -f ./cluster-east-eastwest-gateway.yaml ✔ Ingress gateways installed ✔ Installation complete 安装了东西向网关后，可以查询网关的集群代理配置并将输出过滤为仅包含“catalog”文本的行，来验证SNI集群是否配置了。 # 在 east 集群中执行如下： $ istioctl pc clusters deploy/istio-eastwestgateway.istio-system \\ | grep catalog | awk '{printf \"CLUSTER: %s\\n\", $1}' CLUSTER: catalog.istioinaction.svc.cluster.local CLUSTER: outbound_.80_._.catalog.istioinaction.svc.cluster.local 如何使用SNI Auto Passthrough路由跨集群流量 SNI Auto Passthrough，顾名思义，不需要手动创建“VirtualServices”来路由允许的流量。 这是使用SNI集群完成的，这些集群使用“snii -dnat”路由器模式在东西网关中自动配置。 SNI Auto Passthrough 模式 使用 Istio Gateway 配置 apiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:cross-network-gatewaynamespace:istio-systemspec:selector:istio:eastwestgatewayservers:- port:number:15443name:tlsprotocol:TLStls:mode:AUTO_PASSTHROUGHhosts:- \"*.local\" # 把east-cluster的workload 暴露给 west-cluster # 在 east-cluster 上操作 $ kubectl apply -n istio-system -f 上述.yaml gateway.networking.istio.io/cross-network-gateway created 同样，需要在west-cluster上进行相似的操作 # west-cluster 环境中 # 首先部署 istioOperator 创建东西向网关， kubectl apply -f 下述yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: name: istio-eastwestgateway namespace: istio-system spec: profile: empty components: ingressGateways: - name: istio-eastwestgateway label: istio: eastwestgateway app: istio-eastwestgateway topology.istio.io/network: west-network enabled: true k8s: env: - name: ISTIO_META_ROUTER_MODE value: \"sni-dnat\" # The network to which traffic is routed - name: ISTIO_META_REQUESTED_NETWORK_VIEW value: west-network service: ports: - name: status-port port: 15021 targetPort: 15021 - name: mtls port: 15443 targetPort: 15443 - name: tcp-istiod port: 15012 targetPort: 15012 - name: tcp-webhook port: 15017 targetPort: 15017 values: global: meshID: usmesh multiCluster: clusterName: west-cluster # 注意：这是 west-cluster network: west-network # 然后将集群中的服务暴露给 east-cluster, kubectl apply -f 下述yaml (其实这个和在east-cluster中配置的yaml是一样) apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: cross-network-gateway namespace: istio-system spec: selector: istio: eastwestgateway servers: - port: number: 15443 name: tls protocol: TLS tls: mode: AUTO_PASSTHROUGH hosts: - \"*.local\" 使用 SNI 自动直通配置网关后，网关上的传入流量将使用 SNI 集群路由到预期的工作负载。 Istio 的控制平面侦听这些资源的创建并发现现在存在路由跨集群流量的路径。 因此，它使用新发现的端点更新所有工作负载。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:10:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"第六步 验证跨集群 workload discovery 之前的步骤中，我们已经准备好了测试用例，现在可以派上用场了。 测试用例描述： 在 west-cluster 中，部署了 webapp ，它有 deployment 和 service ，同时部署了 catalog，但是catalog只部署了service 在easy-cluster中，部署了catalog，它有 deployment 和 service 经过之前的部署，现在 east-cluster 的 workloads 已经暴露于 west-cluster ，我们希望 west-cluster 中的 webapp 的 envoy cluster 配置中，存在一个 catalog 的 endpoints， 因为我们并没有在 west-cluster 中部署 catalog 的deployment，而是在 east-cluster 中部署了 catalog 的 deployment，如果集群之间没有打通，那么在 west-cluster 中 是不会存在 catalog 的 endpoints。 并且，这个 endpoints 应该指向 东西向网关的地址，该地址将请求代理到其网络中的 catalog workload # 首先获取 east-cluster 的 east-west gateway 的 地址 (在 east-cluster 下操作) $ kubectl -n istio-system get svc istio-eastwestgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 40.114.190.251 # 将其与 west-cluster 中的 workload 在跨集群通讯时使用的地址进行比较 (在 west-cluster 下操作) $ istioctl pc endpoints deploy/webapp.istioinaction | grep catalog 40.114.190.251:15443 HEALTHY OK outbound|80||catalog.istioinaction.svc.cluster.local ^-----------^^---^ | | | # 如果 catalog 的 endpoint 与 东西向网关的地址匹配，那么就可以发现 workload ,实现跨集群通讯。 # 让我们在west-cluster中手动发起一个到catalog 的请求 $ EXT_IP=$(kubectl -n istio-system get svc istio-ingressgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ curl http://$EXT_IP/api/catalog -H \"Host: webapp.istioinaction.io\" [ { \"id\": 0, \"color\": \"teal\", \"department\": \"Clothing\", \"name\": \"Small Metal Shoes\", \"price\": \"232.00\" } ] # 可以看到，对入口网关的请求被路由到了 west-cluster 中的 webapp 中， 然后被分配到了 east-cluster 中的 caltalog workload，并最终服务于请求。 至此，我们已经验证集群、多网络、多控制平面服务网格的建立，并发现了跨集群的工作负载，它们可以使用东西网关作为通道启动相互验证的连接。 让我们回顾一下建立多集群服务网格需要做些什么: 跨集群工作负载发现通过使用包含服务帐户令牌和证书的* kubecconfig *为每个控制平面提供对对等集群的访问，这个过程使用了’ istioctl ‘，我们只将它应用到相反的集群。 跨集群工作负载连接性通过配置东西网关在不同集群(驻留在不同网络中)的工作负载之间路由流量，并为每个集群标记网络信息，以便Istio知道网络负载驻留。 通过使用颁发相反集群的中间证书的公共信任根来配置集群之间的信任。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:11:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"第七步 集群间的负载均衡 这部分放在 负载均衡 部分，和本地负载均衡一起介绍。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:12:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"istio 流量走向详解.","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"istio 流量走向详解 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:0:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"前言 Istio作为一个service mesh开源项目,其中最重要的功能就是对网格中微服务之间的流量进行管理,包括服务发现,请求路由和服务间的可靠通信。Istio实现了service mesh的控制面，并整合Envoy开源项目作为数据面的sidecar，一起对流量进行控制。 Istio体系中流量管理配置下发以及流量规则如何在数据面生效的机制相对比较复杂，通过官方文档容易管中窥豹，难以了解其实现原理。本文尝试结合系统架构、配置文件和代码对Istio流量管理的架构和实现机制进行分析，以达到从整体上理解Pilot和Envoy的流量管理机制的目的。 Pilot高层架构 Istio控制面中负责流量管理的组件为Pilot，Pilot的高层架构如下图所示： Pilot Architecture（来自Isio官网文档[1]) 根据上图,Pilot主要实现了下述功能： ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:1:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"统一的服务模型 Pilot定义了网格中服务的标准模型，这个标准模型独立于各种底层平台。由于有了该标准模型，各个不同的平台可以通过适配器和Pilot对接，将自己特有的服务数据格式转换为标准格式，填充到Pilot的标准模型中。 例如Pilot中的Kubernetes适配器通过Kubernetes API服务器得到kubernetes中service和pod的相关信息，然后翻译为标准模型提供给Pilot使用。通过适配器模式，Pilot还可以从Mesos, Cloud Foundry, Consul等平台中获取服务信息，还可以开发适配器将其他提供服务发现的组件集成到Pilot中。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:2:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"标准数据面 API Pilo使用了一套起源于Envoy项目的标准数据面API[2]来将服务信息和流量规则下发到数据面的sidecar中。 通过采用该标准API，Istio将控制面和数据面进行了解耦，为多种数据面sidecar实现提供了可能性。事实上基于该标准API已经实现了多种Sidecar代理和Istio的集成，除Istio目前集成的Envoy外，还可以和Linkerd, Nginmesh等第三方通信代理进行集成，也可以基于该API自己编写Sidecar实现。 控制面和数据面解耦是Istio后来居上，风头超过Service mesh鼻祖Linkerd的一招妙棋。Istio站在了控制面的高度上，而Linkerd则成为了可选的一种sidecar实现，可谓降维打击的一个典型成功案例！ 数据面标准API也有利于生态圈的建立，开源，商业的各种sidecar以后可能百花齐放，用户也可以根据自己的业务场景选择不同的sidecar和控制面集成，如高吞吐量的，低延迟的，高安全性的等等。有实力的大厂商可以根据该API定制自己的sidecar，例如蚂蚁金服开源的Golang版本的Sidecar MOSN(Modular Observable Smart Netstub)（SOFAMesh中Golang版本的Sidecar)；小厂商则可以考虑采用成熟的开源项目或者提供服务的商业sidecar实现。 备注：Istio和Envoy项目联合制定了Envoy V2 API,并采用该API作为Istio控制面和数据面流量管理的标准接口。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:3:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"业务DSL语言 Pilot还定义了一套DSL（Domain Specific Language）语言，DSL语言提供了面向业务的高层抽象，可以被运维人员理解和使用。运维人员使用该DSL定义流量规则并下发到Pilot，这些规则被Pilot翻译成数据面的配置，再通过标准API分发到Envoy实例，可以在运行期对微服务的流量进行控制和调整。 Pilot的规则DSL是采用K8S API Server中的Custom Resource (CRD)[3]实现的，因此和其他资源类型如Service Pod Deployment的创建和使用方法类似，都可以用Kubectl进行创建。 通过运用不同的流量规则，可以对网格中微服务进行精细化的流量控制，如按版本分流，断路器，故障注入，灰度发布等。 Istio流量管理相关组件 我们可以通过下图了解Istio流量管理涉及到的相关组件。虽然该图来自Istio Github old pilot repo, 但图中描述的组件及流程和目前Pilot的最新代码的架构基本是一致的。 Pilot Design Overview (来自Istio old_pilot_repo[4]) 图例说明：图中红色的线表示控制流，黑色的线表示数据流。蓝色部分为和Pilot相关的组件。 从上图可以看到，Istio中和流量管理相关的有以下组件： ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:4:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"控制面组件 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:5:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Discovery Services 对应的docker为gcr.io/istio-release/pilot,进程为pilot-discovery，该组件的功能包括： 从Service provider（如kubernetes或者consul）中获取服务信息 从K8S API Server中获取流量规则(K8S CRD Resource) 将服务信息和流量规则转化为数据面可以理解的格式，通过标准的数据面API下发到网格中的各个sidecar中。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:5:1","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"K8S API Server 提供Pilot相关的CRD Resource的增、删、改、查。和Pilot相关的CRD有以下几种: Virtualservice：用于定义路由规则，如根据来源或 Header 制定规则，或在不同服务版本之间分拆流量。 DestinationRule：定义目的服务的配置策略以及可路由子集。策略包括断路器、负载均衡以及 TLS 等。 ServiceEntry：可以使用ServiceEntry向Istio中加入附加的服务条目，以使网格内可以向istio 服务网格之外的服务发出请求。 Gateway：为网格配置网关，以允许一个服务可以被网格外部访问。 EnvoyFilter：可以为Envoy配置过滤器。由于Envoy已经支持Lua过滤器，因此可以通过EnvoyFilter启用Lua过滤器，动态改变Envoy的过滤链行为。我之前一直在考虑如何才能动态扩展Envoy的能力，EnvoyFilter提供了很灵活的扩展性。 Sidecar：缺省情况下，Pilot将会把和Envoy Sidecar所在namespace的所有services的相关配置，包括inbound和outbound listenter, cluster, route等，都下发给Enovy。使用Sidecar可以对Pilot向Envoy Sidcar下发的配置进行更细粒度的调整，例如只向其下发该Sidecar 所在服务需要访问的那些外部服务的相关outbound配置。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:5:2","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"数据面组件 在数据面有两个进程Pilot-agent和envoy，这两个进程被放在一个docker容器gcr.io/istio-release/proxyv2中。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:6:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Pilot-agent 该进程根据K8S API Server中的配置信息生成Envoy的配置文件，并负责启动Envoy进程。注意Envoy的大部分配置信息都是通过xDS接口从Pilot中动态获取的，因此Agent生成的只是用于初始化Envoy的少量静态配置。在后面的章节中，本文将对Agent生成的Envoy配置文件进行进一步分析。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:6:1","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Envoy Envoy由Pilot-agent进程启动，启动后，Envoy读取Pilot-agent为它生成的配置文件，然后根据该文件的配置获取到Pilot的地址，通过数据面标准API的xDS接口从pilot获取动态配置信息，包括路由（route），监听器（listener），服务集群（cluster）和服务端点（endpoint）。Envoy初始化完成后，就根据这些配置信息对微服务间的通信进行寻址和路由。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:6:2","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"命令行工具 kubectl和Istioctl，由于Istio的配置是基于K8S的CRD，因此可以直接采用kubectl对这些资源进行操作。Istioctl则针对Istio对CRD的操作进行了一些封装。Istioctl支持的功能参见该表格。 数据面标准API 前面讲到，Pilot采用了一套标准的API来向数据面Sidecar提供服务发现，负载均衡池和路由表等流量管理的配置信息。该标准API的文档参见Envoy v2 API[5]。Data Plane API Protocol Buffer Definition[6])给出了v2 grpc接口相关的数据结构和接口定义。 （备注：Istio早期采用了Envoy v1 API，目前的版本中则使用V2 API，V1已被废弃）。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:7:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"基本概念和术语 首先我们需要了解数据面API中涉及到的一些基本概念： Host：能够进行网络通信的实体（如移动设备、服务器上的应用程序）。在此文档中，主机是逻辑网络应用程序。一块物理硬件上可能运行有多个主机，只要它们是可以独立寻址的。在EDS接口中，也使用“Endpoint”来表示一个应用实例，对应一个IP+Port的组合。 Downstream：下游主机连接到 Envoy，发送请求并接收响应。 Upstream：上游主机接收来自 Envoy 的连接和请求，并返回响应。 Listener：监听器是命名网地址（例如，端口、unix domain socket等)，可以被下游客户端连接。Envoy 暴露一个或者多个监听器给下游主机连接。在Envoy中,Listener可以绑定到端口上直接对外服务，也可以不绑定到端口上，而是接收其他listener转发的请求。 Cluster：集群是指 Envoy 连接的一组上游主机，集群中的主机是对等的，对外提供相同的服务，组成了一个可以提供负载均衡和高可用的服务集群。Envoy 通过服务发现来发现集群的成员。可以选择通过主动健康检查来确定集群成员的健康状态。Envoy 通过负载均衡策略决定将请求路由到哪个集群成员。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:8:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"XDS服务接口 Istio数据面API定义了xDS服务接口，Pilot通过该接口向数据面sidecar下发动态配置信息，以对Mesh中的数据流量进行控制。xDS中的DS表示discovery service，即发现服务，表示xDS接口使用动态发现的方式提供数据面所需的配置数据。而x则是一个代词，表示有多种discover service。这些发现服务及对应的数据结构如下： LDS (Listener Discovery Service) envoy.api.v2.Listener CDS (Cluster Discovery Service) envoy.api.v2.RouteConfiguration EDS (Endpoint Discovery Service) envoy.api.v2.Cluster RDS (Route Discovery Service) envoy.api.v2.ClusterLoadAssignment ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:9:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"XDS服务接口的最终一致性考虑 xDS的几个接口是相互独立的，接口下发的配置数据是最终一致的。但在配置更新过程中，可能暂时出现各个接口的数据不匹配的情况，从而导致部分流量在更新过程中丢失。 设想这种场景：在CDS/EDS只知道cluster X的情况下,RDS的一条路由配置将指向Cluster X的流量调整到了Cluster Y。在CDS/EDS向Mesh中Envoy提供Cluster Y的更新前，这部分导向Cluster Y的流量将会因为Envoy不知道Cluster Y的信息而被丢弃。 对于某些应用来说，短暂的部分流量丢失是可以接受的，例如客户端重试可以解决该问题，并不影响业务逻辑。对于另一些场景来说，这种情况可能无法容忍。可以通过调整xDS接口的更新逻辑来避免该问题，对上面的情况，可以先通过CDS/EDS更新Y Cluster，然后再通过RDS将X的流量路由到Y。 一般来说，为了避免Envoy配置数据更新过程中出现流量丢失的情况，xDS接口应采用下面的顺序： CDS 首先更新Cluster数据（如果有变化） EDS 更新相应Cluster的Endpoint信息（如果有变化） LDS 更新CDS/EDS相应的Listener。 RDS 最后更新新增Listener相关的Route配置。 删除不再使用的CDS cluster和 EDS endpoints。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:10:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"ADS聚合发现服务 保证控制面下发数据一致性，避免流量在配置更新过程中丢失的另一个方式是使用ADS(Aggregated Discovery Services)，即聚合的发现服务。ADS通过一个gRPC流来发布所有的配置更新，以保证各个xDS接口的调用顺序，避免由于xDS接口更新顺序导致的配置数据不一致问题。 关于XDS接口的详细介绍可参考xDS REST and gRPC protocol[7] Bookinfo 示例程序分析 下面我们以Bookinfo为例对Istio中的流量管理实现机制，以及控制面和数据面的交互进行进一步分析。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:11:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Bookinfo程序结构 下图显示了Bookinfo示例程序中各个组件的IP地址，端口和调用关系，以用于后续的分析。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:12:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"xDS接口调试方法 首先我们看看如何对xDS接口的相关数据进行查看和分析。Envoy v2接口采用了gRPC，由于gRPC是基于二进制的RPC协议，无法像V1的REST接口一样通过curl和浏览器进行进行分析。但我们还是可以通过Pilot和Envoy的调试接口查看xDS接口的相关数据。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:13:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Pilot调试方法 Pilot在15014端口提供了下述调试接口[8]下述方法查看xDS接口相关数据。 PILOT=istio-pilot.istio-system:15014 # What is sent to envoy # Listeners and routes curl $PILOT/debug/adsz # Endpoints curl $PILOT/debug/edsz # Clusters curl $PILOT/debug/cdsz ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:13:1","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Envoy调试方法 Envoy提供了管理接口，缺省为localhost的15000端口，可以获取listener，cluster以及完整的配置数据导出功能。 kubectl exec productpage-v1-6d8bc58dd7-ts8kw -c istio-proxy curl http://127.0.0.1:15000/help /: Admin home page /certs: print certs on machine /clusters: upstream cluster status /config_dump: dump current Envoy configs (experimental) /cpuprofiler: enable/disable the CPU profiler /healthcheck/fail: cause the server to fail health checks /healthcheck/ok: cause the server to pass health checks /help: print out list of admin commands /hot_restart_version: print the hot restart compatibility version /listeners: print listener addresses /logging: query/change logging levels /quitquitquit: exit the server /reset_counters: reset all counters to zero /runtime: print runtime values /runtime_modify: modify runtime values /server_info: print server version/status information /stats: print server stats /stats/prometheus: print server stats in prometheus format 进入productpage pod 中的istio-proxy(Envoy) container，可以看到有下面的监听端口 9080: productpage进程对外提供的服务端口 15001: Envoy的Virtual Outbound监听器，iptable会将productpage服务发出的出向流量导入该端口中由Envoy进行处理 15006: Envoy的Virtual Inbound监听器，iptable会将发到productpage的入向流量导入该端口中由Envoy进行处理 15000: Envoy管理端口，该端口绑定在本地环回地址上，只能在Pod内访问。 15090：指向127.0.0.1：15000/stats/prometheus, 用于对外提供Envoy的性能统计指标 master $ kubectl exec productpage-v1-6d8bc58dd7-ts8kw -c istio-proxy -- netstat -ln Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:15090 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:9080 0.0.0.0:* LISTEN tcp 0 0 127.0.0.1:15000 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:15001 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:15006 0.0.0.0:* LISTEN tcp6 0 0 :::15020 :::* LISTEN ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:13:2","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Envoy启动过程分析 Istio通过K8s的Admission webhook[9]机制实现了sidecar的自动注入，Mesh中的每个微服务会被加入Envoy相关的容器。下面是Productpage微服务的Pod内容，可见除productpage之外，Istio还在该Pod中注入了两个容器istio-init和istio-proxy，为了节约下载镜像的时间，加快业务Pod的启动速度，这两个容器使用了相同的镜像文件，但启动命令不同。 备注：下面Pod description中只保留了需要关注的内容，删除了其它不重要的部分。为方便查看，本文中后续的其它配置文件以及命令行输出也会进行类似处理。 master $ kubectl describe pod productpage-v1-6d8bc58dd7-ts8kw Name: productpage-v1-6d8bc58dd7-ts8kw Namespace: default Labels: app=productpage version=v1 Init Containers: istio-init: Image: docker.io/istio/proxyv2:1.4.1 Command: istio-iptables -p 15001 -z 15006 -u 1337 -m REDIRECT -i * -x -b * -d 15020 Containers: productpage: Image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0 Port: 9080/TCP istio-proxy: Image: docker.io/istio/proxyv2:1.4.1 Port: 15090/TCP Args: proxy sidecar --domain $(POD_NAMESPACE).svc.cluster.local --configPath /etc/istio/proxy --binaryPath /usr/local/bin/envoy --serviceCluster productpage.$(POD_NAMESPACE) --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istio-pilot.istio-system:15010 --zipkinAddress zipkin.istio-system:9411 --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --connectTimeout 10s --proxyAdminPort 15000 --concurrency 2 --controlPlaneAuthPolicy NONE --dnsRefreshRate 300s --statusPort 15020 --applicationPorts 9080 --trust-domain=cluster.local ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:14:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Proxy_init Productpage的Pod中有一个InitContainer proxy_init，InitContrainer是K8S提供的机制，用于在Pod中执行一些初始化任务.在Initialcontainer执行完毕并退出后，才会启动Pod中的其它container。 从上面的Pod description可以看到，proxy_init容器执行的命令是istio-iptables，这是一个go编译出来的二进制文件，该二进制文件会调用iptables命令创建了一些列iptables规则来劫持Pod中的流量。该命令有这些关键的参数： 命令行参数 -p 15001表示出向流量被iptable重定向到Envoy的15001端口 命令行参数 -z 15006表示入向流量被iptable重定向到Envoy的15006端口 命令行参数 -u 1337参数用于排除用户ID为1337，即Envoy自身的流量，以避免Iptable把Envoy发出的数据又重定向到Envoy，形成死循环。 Iptables规则的详细内容参见istio源码中的shell脚本tools/packaging/common/istio-iptables.sh。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:14:1","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Proxyv2 前面提到，该容器中有两个进程Pilot-agent和envoy。我们进入容器中看看这两个进程的相关信息。 master $ kubectl exec productpage-v1-6d8bc58dd7-ts8kw -c istio-proxy -- ps -ef UID PID PPID C STIME TTY TIME CMD istio-p+ 1 0 0 10:46 ? 00:00:02 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --configPath /etc/istio/proxy --binaryPath/usr/local/bin/envoy --serviceCluster productpage.default --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istio-pilot.istio-system:15010 --zipkinAddress zipkin.istio-system:9411 --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --connectTimeout 10s --proxyAdminPort 15000 --concurrency 2 --controlPlaneAuthPolicy NONE --dnsRefreshRate 300s --statusPort 15020 --applicationPorts 9080 --trust-domain=cluster.local istio-p+ 20 1 0 10:46 ? 00:00:07 /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster productpage.default --service-node sidecar~10.40.0.18~productpage-v1-6d8bc58dd7-ts8kw.default~default.svc.cluster.local --max-obj-name-len 189 --local-address-ip-version v4 --log-format [Envoy (Epoch 0)] [%Y-%m-%d %T.%e][%t][%l][%n] %v -l warning --component-log-level misc:error --concurrency 2 istio-p+ 68 0 0 11:27 ? 00:00:00 ps -ef Envoy的大部分配置都是dynamic resource，包括网格中服务相关的service cluster, listener, route规则等。这些dynamic resource是通过xDS接口从Istio控制面中动态获取的。但Envoy如何知道xDS server的地址呢？这是在Envoy初始化配置文件中以static resource的方式配置的。 Envoy初始配置文件 Pilot-agent进程根据启动参数和K8S API Server中的配置信息生成Envoy的初始配置文件，并负责启动Envoy进程。从ps命令输出可以看到Pilot-agent在启动Envoy进程时传入了pilot地址和zipkin地址，并为Envoy生成了一个初始化配置文件envoy-rev0.json。 可以使用下面的命令将productpage pod中该文件导出来查看其中的内容： kubectl exec productpage-v1-6d8bc58dd7-ts8kw -c istio-proxy cat /etc/istio/proxy/envoy-rev0.json \u003e envoy-rev0.json 配置文件的结构如图所示： 其中各个配置节点的内容如下： Node 包含了Envoy所在节点相关信息。 { \"node\": { \"id\": \"sidecar~10.40.0.18~productpage-v1-6d8bc58dd7-ts8kw.default~default.svc.cluster.local\", \"cluster\": \"productpage.default\", \"locality\": {}, \"metadata\": { \"CLUSTER_ID\": \"Kubernetes\", \"CONFIG_NAMESPACE\": \"default\", \"EXCHANGE_KEYS\": \"NAME,NAMESPACE,INSTANCE_IPS,LABELS,OWNER,PLATFORM_METADATA,WORKLOAD_NAME,CANONICAL_TELEMETRY_SERVICE,MESH_ID,SERVICE_ACCOUNT\", \"INCLUDE_INBOUND_PORTS\": \"9080\", \"INSTANCE_IPS\": \"10.40.0.18,fe80::94df:47ff:fef3:bc99\", \"INTERCEPTION_MODE\": \"REDIRECT\", \"ISTIO_PROXY_SHA\": \"istio-proxy:3af92d895f6cb80993fc8bb04dc9b2008183f2ba\", \"ISTIO_VERSION\": \"1.4.1\", \"LABELS\": { \"app\": \"productpage\", \"pod-template-hash\": \"6d8bc58dd7\", \"security.istio.io/tlsMode\": \"istio\", \"version\": \"v1\" }, \"MESH_ID\": \"cluster.local\", \"NAME\": \"productpage-v1-6d8bc58dd7-ts8kw\", \"NAMESPACE\": \"default\", \"OWNER\": \"kubernetes://api/apps/v1/namespaces/default/deployments/productpage-v1\", \"POD_NAME\": \"productpage-v1-6d8bc58dd7-ts8kw\", \"POD_PORTS\": \"[{\\\"containerPort\\\":9080,\\\"protocol\\\":\\\"TCP\\\"},{\\\"name\\\":\\\"http-envoy-prom\\\",\\\"containerPort\\\":15090,\\\"protocol\\\":\\\"TCP\\\"}]\", \"SERVICE_ACCOUNT\": \"bookinfo-productpage\", \"WORKLOAD_NAME\": \"productpage-v1\", \"app\": \"productpage\", \"pod-template-hash\": \"6d8bc58dd7\", \"security.istio.io/tlsMode\": \"istio\", \"sidecar.istio.io/status\": \"{\\\"version\\\":\\\"8d80e9685defcc00b0d8c9274b60071ba8810537e0ed310ea96c1de0785272c7\\\",\\\"initContainers\\\":[\\\"istio-init\\\"],\\\"containers\\\":[\\\"istio-proxy\\\"],\\\"volumes\\\":[\\\"istio-envoy\\\",\\\"istio-certs\\\"],\\\"imagePullSecrets\\\":null}\", \"version\": \"v1\" } } } Admin 配置Envoy的日志路径以及管理端口。 \"admin\": { \"access_log_path\": \"/dev/null\", \"address\": { \"socket_address\": { \"address\": \"127.0.0.1\", \"port_value\": 15000 } } } Dynamic_resources 配置动态资源,这里配置了ADS服务器。 { \"dynamic_resources\": { \"lds_config\": { \"ads\": {} }, \"cds_config\": { \"ads\": {} }, \"ads_config\": { \"api_type\": \"GRPC\", \"grpc_services\": [ { \"envoy_grpc\": { \"cluster_name\": \"xds-grpc\" } } ] } } } Static_resources 配置静态资源，包括了prometheus_stats、xds-grpc和zipkin三个cluster和一个在15090上监听的listener。其中xds-grpc cluster对应前面dynamic_resources中ADS配置，指明了Envoy用于获取动态资源的服务器地址。prometheus_stats cluster和15090 listener用于对外提供兼容prometheus格式的统计指标。zipkin cluster则是外部的z","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:14:2","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Envoy配置分析 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:15:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"通过管理接口获取完整配置 从Envoy初始化配置文件中，我们可以大致看到Istio通过Envoy来实现服务发现和流量管理的基本原理。即控制面将xDS server信息通过static resource的方式配置到Envoy的初始化配置文件中，Envoy启动后通过xDS server获取到dynamic resource，包括网格中的service信息及路由规则。 Envoy配置初始化流程： Pilot-agent根据启动参数和K8S API Server中的配置信息生成Envoy的初始配置文件envoy-rev0.json，该文件告诉Envoy从xDS server中获取动态配置信息，并配置了xDS server的地址信息，即控制面的Pilot。 Pilot-agent使用envoy-rev0.json启动Envoy进程。 Envoy根据初始配置获得Pilot地址，采用xDS接口从Pilot获取到Listener，Cluster，Route等d动态配置信息。 Envoy根据获取到的动态配置启动Listener，并根据Listener的配置，结合Route和Cluster对拦截到的流量进行处理。 可以看到，Envoy中实际生效的配置是由初始化配置文件中的静态配置和从Pilot获取的动态配置一起组成的。因此只对envoy-rev0 .json进行分析并不能看到Mesh中流量管理的全貌。那么有没有办法可以看到Envoy中实际生效的完整配置呢？答案是可以的，我们可以通过Envoy的管理接口来获取Envoy的完整配置。 kubectl exec -it productpage-v1-6d8bc58dd7-ts8kw -c istio-proxy curl http://127.0.0.1:15000/config_dump \u003e config_dump 该文件内容长达近一万行，本文中就不贴出来了，在https://github.com/zhaohuabing/bookinfo-bookinfo-config-dump/blob/istio1.4.0/productpage-config-dump中可以查看到文件的全部内容。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:15:1","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Envoy配置文件结构 从dump文件中可以看到Envoy中包括下述配置： Bootstrap 从名字可以大致猜出这是Envoy的初始化配置，打开该节点，可以看到文件中的内容和前一章节中介绍的envoy-rev0.json是一致的，这里不再赘述。 Clusters 在Envoy中，Cluster是一个服务集群，Cluster中包含一个到多个endpoint，每个endpoint都可以提供服务，Envoy根据负载均衡算法将请求发送到这些endpoint中。 在Productpage的clusters配置中包含static_clusters和dynamic_active_clusters两部分，其中static_clusters是来自于envoy-rev0.json的初始化配置中的prometheus_stats、xDS server和zipkin server信息。dynamic_active_clusters是通过xDS接口从Istio控制面获取的动态服务信息。 Dynamic Cluster中有以下几类Cluster： Outbound Cluster 这部分的Cluster占了绝大多数，该类Cluster对应于Envoy所在节点的外部服务。以reviews为例，对于Productpage来说,reviews是一个外部服务，因此其Cluster名称中包含outbound字样。 从reviews 服务对应的cluster配置中可以看到，其类型为EDS，即表示该Cluster的endpoint来自于动态发现，动态发现中eds_config则指向了ads，最终指向static Resource中配置的xds-grpc cluster,即Pilot的地址。 { \"version_info\": \"2019-12-04T03:08:06Z/13\", \"cluster\": { \"name\": \"outbound|9080||reviews.default.svc.cluster.local\", \"type\": \"EDS\", \"eds_cluster_config\": { \"eds_config\": { \"ads\": {} }, \"service_name\": \"outbound|9080||reviews.default.svc.cluster.local\" }, \"connect_timeout\": \"1s\", \"circuit_breakers\": { \"thresholds\": [ { \"max_connections\": 4294967295, \"max_pending_requests\": 4294967295, \"max_requests\": 4294967295, \"max_retries\": 4294967295 } ] } }, \"last_updated\": \"2019-12-04T03:08:22.658Z\" } 可以通过Pilot的调试接口获取该Cluster的endpoint： curl http://10.97.222.108:15014/debug/edsz \u003e pilot_eds_dump 导出的文件较长，本文只贴出reviews服务相关的endpoint配置，完整文件参见:https://github.com/zhaohuabing/bookinfo-bookinfo-config-dump/blob/istio1.4.0/pilot_eds_dump 从下面的文件内容可以看到，reviews cluster配置了3个endpoint地址，是reviews的pod ip。 { \"clusterName\": \"outbound|9080||reviews.default.svc.cluster.local\", \"endpoints\": [ { \"lbEndpoints\": [ { \"endpoint\": { \"address\": { \"socketAddress\": { \"address\": \"10.40.0.15\", \"portValue\": 9080 } } }, \"metadata\": { \"filterMetadata\": { \"envoy.transport_socket_match\": { \"tlsMode\": \"istio\" }, \"istio\": { \"uid\": \"kubernetes://reviews-v1-75b979578c-pw8zs.default\" } } }, \"loadBalancingWeight\": 1 }, { \"endpoint\": { \"address\": { \"socketAddress\": { \"address\": \"10.40.0.16\", \"portValue\": 9080 } } }, \"metadata\": { \"filterMetadata\": { \"envoy.transport_socket_match\": { \"tlsMode\": \"istio\" }, \"istio\": { \"uid\": \"kubernetes://reviews-v3-54c6c64795-wbls7.default\" } } }, \"loadBalancingWeight\": 1 }, { \"endpoint\": { \"address\": { \"socketAddress\": { \"address\": \"10.40.0.17\", \"portValue\": 9080 } } }, \"metadata\": { \"filterMetadata\": { \"envoy.transport_socket_match\": { \"tlsMode\": \"istio\" }, \"istio\": { \"uid\": \"kubernetes://reviews-v2-597bf96c8f-l2fp8.default\" } } }, \"loadBalancingWeight\": 1 } ], \"loadBalancingWeight\": 3 } ] } Inbound Cluster 该类Cluster对应于Envoy所在节点上的服务。如果该服务接收到请求，当然就是一个入站请求。对于Productpage Pod上的Envoy，其对应的Inbound Cluster只有一个，即productpage。该cluster对应的host为127.0.0.1,即环回地址上productpage的监听端口。由于iptable规则中排除了127.0.0.1,入站请求通过该Inbound cluster处理后将跳过Envoy，直接发送给Productpage进程处理。 { \"version_info\": \"2019-12-04T03:08:06Z/13\", \"cluster\": { \"name\": \"inbound|9080|http|productpage.default.svc.cluster.local\", \"type\": \"STATIC\", \"connect_timeout\": \"1s\", \"circuit_breakers\": { \"thresholds\": [ { \"max_connections\": 4294967295, \"max_pending_requests\": 4294967295, \"max_requests\": 4294967295, \"max_retries\": 4294967295 } ] }, \"load_assignment\": { \"cluster_name\": \"inbound|9080|http|productpage.default.svc.cluster.local\", \"endpoints\": [ { \"lb_endpoints\": [ { \"endpoint\": { \"address\": { \"socket_address\": { \"address\": \"127.0.0.1\", \"port_value\": 9080 } } } } ] } ] } }, \"last_updated\": \"2019-12-04T03:08:22.658Z\" } BlackHoleCluster 这是一个特殊的Cluster，并没有配置后端处理请求的Host。如其名字所暗示的一样，请求进入后将被直接丢弃掉。如果一个请求没有找到其对的目的服务，则被发到cluste。 { \"version_info\": \"2019-12-04T03:08:06Z/13\", \"cluster\": { \"name\": \"BlackHoleCluster\", \"type\": \"STATIC\", \"connect_timeout\": \"1s\" }, \"last_updated\": \"2019-12-04T03:08:22.658Z\" } PassthroughCluster 和BlackHoleCluter相反，发向PassthroughCluster的请求会被直接发送到其请求中要求的原始目地的，Envoy不会对请求进行重新路由。 { \"version_info\": \"2019-12-04T03:08:06Z/13\", \"cluster\": { \"name\": \"PassthroughCluster\", \"type\": \"ORIGINAL_DST\", \"connect_timeout\": \"1s\", \"lb_policy\": \"CLUSTER_PROVIDED\", \"circuit_breakers\": { \"thresholds\": ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:15:2","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Bookinfo端到端调用分析 通过前面章节对Envoy配置文件的分析，我们了解到Istio控制面如何将服务和路由信息通过xDS接口下发到数据面中；并介绍了Envoy上生成的各种配置数据的结构，包括listener,cluster,route和endpoint。 下面我们来分析一个端到端的调用请求，通过调用请求的流程把这些配置串连起来，以从全局上理解Istio控制面的流量控制能力是如何在数据面的Envoy上实现的。 下图描述了一个Productpage服务调用Reviews服务的请求流程： Virtual Inbound Listener Productpage发起对Reviews服务的调用：http://reviews:9080/reviews/0 。 请求被Productpage Pod的iptable规则拦截，重定向到本地的15001端口。 在15001端口上监听的Envoy Virtual Outbound Listener收到了该请求。 请求被Virtual Outbound Listener根据原目标IP（通配）和端口（9080）转发到0.0.0.0_9080这个 outbound listener。 { \"version_info\": \"2019-12-04T03:08:06Z/13\", \"listener\": { \"name\": \"virtualOutbound\", \"address\": { \"socket_address\": { \"address\": \"0.0.0.0\", \"port_value\": 15001 } }, ...... \"use_original_dst\": true //请求转发给和原始目的IP:Port匹配的listener }, \"last_updated\": \"2019-12-04T03:08:22.919Z\" } 根据0.0.0.0_9080 listener的http_connection_manager filter配置,该请求采用“9080” route进行分发。 { \"version_info\": \"2019-12-04T03:08:06Z/13\", \"listener\": { \"name\": \"0.0.0.0_9080\", \"address\": { \"socket_address\": { \"address\": \"0.0.0.0\", \"port_value\": 9080 } }, \"filter_chains\": [ { \"filters\": [ { \"name\": \"envoy.http_connection_manager\", \"typed_config\": { \"@type\": \"type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\", \"stat_prefix\": \"outbound_0.0.0.0_9080\", \"http_filters\": [ { \"name\": \"mixer\", ...... }, { \"name\": \"envoy.cors\" }, { \"name\": \"envoy.fault\" }, { \"name\": \"envoy.router\" } ], ...... \"rds\": { \"config_source\": { \"ads\": {} }, \"route_config_name\": \"9080\" //采用“9080” route进行分发 } } } ] } ], \"deprecated_v1\": { \"bind_to_port\": false }, \"listener_filters_timeout\": \"0.100s\", \"traffic_direction\": \"OUTBOUND\", \"continue_on_listener_filters_timeout\": true }, \"last_updated\": \"2019-12-04T03:08:22.822Z\" } “9080”这个route的配置中，host name为reviews:9080的请求对应的cluster为outbound|9080||reviews.default.svc.cluster.local { \"name\": \"reviews.default.svc.cluster.local:9080\", \"domains\": [ \"reviews.default.svc.cluster.local\", \"reviews.default.svc.cluster.local:9080\", \"reviews\", \"reviews:9080\", \"reviews.default.svc.cluster\", \"reviews.default.svc.cluster:9080\", \"reviews.default.svc\", \"reviews.default.svc:9080\", \"reviews.default\", \"reviews.default:9080\", \"10.102.108.56\", \"10.102.108.56:9080\" ], \"routes\": [ { \"match\": { \"prefix\": \"/\" }, \"route\": { \"cluster\": \"outbound|9080||reviews.default.svc.cluster.local\", \"timeout\": \"0s\", \"retry_policy\": { \"retry_on\": \"connect-failure,refused-stream,unavailable,cancelled,resource-exhausted,retriable-status-codes\", \"num_retries\": 2, \"retry_host_predicate\": [ { \"name\": \"envoy.retry_host_predicates.previous_hosts\" } ], \"host_selection_retry_max_attempts\": \"5\", \"retriable_status_codes\": [ 503 ] }, \"max_grpc_timeout\": \"0s\" }, \"decorator\": { \"operation\": \"reviews.default.svc.cluster.local:9080/*\" }, \"typed_per_filter_config\": { \"mixer\": { \"@type\": \"type.googleapis.com/istio.mixer.v1.config.client.ServiceConfig\", \"disable_check_calls\": true, \"mixer_attributes\": { \"attributes\": { \"destination.service.host\": { \"string_value\": \"reviews.default.svc.cluster.local\" }, \"destination.service.name\": { \"string_value\": \"reviews\" }, \"destination.service.namespace\": { \"string_value\": \"default\" }, \"destination.service.uid\": { \"string_value\": \"istio://default/services/reviews\" } } }, \"forward_attributes\": { \"attributes\": { \"destination.service.host\": { \"string_value\": \"reviews.default.svc.cluster.local\" }, \"destination.service.name\": { \"string_value\": \"reviews\" }, \"destination.service.namespace\": { \"string_value\": \"default\" }, \"destination.service.uid\": { \"string_value\": \"istio://default/services/reviews\" } } } } }, \"name\": \"default\" } ] } outbound|9080||reviews.default.svc.cluster.local cluster为动态资源，通过eds查询得到该cluster中有3个endpoint。 { \"clusterName\": \"outbound|9080||reviews.default.svc.cluster.local\", \"endpoints\": [ { \"lbEndpoints\": [ { \"endpoint\": { \"address\": { \"socketAddress\": { \"address\": \"10.40.0.15\", \"portValue\": 9080 } } }, \"metadata\": { \"filterMetadata\": { \"envoy.transport_socket_match\": { \"tlsMode\": \"istio\" }, \"istio\": { \"uid\": \"kubernete","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:16:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["kubernetes"],"content":"k8s-ingress 原理和使用","date":"212111-08-2151","objectID":"/ingress/","tags":["kubernetes"],"title":"k8s-ingress 原理和使用","uri":"/ingress/"},{"categories":["kubernetes"],"content":"ingress是啥东东 上篇文章介绍service时有说了暴露了service的三种方式ClusterIP、NodePort与LoadBalance，这几种方式都是在service的维度提供的，service的作用体现在两个方面，对集群内部，它不断跟踪pod的变化，更新endpoint中对应pod的对象，提供了ip不断变化的pod的服务发现机制，对集群外部，他类似负载均衡器，可以在集群内外部对pod进行访问。但是，单独用service暴露服务的方式，在实际生产环境中不太合适： ClusterIP的方式只能在集群内部访问。 NodePort方式的话，测试环境使用还行，当有几十上百的服务在集群中运行时，NodePort的端口管理是灾难。 LoadBalance方式受限于云平台，且通常在云平台部署ELB还需要额外的费用。 所幸k8s还提供了一种集群维度暴露服务的方式，也就是ingress。ingress可以简单理解为service的service，他通过独立的ingress对象来制定请求转发的规则，把请求路由到一个或多个service中。这样就把服务与请求规则解耦了，可以从业务维度统一考虑业务的暴露，而不用为每个service单独考虑。 举个例子，现在集群有api、文件存储、前端3个service，可以通过一个ingress对象来实现图中的请求转发： ingress规则是很灵活的，可以根据不同域名、不同path转发请求到不同的service，并且支持https/http。 ","date":"212111-08-2151","objectID":"/ingress/:1:0","tags":["kubernetes"],"title":"k8s-ingress 原理和使用","uri":"/ingress/"},{"categories":["kubernetes"],"content":"ingress与ingress-controller 要理解ingress，需要区分两个概念，ingress和ingress-controller： ingress对象： 指的是k8s中的一个api对象，一般用yaml配置。作用是定义请求如何转发到service的规则，可以理解为配置模板。 ingress-controller： 具体实现反向代理及负载均衡的程序，对ingress定义的规则进行解析，根据配置的规则来实现请求转发。 简单来说，ingress-controller才是负责具体转发的组件，通过各种方式将它暴露在集群入口，外部对集群的请求流量会先到ingress-controller，而ingress对象是用来告诉ingress-controller该如何转发请求，比如哪些域名哪些path要转发到哪些服务等等。 ","date":"212111-08-2151","objectID":"/ingress/:2:0","tags":["kubernetes"],"title":"k8s-ingress 原理和使用","uri":"/ingress/"},{"categories":["kubernetes"],"content":"ingress-controller ingress-controller并不是k8s自带的组件，实际上ingress-controller只是一个统称，用户可以选择不同的ingress-controller实现，目前，由k8s维护的ingress-controller只有google云的GCE与ingress-nginx两个，其他还有很多第三方维护的ingress-controller，具体可以参考官方文档。但是不管哪一种ingress-controller，实现的机制都大同小异，只是在具体配置上有差异。一般来说，ingress-controller的形式都是一个pod，里面跑着daemon程序和反向代理程序。daemon负责不断监控集群的变化，根据ingress对象生成配置并应用新配置到反向代理，比如nginx-ingress就是动态生成nginx配置，动态更新upstream，并在需要的时候reload程序应用新配置。为了方便，后面的例子都以k8s官方维护的nginx-ingress为例。 ","date":"212111-08-2151","objectID":"/ingress/:2:1","tags":["kubernetes"],"title":"k8s-ingress 原理和使用","uri":"/ingress/"},{"categories":["kubernetes"],"content":"ingress ingress是一个API对象，和其他对象一样，通过yaml文件来配置。ingress通过http或https暴露集群内部service，给service提供外部URL、负载均衡、SSL/TLS能力以及基于host的方向代理。ingress要依靠ingress-controller来具体实现以上功能。前一小节的图如果用ingress来表示，大概就是如下配置： apiVersion:extensions/v1beta1kind:Ingressmetadata:name:abc-ingressannotations:kubernetes.io/ingress.class:\"nginx\"nginx.ingress.kubernetes.io/use-regex:\"true\"spec:tls:- hosts:- api.abc.comsecretName:abc-tlsrules:- host:api.abc.comhttp:paths:- backend:serviceName:apiserverservicePort:80- host:www.abc.comhttp:paths:- path:/image/*backend:serviceName:fileserverservicePort:80- host:www.abc.comhttp:paths:- backend:serviceName:feserverservicePort:8080 与其他k8s对象一样，ingress配置也包含了apiVersion、kind、metadata、spec等关键字段。有几个关注的在spec字段中，tls用于定义https密钥、证书。rule用于指定请求路由规则。这里值得关注的是metadata.annotations字段。在ingress配置中，annotations很重要。前面有说ingress-controller有很多不同的实现，而不同的ingress-controller就可以根据\"kubernetes.io/ingress.class:“来判断要使用哪些ingress配置，同时，不同的ingress-controller也有对应的annotations配置，用于自定义一些参数。列如上面配置的’nginx.ingress.kubernetes.io/use-regex: “true”',最终是在生成nginx配置中，会采用location ~来表示正则匹配。 ","date":"212111-08-2151","objectID":"/ingress/:2:2","tags":["kubernetes"],"title":"k8s-ingress 原理和使用","uri":"/ingress/"},{"categories":["kubernetes"],"content":"ingress的部署 ingress的部署，需要考虑两个方面： ingress-controller是作为pod来运行的，以什么方式部署比较好 ingress解决了把如何请求路由到集群内部，那它自己怎么暴露给外部比较好 下面列举一些目前常见的部署和暴露方式，具体使用哪种方式还是得根据实际需求来考虑决定。 ","date":"212111-08-2151","objectID":"/ingress/:3:0","tags":["kubernetes"],"title":"k8s-ingress 原理和使用","uri":"/ingress/"},{"categories":["kubernetes"],"content":"Deployment+LoadBalancer模式的Service 如果要把ingress部署在公有云，那用这种方式比较合适。用Deployment部署ingress-controller，创建一个type为LoadBalancer的service关联这组pod。大部分公有云，都会为LoadBalancer的service自动创建一个负载均衡器，通常还绑定了公网地址。只要把域名解析指向该地址，就实现了集群服务的对外暴露。 ","date":"212111-08-2151","objectID":"/ingress/:3:1","tags":["kubernetes"],"title":"k8s-ingress 原理和使用","uri":"/ingress/"},{"categories":["kubernetes"],"content":"Deployment+NodePort模式的Service 同样用deployment模式部署ingress-controller，并创建对应的服务，但是type为NodePort。这样，ingress就会暴露在集群节点ip的特定端口上。由于nodeport暴露的端口是随机端口，一般会在前面再搭建一套负载均衡器来转发请求。该方式一般用于宿主机是相对固定的环境ip地址不变的场景。 NodePort方式暴露ingress虽然简单方便，但是NodePort多了一层NAT，在请求量级很大时可能对性能会有一定影响。 ","date":"212111-08-2151","objectID":"/ingress/:3:2","tags":["kubernetes"],"title":"k8s-ingress 原理和使用","uri":"/ingress/"},{"categories":["kubernetes"],"content":"DaemonSet+HostNetwork+nodeSelector 用DaemonSet结合nodeselector来部署ingress-controller到特定的node上，然后使用HostNetwork直接把该pod与宿主机node的网络打通，直接使用宿主机的80/433端口就能访问服务。这时，ingress-controller所在的node机器就很类似传统架构的边缘节点，比如机房入口的nginx服务器。该方式整个请求链路最简单，性能相对NodePort模式更好。缺点是由于直接利用宿主机节点的网络和端口，一个node只能部署一个ingress-controller pod。比较适合大并发的生产环境使用。 ","date":"212111-08-2151","objectID":"/ingress/:3:3","tags":["kubernetes"],"title":"k8s-ingress 原理和使用","uri":"/ingress/"},{"categories":["kubernetes"],"content":"ingress测试 我们来实际部署和简单测试一下ingress。测试集群中已经部署有2个服务gowebhost与gowebip，每次请求能返回容器hostname与ip。测试搭建一个ingress来实现通过域名的不同path来访问这两个服务： 测试ingress使用k8s社区的ingress-nginx，部署方式用DaemonSet+HostNetwork。 ","date":"212111-08-2151","objectID":"/ingress/:4:0","tags":["kubernetes"],"title":"k8s-ingress 原理和使用","uri":"/ingress/"},{"categories":["kubernetes"],"content":"部署ingress-controller 部署ingress-controller pod及相关资源 官方文档中，部署只要简单的执行一个yaml https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml mandatory.yaml这一个yaml中包含了很多资源的创建，包括namespace、ConfigMap、role，ServiceAccount等等所有部署ingress-controller需要的资源，配置太多就不粘出来了，我们重点看下deployment部分： apiVersion:apps/v1kind:Deploymentmetadata:name:nginx-ingress-controllernamespace:ingress-nginxlabels:app.kubernetes.io/name:ingress-nginxapp.kubernetes.io/part-of:ingress-nginxspec:replicas:1selector:matchLabels:app.kubernetes.io/name:ingress-nginxapp.kubernetes.io/part-of:ingress-nginxtemplate:metadata:labels:app.kubernetes.io/name:ingress-nginxapp.kubernetes.io/part-of:ingress-nginxannotations:prometheus.io/port:\"10254\"prometheus.io/scrape:\"true\"spec:serviceAccountName:nginx-ingress-serviceaccountcontainers:- name:nginx-ingress-controllerimage:quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.25.0args:- /nginx-ingress-controller- --configmap=$(POD_NAMESPACE)/nginx-configuration- --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services- --udp-services-configmap=$(POD_NAMESPACE)/udp-services- --publish-service=$(POD_NAMESPACE)/ingress-nginx- --annotations-prefix=nginx.ingress.kubernetes.iosecurityContext:allowPrivilegeEscalation:truecapabilities:drop:- ALLadd:- NET_BIND_SERVICE# www-data -\u003e 33runAsUser:33env:- name:POD_NAMEvalueFrom:fieldRef:fieldPath:metadata.name- name:POD_NAMESPACEvalueFrom:fieldRef:fieldPath:metadata.namespaceports:- name:httpcontainerPort:80- name:httpscontainerPort:443livenessProbe:failureThreshold:3httpGet:path:/healthzport:10254scheme:HTTPinitialDelaySeconds:10periodSeconds:10successThreshold:1timeoutSeconds:10readinessProbe:failureThreshold:3httpGet:path:/healthzport:10254scheme:HTTPperiodSeconds:10successThreshold:1timeoutSeconds:10 可以看到主要使用了“quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.25.0”这个镜像，指定了一些启动参数。同时开放了80与443两个端口，并在10254端口做了健康检查。 我们需要使用daemonset部署到特定node，需要修改部分配置：先给要部署nginx-ingress的node打上特定标签,这里测试部署在\"node-1\"这个节点。 $ kubectl label node node-1 isIngress=\"true\" 然后修改上面mandatory.yaml的deployment部分配置为： # 修改api版本及kind# apiVersion: apps/v1# kind: DeploymentapiVersion:extensions/v1beta1kind:DaemonSetmetadata:name:nginx-ingress-controllernamespace:ingress-nginxlabels:app.kubernetes.io/name:ingress-nginxapp.kubernetes.io/part-of:ingress-nginxspec:# 删除Replicas# replicas: 1selector:matchLabels:app.kubernetes.io/name:ingress-nginxapp.kubernetes.io/part-of:ingress-nginxtemplate:metadata:labels:app.kubernetes.io/name:ingress-nginxapp.kubernetes.io/part-of:ingress-nginxannotations:prometheus.io/port:\"10254\"prometheus.io/scrape:\"true\"spec:serviceAccountName:nginx-ingress-serviceaccount# 选择对应标签的nodenodeSelector:isIngress:\"true\"# 使用hostNetwork暴露服务hostNetwork:truecontainers:- name:nginx-ingress-controllerimage:quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.25.0args:- /nginx-ingress-controller- --configmap=$(POD_NAMESPACE)/nginx-configuration- --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services- --udp-services-configmap=$(POD_NAMESPACE)/udp-services- --publish-service=$(POD_NAMESPACE)/ingress-nginx- --annotations-prefix=nginx.ingress.kubernetes.iosecurityContext:allowPrivilegeEscalation:truecapabilities:drop:- ALLadd:- NET_BIND_SERVICE# www-data -\u003e 33runAsUser:33env:- name:POD_NAMEvalueFrom:fieldRef:fieldPath:metadata.name- name:POD_NAMESPACEvalueFrom:fieldRef:fieldPath:metadata.namespaceports:- name:httpcontainerPort:80- name:httpscontainerPort:443livenessProbe:failureThreshold:3httpGet:path:/healthzport:10254scheme:HTTPinitialDelaySeconds:10periodSeconds:10successThreshold:1timeoutSeconds:10readinessProbe:failureThreshold:3httpGet:path:/healthzport:10254scheme:HTTPperiodSeconds:10successThreshold:1timeoutSeconds:10 修改完后执行apply,并检查服务 $ kubectl apply -f mandatory.yaml namespace/ingress-nginx created configmap/nginx-configuration created configmap/tcp-services created configmap/udp-services created serviceaccount/nginx-ingress-serviceaccount created clusterrole.rbac","date":"212111-08-2151","objectID":"/ingress/:4:1","tags":["kubernetes"],"title":"k8s-ingress 原理和使用","uri":"/ingress/"},{"categories":["kubernetes"],"content":"配置ingress资源 部署完ingress-controller，接下来就按照测试的需求来创建ingress资源。 # ingresstest.yamlapiVersion:extensions/v1beta1kind:Ingressmetadata:name:ingress-testannotations:kubernetes.io/ingress.class:\"nginx\"# 开启use-regex，启用path的正则匹配 nginx.ingress.kubernetes.io/use-regex:\"true\"spec:rules:# 定义域名- host:test.ingress.comhttp:paths:# 不同path转发到不同端口- path:/ipbackend:serviceName:gowebip-svcservicePort:80- path:/hostbackend:serviceName:gowebhost-svcservicePort:80 部署资源 $ kubectl apply -f ingresstest.yaml 测试访问 部署好以后，做一条本地host来模拟解析test.ingress.com到node的ip地址。测试访问 可以看到，请求不同的path已经按照需求请求到不同服务了。 由于没有配置默认后端，所以访问其他path会提示404： 关于ingress-nginx 关于ingress-nginx多说几句，上面测试的例子是非常简单的，实际ingress-nginx的有非常多的配置，都可以单独开几篇文章来讨论了。但本文主要想说明ingress，所以不过多涉及。具体可以参考ingress-nginx的官方文档。同时，在生产环境使用ingress-nginx还有很多要考虑的地方，这篇文章写得很好，总结了不少最佳实践，值得参考。 ","date":"212111-08-2151","objectID":"/ingress/:4:2","tags":["kubernetes"],"title":"k8s-ingress 原理和使用","uri":"/ingress/"},{"categories":["kubernetes"],"content":"最后 ingress是k8s集群的请求入口，可以理解为对多个service的再次抽象 通常说的ingress一般包括ingress资源对象及ingress-controller两部分组成 ingress-controller有多种实现，社区原生的是ingress-nginx，根据具体需求选择 ingress自身的暴露有多种方式，需要根据基础环境及业务类型选择合适的方式 ","date":"212111-08-2151","objectID":"/ingress/:5:0","tags":["kubernetes"],"title":"k8s-ingress 原理和使用","uri":"/ingress/"},{"categories":["kubernetes"],"content":"参考 Kubernetes Document NGINX Ingress Controller Document Kubernetes Ingress Controller的使用介绍及高可用落地 通俗理解Kubernetes中Service、Ingress与Ingress Controller的作用与关系 ","date":"212111-08-2151","objectID":"/ingress/:6:0","tags":["kubernetes"],"title":"k8s-ingress 原理和使用","uri":"/ingress/"},{"categories":null,"content":"个人介绍 郑天驰，浙江工业大学硕士研究生。 云原生领域从业者，擅长 golang、c++、kubernetes、service mesh、istio、docker、envoy ","date":"20211-08-24","objectID":"/about/:0:0","tags":null,"title":"关于卷王郑天驰","uri":"/about/"},{"categories":null,"content":"职场经历 时间 公司 职务 工作内容 2021.4.1 - 2021.12.3 恒生电子股份有限公司 研发中心/云原生产品部/service mesh小组/研发工程师 service mesh 控制台后端开发，service mesh operator 化开发 2021.12.6 - 至今 默安科技有限公司 研发中心/云原生组/研发工程师 云原生网络安全开发 ","date":"20211-08-24","objectID":"/about/:1:0","tags":null,"title":"关于卷王郑天驰","uri":"/about/"},{"categories":null,"content":"博客规划 围绕 Envoy, istio, kubernetes, cilium, golang 为主 初期计划介绍功能和架构，后期进阶会围绕源码分析 ","date":"20211-08-24","objectID":"/about/:2:0","tags":null,"title":"关于卷王郑天驰","uri":"/about/"},{"categories":null,"content":"介绍¶ Envoy 是一个面向服务架构的 L7 代理和通信总线而设计的，这个项目诞生是出于以下目标： 对于应用程序而言，网络应该是透明的，当发生网络和应用程序故障时，能够很容易定位出问题的根源。 Envoy 的核心功能： 非侵入的架构：Envoy 是和应用服务并行运行的，透明地代理应用服务发出/接收的流量。应用服务只需要和 Envoy 通信，无需知道其他微服务应用在哪里。 基于 Modern C++11 实现，性能优异。 L3/L4 过滤器架构：Envoy 的核心是一个 L3/L4 代理，然后通过插件式的过滤器(network filters)链条来执行 TCP/UDP 的相关任务，例如 TCP 转发，TLS 认证等工作。 HTTP L7 过滤器架构：HTTP 在现代应用体系里是地位非常特殊的应用层协议，所以 Envoy 内置了一个非常核心的过滤器: http_connection_manager。http_connection_manager 本身是如此特殊和复杂，支持丰富的配置，以及本身也是过滤器架构，可以通过一系列 http 过滤器来实现 http 协议层面的任务，例如：http 路由，重定向，CORS 支持等等。 HTTP/2 作为第一公民：Envoy 支持 HTTP/1.1 和 HTTP/2，推荐使用 HTTP/2。 gRPC 支持：因为对 HTTP/2 的良好支持，Envoy 可以方便的支持 gRPC，特别是在负载和代理上。 服务发现： 支持包括 DNS, EDS 在内的多种服务发现方案。 健康检查：内置健康检查子系统。 高级的负载均衡方案：除了一般的负载均衡，Envoy 还支持基于 rate limit 服务的多种高级负载均衡方案，包括： automatic retries、circuit breaking、global rate limiting 等。 Tracing：方便集成 Open Tracing 系统，追踪请求 统计与监控：内置 stats 模块，方便集成诸如 prometheus/statsd 等监控方案 动态配置：通过“动态配置API”实现配置的动态调整，而无需重启 Envoy 服务的。 Envoy 入门¶ Envoy 是一个开源的边缘服务代理，也是 Istio Service Mesh 默认的数据平面，专为云原生应用程序设计。 下面我们通过一个简单的示例来介绍 Envoy 的基本使用。 ","date":"5059-08-529","objectID":"/envoy_1/:0:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"1. 配置¶ ","date":"5059-08-529","objectID":"/envoy_1/:1:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"创建代理配置¶ Envoy 使用 YAML 配置文件来控制代理的行为。在下面的步骤中，我们将使用静态配置接口来构建配置，也意味着所有设置都是预定义在配置文件中的。此外 Envoy 也支持动态配置，这样可以通过外部一些源来自动发现进行设置。 ","date":"5059-08-529","objectID":"/envoy_1/:1:1","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"资源¶ Envoy 配置的第一行定义了正在使用的接口配置，在这里我们将配置静态 API，因此第一行应为 static_resources： static_resources: ","date":"5059-08-529","objectID":"/envoy_1/:1:2","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"监听器¶ 在配置的开始定义了监听器（Listeners）。监听器是 Envoy 监听请求的网络配置，例如 IP 地址和端口。我们这里的 Envoy 在 Docker 容器内运行，因此它需要监听 IP 地址 0.0.0.0，在这种情况下，Envoy 将在端口 10000 上进行监听。 下面是定义监听器的配置： static_resources:listeners:- name:listener_0address:socket_address:{address: 0.0.0.0, port_value:10000} ","date":"5059-08-529","objectID":"/envoy_1/:1:3","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"过滤器¶ 通过 Envoy 监听传入的流量，下一步是定义如何处理这些请求。每个监听器都有一组过滤器，并且不同的监听器可以具有一组不同的过滤器。 在我们这个示例中，我们将所有流量代理到 baidu.com，配置完成后我们应该能够通过请求 Envoy 的端点就可以直接看到百度的主页了，而无需更改 URL 地址。 过滤器是通过 filter_chains 来定义的，每个过滤器的目的是找到传入请求的匹配项，以使其与目标地址进行匹配： static_resources: listeners: - name: listener_0 address: socket_address: { address: 0.0.0.0, port_value: 10000 } filter_chains: - filters: - name: envoy.http_connection_manager config: stat_prefix: ingress_http route_config: name: local_route virtual_hosts: - name: local_service domains: [\"*\"] # 匹配的主机域名，满足才会路由转发 routes: # 路由转发 - match: { prefix: \"/\" } # 请求匹配前缀 route: { host_rewrite: www.baidu.com, cluster: service_baidu } # 更改requeset header 中的host，以及处理请求的cluster http_filters: - name: envoy.router 该过滤器使用了 envoy.http_connection_manager，这是为 HTTP 连接设计的一个内置过滤器: stat_prefix：为连接管理器发出统计信息时使用的一个前缀。 route_config：路由配置，如果虚拟主机 virtual_hosts 匹配上了则检查路由。在我们这里的配置中，无论请求的主机域名是什么，route_config 都匹配所有传入的 HTTP 请求，因为domains 是 *，所以匹配所有的请求。 routes：如果 URL 前缀匹配，则一组路由规则定义了下一步将发生的状况。/ 表示匹配根路由。 host_rewrite：更改 HTTP 请求的入站 Host 头信息。 cluster: 将要处理请求的集群名称，下面会有相应的实现。 http_filters: 该过滤器允许 Envoy 在处理请求时去适应和修改请求。 ","date":"5059-08-529","objectID":"/envoy_1/:1:4","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"集群¶ 当请求于过滤器匹配时，该请求将会传递到集群cluster。下面的配置就是将主机定义为访问 HTTPS 的 baidu.com 域名，如果定义了多个主机，则 Envoy 将执行轮询（Round Robin）策略。配置如下所示： static_resources:listeners:- name:listener_0address:socket_address:{address: 0.0.0.0, port_value:10000}filter_chains:- filters:- name:envoy.http_connection_managerconfig:stat_prefix:ingress_httproute_config:name:local_routevirtual_hosts:- name:local_servicedomains:[\"*\"]routes:- match:{prefix:\"/\"}route:{host_rewrite: www.baidu.com, cluster:service_baidu }http_filters:- name:envoy.routerclusters:# 集群定义- name:service_baiduconnect_timeout:0.25s # 连接超时type:LOGICAL_DNSdns_lookup_family:V4_ONLYlb_policy:ROUND_ROBINhosts:[{socket_address:{address: www.baidu.com, port_value:443}}]tls_context:{sni:baidu.com } ","date":"5059-08-529","objectID":"/envoy_1/:1:5","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"管理¶ 最后，还需要配置一个管理模块： admin:access_log_path:/tmp/admin_access.logaddress:socket_address:{address: 0.0.0.0, port_value:9901} 上面的配置定义了 Envoy 的静态配置模板，监听器定义了 Envoy 的端口和 IP 地址，监听器具有一组过滤器来匹配传入的请求，匹配请求后，将请求转发到集群。 ","date":"5059-08-529","objectID":"/envoy_1/:1:6","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"2. 开启代理¶ 配置完成后，可以通过 Docker 容器来启动 Envoy，将上面的配置文件通过 Volume 挂载到容器中的 /etc/envoy/envoy.yaml 文件。 然后使用以下命令启动绑定到端口 80 的 Envoy 容器： $ docker run --name=envoy -d \\ -p 80:10000 \\ -v $(pwd)/manifests/envoy.yaml:/etc/envoy/envoy.yaml \\ envoyproxy/envoy:latest 启动后，我们可以在本地的 80 端口上去访问应用 curl localhost 来测试代理是否成功。同样我们也可以通过在本地浏览器中访问 localhost 来查看： 可以看到请求被代理到了 baidu.com，而且应该也可以看到 URL 地址没有变化，还是 localhost。 ","date":"5059-08-529","objectID":"/envoy_1/:2:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"3. 管理视图¶ Envoy 提供了一个管理视图，可以让我们去查看配置、统计信息、日志以及其他 Envoy 内部的一些数据。 我们可以通过添加其他的资源定义来配置 admin，其中也可以定义管理视图的端口，不过需要注意该端口不要和其他监听器配置冲突。 admin:access_log_path:/tmp/admin_access.logaddress:socket_address:{address: 0.0.0.0, port_value:9901} 当然我们也可以通过 Docker 容器将管理端口暴露给外部用户。上面的配置就会将管理页面暴露给外部用户，当然我们这里仅仅用于演示是可以的，如果你是用于线上环境还需要做好一些安全保护措施，可以查看 Envoy 的相关文档 了解更多安全配置。 要将管理页面也暴露给外部用户，我们使用如下命令运行另外一个容器： $ docker run --name=envoy-with-admin -d \\ -p 9901:9901 \\ -p 10000:10000 \\ -v $(pwd)/manifests/envoy.yaml:/etc/envoy/envoy.yaml \\ envoyproxy/envoy:latest 运行成功后，现在我们可以在浏览器里面输入 localhost:9901 来访问 Envoy 的管理页面： 迁移 NGINX 到 Envoy¶ 大部分的应用可能还是使用的比较传统的 Nginx 来做服务代理，本文我们将介绍如何将 Nginx 的配置迁移到 Envoy 上来。我们将学到： 如何设置 Envoy 代理配置 配置 Envoy 代理转发请求到外部服务 配置访问和错误日志 最后我们还会了解到 Envoy 代理的核心功能，以及如何将现有的 Nginx 配置迁移到 Envoy 上来。 ","date":"5059-08-529","objectID":"/envoy_1/:3:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"1. Nginx 示例¶ 这里我们使用 Nginx 官方 Wiki 的完整示例来进行说明，完整的 nginx.conf 配置如下所示： user www www; pid /var/run/nginx.pid; worker_processes 2; events { worker_connections 2000; } http { gzip on; gzip_min_length 1100; gzip_buffers 4 8k; gzip_types text/plain; log_format main '$remote_addr - $remote_user [$time_local] ' '\"$request\" $status $bytes_sent ' '\"$http_referer\" \"$http_user_agent\" ' '\"$gzip_ratio\"'; log_format download '$remote_addr - $remote_user [$time_local] ' '\"$request\" $status $bytes_sent ' '\"$http_referer\" \"$http_user_agent\" ' '\"$http_range\" \"$sent_http_content_range\"'; upstream targetCluster { 172.17.0.3:80; 172.17.0.4:80; } server { listen 8080; server_name one.example.com www.one.example.com; access_log /var/log/nginx.access_log main; error_log /var/log/nginx.error_log info; location / { proxy_pass http://targetCluster/; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; } } } 上面的 Nginx 配置有3个核心配置： 配置 Nginx 服务、日志结构和 Gzip 功能 配置 Nginx 在端口 8080 上接受对 one.example.com 域名的请求 根据不同的路径配置将流量转发给目标服务 并不是所有的 Nginx 的配置都适用于 Envoy，有些方面的配置我们可以不用关系。Envoy 代理主要有4种主要的配置类型，它们是支持 Nginx 提供的核心基础结构的： Listeners（监听器）：他们定义 Envoy 代理如何接收传入的网络请求，建立连接后，它会传递到一组过滤器进行处理 Filters（过滤器）：过滤器是处理传入和传出请求的管道结构的一部分，比如可以开启类似于 Gzip 之类的过滤器，该过滤器就会在将数据发送到客户端之前进行压缩 Routers（路由器）：这些路由器负责将流量转发到定义的目的集群去 Clusters（集群）：集群定义了流量的目标端点和相关配置。 我们将使用这4个组件来创建 Envoy 代理配置，去匹配 Nginx 中的配置。Envoy 的重点一直是在 API 和动态配置上，但是我们这里需要使用静态配置。 ","date":"5059-08-529","objectID":"/envoy_1/:4:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"2. Nginx 配置¶ 现在我们来仔细看下上面的 nginx.conf 配置文件的内容。 ","date":"5059-08-529","objectID":"/envoy_1/:5:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"工作连接¶ 首先是 Worker 连接数配置，主要是用于定义工作进程和连接的数量，用来配置 Nginx 扩展请求： worker_processes 2; events { worker_connections 2000; } Envoy 代理用另外一种不同的方式来管理工作进程和连接。Envoy 使用单进程-多线程的架构模型。一个 master 线程管理各种琐碎的任务，而一些 worker 线程则负责执行监听、过滤和转发。当监听器接收到一个连接请求时，该连接将其生命周期绑定到一个单独的 worker 线程。这使得 Envoy 主要使用大量单线程处理工作，并且只有少量的复杂代码用于实现 worker 线程之间的协调工作。通常情况下，Envoy 实现了100%的非阻塞。对于大多数工作负载，我们建议将 worker 线程数配置为物理机器的线程数。关于 Envoy 线程模型的更多信息，可以查看 Envoy 官方博客介绍：Envoy threading model ","date":"5059-08-529","objectID":"/envoy_1/:5:1","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"HTTP 配置¶ 然后 Nginx 配置的下一个部分是定义 HTTP 配置，比如： 定义支持哪些 MIME 类型 默认的超时时间 Gzip 配置 我们可以通过 Envoy 代理中的过滤器来配置这些内容。在 HTTP 配置部分，Nginx 配置指定了监听的端口 8080，并响应域名 one.example.com 和 www.one.example.com 的传入请求： server { listen 8080; server_name one.example.com www.one.example.com; ...... } 在 Envoy 中，这部分就是监听器来管理的。开始一个 Envoy 代理最重要的方面就是定义监听器，我们需要创建一个配置文件来描述我们如何去运行 Envoy 实例。 下面的配置将创建一个新的监听器并将其绑定到 8080 端口上，该配置指示了 Envoy 代理用于接收网络请求的端口。Envoy 配置使用的是 YAML 文件，如果你对 YAML 文件格式语法不是很熟悉的，可以点此先查看官方对应的介绍。 static_resources:listeners:- name:listener_0address:socket_address:{address: 0.0.0.0, port_value:8080} 需要注意的是我们没有在监听器部分定义 server_name，我们会在过滤器部分进行处理。 ","date":"5059-08-529","objectID":"/envoy_1/:5:2","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"Location 配置¶ 当请求进入 Nginx 时，location 部分定义了如何处理流量以及在什么地方转发流量。在下面的配置中，站点的所有传入流量都将被代理到一个名为 targetCluster 的上游（upstream）集群，上游集群定义了处理请求的节点。 location / { proxy_pass http://targetCluster/; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; } 在 Envoy 中，这部分将由过滤器来进行配置管理。在静态配置中，过滤器定义了如何处理传入的请求，在我们这里，将配置一个过滤器去匹配上一步中的 server_names，当接收到与定义的域名和路由匹配的传入请求时，流量将转发到集群，集群和 Nginx 配置中的 upstream 是一致的。 filter_chains:- filters:- name:envoy.http_connection_managerconfig:codec_type:autostat_prefix:ingress_httproute_config:name:local_routevirtual_hosts:- name:backenddomains:- \"one.example.com\"- \"www.one.example.com\"routes:- match:prefix:\"/\"route:cluster:targetClusterhttp_filters:- name:envoy.router 其中 envoy.http_connection_manager 是 Envoy 内置的一个过滤器，用于处理 HTTP 连接的，除此之外，还有其他的一些内置的过滤器，比如 Redis、Mongo、TCP。 ","date":"5059-08-529","objectID":"/envoy_1/:5:3","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"upstream 配置¶ 在 Nginx 中，upstream（上游）配置定义了处理请求的目标服务器集群，在我们这里的示例中，分配了两个集群。 upstream targetCluster { 172.17.0.3:80; 172.17.0.4:80; } 在 Envoy 代理中，这部分是通过集群进行配置管理的。upstream 等同与 Envoy 中的 clusters 定义，我们这里通过集群定义了主机被访问的方式，还可以配置超时和负载均衡等方面更精细的控制。 clusters:- name:targetClusterconnect_timeout:0.25s # 负载均衡type:STRICT_DNSdns_lookup_family:V4_ONLYlb_policy:ROUND_ROBIN # 负载均衡hosts:[{socket_address:{address: 172.17.0.3, port_value:80}},{socket_address:{address: 172.17.0.4, port_value:80}}] 上面我们配置了 STRICT_DNS 类型的服务发现，Envoy 会持续异步地解析指定的 DNS 目标。DNS 解析结果返回的每个 IP 地址都将被视为上游集群的主机。所以如果产线返回两个 IP 地址，则 Envoy 将认为集群由两个主机，并且两个主机都应进行负载均衡，如果从结果中删除了一个主机，则 Envoy 会从现有的连接池中将其剔出掉。 ","date":"5059-08-529","objectID":"/envoy_1/:5:4","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"日志配置¶ 最后需要配置的日志部分，Envoy 采用云原生的方式，将应用程序日志都输出到 stdout 和 stderr，而不是将错误日志输出到磁盘。 当用户发起一个请求时，访问日志默认是被禁用的，我们可以手动开启。要为 HTTP 请求开启访问日志，需要在 HTTP 连接管理器中包含一个 access_log 的配置，该路径可以是设备，比如 stdout，也可以是磁盘上的某个文件，这依赖于我们自己的实际情况。 下面过滤器中的配置就会将所有访问日志通过管理传输到 stdout： - name:envoy.http_connection_managerconfig:codec_type:autostat_prefix:ingress_httpaccess_log:- name:envoy.file_access_logconfig:path:\"/dev/stdout\"route_config:...... 默认情况下，Envoy 访问日志格式包含整个 HTTP 请求的详细信息： [%START_TIME%] \"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\" %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \"%REQ(X-FORWARDED-FOR)%\" \"%REQ(USER-AGENT)%\" \"%REQ(X-REQUEST-ID)%\" \"%REQ(:AUTHORITY)%\" \"%UPSTREAM_HOST%\"\\n 输出结果格式化后如下所示： [2020-04-08T04:51:00.281Z] \"GET / HTTP/1.1\" 200 - 0 58 4 1 \"-\" \"curl/7.47.0\" \"f21ebd42-6770-4aa5-88d4-e56118165a7d\" \"one.example.com\" \"172.18.0.4:80\" 我们也可以通过设置 format 字段来自定义输出日志的格式，例如： access_log:- name:envoy.file_access_logconfig:path:\"/dev/stdout\"format:\"[%START_TIME%] \"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\" %RESPONSE_CODE% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \"%REQ(X-REQUEST-ID)%\" \"%REQ(:AUTHORITY)%\" \"%UPSTREAM_HOST%\"\\n\" 此外我们也可以通过设置 json_format 字段来将日志作为 JSON 格式输出，例如： access_log:- name:envoy.file_access_logconfig:path:\"/dev/stdout\"json_format:{\"protocol\": \"%PROTOCOL%\", \"duration\": \"%DURATION%\", \"request_method\": \"%REQ(:METHOD)%\"} 要注意的是，访问日志会在未设置、或者空值的位置加入一个字符：-。不同类型的访问日志（例如 HTTP 和 TCP）共用同样的格式字符串。不同类型的日志中，某些字段可能会有不同的含义。有关 Envoy 日志的更多信息，可以查看官方文档对应的说明。当然日志并不是 Envoy 代理获得请求可见性的唯一方法，Envoy 还内置了高级跟踪和指标功能，我们会在后续章节中慢慢接触到。 ","date":"5059-08-529","objectID":"/envoy_1/:5:5","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"3. 测试¶ 现在我们已经将 Nginx 配置转换为了 Envoy 代理，接下来我们可以来启动 Envoy 代理进行测试验证。 在 Nginx 配置的顶部，有一行配置 user www www;，表示用非 root 用户来运行 Nginx 以提高安全性。而 Envoy 代理采用云原生的方法来管理使用这，我们通过容器启动 Envoy 代理的时候，可以指定一个低特权的用户。 下面的命令将通过 Docker 容器来启动一个 Envoy 实例，该命令使 Envoy 可以监听 80 端口上的流量请求，但是我们在 Envoy 的监听器配置中指定的是 8080 端口，所以我们用一个低特权用户身份来运行： $ docker run --name proxy1 -p 80:8080 --user 1000:1000 -v $(pwd)/manifests/envoy.yaml:/etc/envoy/envoy.yaml envoyproxy/envoy 启动代理后，就可以开始测试了，下面我们用 curl 命令使用代理配置的 host 头发起一个网络请求： $ curl -H \"Host: one.example.com\" localhost -i HTTP/1.1 503 Service Unavailable content-length: 91 content-type: text/plain date: Wed, 08 Apr 2020 04:25:59 GMT server: envoy upstream connect error or disconnect/reset before headers. reset reason: connection failure% 我们可以看到会出现 503 错误，这是因为我们配置的上游集群主机根本就没有运行，所以 Envoy 代理请求到不可用的主机上去了，就出现了这样的错误。我们可以使用下面的命令启动两个 HTTP 服务，用来表示上游主机： $ docker run -d cnych/docker-http-server; docker run -d cnych/docker-http-server; $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES fd3018535a52 cnych/docker-http-server \"/app\" 2 minutes ago Up 2 minutes 80/tcp practical_golick ace25541d654 cnych/docker-http-server \"/app\" 2 minutes ago Up 2 minutes 80/tcp lucid_hugle 3c83dfb9392f envoyproxy/envoy \"/docker-entrypoint.…\" 2 minutes ago Up 2 minutes 10000/tcp, 0.0.0.0:80-\u003e8080/tcp proxy1 当上面两个服务启动成功后，现在我们再通过 Envoy 去访问目标服务就正常了： $ curl -H \"Host: one.example.com\" localhost -i HTTP/1.1 200 OK date: Wed, 08 Apr 2020 04:32:01 GMT content-length: 58 content-type: text/html; charset=utf-8 x-envoy-upstream-service-time: 3 server: envoy \u003ch1\u003eThis request was processed by host: fd3018535a52\u003c/h1\u003e $ curl -H \"Host: one.example.com\" localhost -i HTTP/1.1 200 OK date: Wed, 08 Apr 2020 04:32:05 GMT content-length: 58 content-type: text/html; charset=utf-8 x-envoy-upstream-service-time: 0 server: envoy \u003ch1\u003eThis request was processed by host: ace25541d654\u003c/h1\u003e 当访问请求的时候，我们可以看到是哪个容器处理了请求，在 Envoy 代理容器中，也可以看到请求的日志输出： [2020-04-08T04:32:06.201Z] \"GET / HTTP/1.1\" 200 - 0 58 1 0 \"-\" \"curl/7.54.0\" \"ac61099b-f100-46a9-9c08-c323c5ac2320\" \"one.example.com\" \"172.17.0.3:80\" [2020-04-08T04:32:08.168Z] \"GET / HTTP/1.1\" 200 - 0 58 0 0 \"-\" \"curl/7.54.0\" \"15ee6ca9-b161-4630-a51c-c641d0760cd0\" \"one.example.com\" \"172.17.0.4:80\" 最后转换过后的完整的 Envoy 配置如下： static_resources:listeners:- name:listener_0address:socket_address:{address: 0.0.0.0, port_value:8080}filter_chains:- filters:- name:envoy.http_connection_managerconfig:codec_type:autostat_prefix:ingress_httpaccess_log:- name:envoy.file_access_logconfig:path:\"/dev/stdout\"route_config:name:local_routevirtual_hosts:- name:backenddomains:- \"one.example.com\"- \"www.one.example.com\"routes:- match:prefix:\"/\"route:cluster:targetClusterhttp_filters:- name:envoy.routerclusters:- name:targetClusterconnect_timeout:0.25stype:STRICT_DNSdns_lookup_family:V4_ONLYlb_policy:ROUND_ROBINhosts:[{socket_address:{address: 172.17.0.3, port_value:80}},{socket_address:{address: 172.17.0.4, port_value:80}}] 使用 SSL/TLS 保护流量¶ 本节我们将演示如何使用 Envoy 保护 HTTP 网络请求。确保 HTTP 流量安全对于保护用户隐私和数据是至关重要的。下面我们来了解下如何在 Envoy 中配置 SSL 证书。 ","date":"5059-08-529","objectID":"/envoy_1/:6:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"1. SSL 证书¶ 这里我们将为 example.com 域名生成一个自签名的证书，当然如果在生产环境时候，需要使用正规 CA 机构购买的证书，或者 Let's Encrypt 的免费证书服务。 下面的命令会在目录 certs/ 中创建一个新的证书和密钥： $ mkdir certs; cd certs; $ openssl req -nodes -new -x509 \\ -keyout example-com.key -out example-com.crt \\ -days 365 \\ -subj '/CN=example.com/O=youdianzhishi./C=CN'; Generating a RSA private key ..+++++ .................................................................................................................+++++ writing new private key to 'example-com.key' ----- $ cd - ","date":"5059-08-529","objectID":"/envoy_1/:7:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"2. 流量保护¶ 在 Envoy 中保护 HTTP 流量，需要通过添加 tls_context 过滤器，TLS 上下文提供了为 Envoy 代理中配置的域名指定证书的功能，请求 HTTPS 请求时候，就使用匹配的证书。我们这里直接使用上一步中生成的自签名证书即可。 我们这里的 Envoy 配置文件中包含了所需的 HTTPS 支持的配置，我们添加了两个监听器，一个监听器在 8080 端口上用于 HTTP 通信，另外一个监听器在 8443 端口上用于 HTTPS 通信。 在 HTTPS 监听器中定义了 HTTP 连接管理器，该代理将代理 /service/1 和 /service/2 这两个端点的传入请求，这里我们需要通过 tls_context 配置相关证书，如下所示： tls_context:common_tls_context:tls_certificates:- certificate_chain:filename:\"/etc/envoy/certs/example-com.crt\"private_key:filename:\"/etc/envoy/certs/example-com.key\" 在 TLS 上下文中定义了生成的证书和密钥，如果我们有多个域名，每个域名都有自己的证书，则需要通过 tls_certificates 定义多个证书链。 ","date":"5059-08-529","objectID":"/envoy_1/:8:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"3. 自动跳转¶ 定义了 TLS 上下文后，该站点将能够通过 HTTPS 提供流量了，但是如果用户是通过 HTTP 来访问的服务，为了确保安全，我们也可以将其重定向到 HTTPS 版本服务上去。 在 HTTP 配置中，我们将 https_redirect: true 的标志添加到过滤器的配置中即可实现跳转功能。 route_config:virtual_hosts:- name:backenddomains:- \"example.com\"routes:- match:prefix:\"/\"redirect:path_redirect:\"/\"https_redirect:true# 实现https跳转 当用户访问网站的 HTTP 版本时，Envoy 代理将根据过滤器配置来匹配域名和路径，匹配到过后将请求重定向到站点的 HTTPS 版本去。完整的 Envoy 配置如下所示： static_resources:listeners:- name:listener_httpaddress:socket_address:{address: 0.0.0.0, port_value:8080}filter_chains:- filters:- name:envoy.http_connection_managerconfig:codec_type:autostat_prefix:ingress_httproute_config:virtual_hosts:- name:backenddomains:- \"example.com\"routes:- match:prefix:\"/\"redirect:path_redirect:\"/\"https_redirect:truehttp_filters:- name:envoy.routerconfig:{}- name:listener_httpsaddress:socket_address:{address: 0.0.0.0, port_value:8443}filter_chains:- filters:- name:envoy.http_connection_managerconfig:codec_type:autostat_prefix:ingress_httproute_config:name:local_routevirtual_hosts:- name:backenddomains:- \"example.com\"routes:- match:prefix:\"/service/1\"route:cluster:service1- match:prefix:\"/service/2\"route:cluster:service2http_filters:- name:envoy.routerconfig:{}tls_context:common_tls_context:tls_certificates:- certificate_chain:filename:\"/etc/envoy/certs/example-com.crt\"private_key:filename:\"/etc/envoy/certs/example-com.key\"clusters:- name:service1connect_timeout:0.25stype:strict_dnslb_policy:round_robinhosts:- socket_address:address:172.17.0.3port_value:80- name:service2connect_timeout:0.25stype:strict_dnslb_policy:round_robinhosts:- socket_address:address:172.17.0.4port_value:80admin:access_log_path:/tmp/admin_access.logaddress:socket_address:address:0.0.0.0port_value:8001 ","date":"5059-08-529","objectID":"/envoy_1/:9:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"4. 测试¶ 现在配置已经完成了，我们就可以启动 Envoy 实例来进行测试了。在我们这个示例中，Envoy 暴露 80 端口来处理 HTTP 请求，暴露 443 端口来处理 HTTPS 请求，此外还在 8001 端口上暴露了管理页面，我们可以通过管理页面查看有关证书的信息。 使用如下命令启动 Envoy 代理： $ docker run -it --name proxy1 -p 80:8080 -p 443:8443 -p 8001:8001 -v $(pwd):/etc/envoy/ envoyproxy/envoy 启动完成后所有的 HTTPS 和 TLS 校验都是通过 Envoy 来进行处理的，所以我们不需要去修改应该程序。同样我们启动两个 HTTP 服务来处理传入的请求： $ docker run -d cnych/docker-http-server; docker run -d cnych/docker-http-server; 145738e12c174606f9e6e085ad2ec0ae9bf15a75d372b2bec8929e5d5df96be3 8f9a9355333d91b06a14d2bccc1a0d4a9afd20b258df561278fb94f01cdcd881 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8f9a9355333d cnych/docker-http-server \"/app\" 5 seconds ago Up 4 seconds 80/tcp eager_kapitsa 145738e12c17 cnych/docker-http-server \"/app\" 6 seconds ago Up 5 seconds 80/tcp beautiful_hermann a499a8ccaedc envoyproxy/envoy \"/docker-entrypoint.…\" About a minute ago Up About a minute 0.0.0.0:8001-\u003e8001/tcp, 10000/tcp, 0.0.0.0:80-\u003e8080/tcp, 0.0.0.0:443-\u003e8443/tcp proxy1 上面的几个容器启动完成后，就可以进行测试了，首先我们请求 HTTP 的服务，由于配置了自动跳转，所以应该会被重定向到 HTTPS 的版本上去： $ curl -H \"Host: example.com\" http://localhost -i HTTP/1.1 301 Moved Permanently # 重定向响应 location: https://example.com/ date: Wed, 08 Apr 2020 06:53:51 GMT server: envoy content-length: 0 我们可以看到上面有 HTTP/1.1 301 Moved Permanently 这样的重定向响应信息。然后我们尝试直接请求 HTTPS 的服务： $ curl -k -H \"Host: example.com\" https://localhost/service/1 -i HTTP/1.1 200 OK date: Wed, 08 Apr 2020 06:55:27 GMT content-length: 58 content-type: text/html; charset=utf-8 x-envoy-upstream-service-time: 0 server: envoy \u003ch1\u003eThis request was processed by host: 145738e12c17\u003c/h1\u003e $ curl -k -H \"Host: example.com\" https://localhost/service/2 -i HTTP/1.1 200 OK date: Wed, 08 Apr 2020 06:55:49 GMT content-length: 58 content-type: text/html; charset=utf-8 x-envoy-upstream-service-time: 0 server: envoy \u003ch1\u003eThis request was processed by host: 8f9a9355333d\u003c/h1\u003e 我们可以看到通过 HTTPS 进行访问可以正常得到对应的响应，需要注意的是由于我们这里使用的是自签名的证书，所以需要加上 -k 参数来忽略证书校验，如果没有这个参数则在请求的时候会报错： $ curl -H \"Host: example.com\" https://localhost/service/2 -i curl: (60) SSL certificate problem: self signed certificate More details here: https://curl.haxx.se/docs/sslcerts.html curl performs SSL certificate verification by default, using a \"bundle\" of Certificate Authority (CA) public keys (CA certs). If the default bundle file isn't adequate, you can specify an alternate file using the --cacert option. If this HTTPS server uses a certificate signed by a CA represented in the bundle, the certificate verification probably failed due to a problem with the certificate (it might be expired, or the name might not match the domain name in the URL). If you'd like to turn off curl's verification of the certificate, use the -k (or --insecure) option. HTTPS-proxy has similar options --proxy-cacert and --proxy-insecure. 我们也可以通过管理页面去查看证书相关的信息，上面我们启动容器的时候绑定了宿主机的 8001 端口，所以我们可以通过访问 http://localhost:8001/certs 来获取到证书相关的信息： 基于文件的动态配置¶ Envoy 除了支持静态配置之外，还支持动态配置，而且动态配置也是 Envoy 重点关注的功能，本节我们将学习如何将 Envoy 静态配置转换为动态配置，从而允许 Envoy 自动更新。 ","date":"5059-08-529","objectID":"/envoy_1/:10:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"1. Envoy 动态配置¶ 前面的章节中，我们都是直接使用的静态配置，但是当我们需要更改配置的时候就比较麻烦了，需要重启 Envoy 代理才会生效。要解决这个问题，我们可以将静态配置更改成动态配置，当我们使用动态配置的时候，更改了配置，Envoy 将会自动去重新加载配置。 Envoy 支持不同的模块进行动态配置，可配置的有如下几个 API： EDS：端点发现服务（EDS）可以让 Envoy 自动发现上游集群的成员，这使得我们可以动态添加或者删除处理流量请求的服务。 CDS：集群发现服务（CDS）可以让 Envoy 通过该机制自动发现在路由过程中使用的上游集群。 RDS：路由发现服务（RDS）可以让 Envoy 在运行时自动发现 HTTP 连接管理过滤器的整个路由配置，这可以让我们来完成诸如动态更改流量分配或者蓝绿发布之类的功能。 VHDS：虚拟主机发现服务（VHDS）允许根据需要与路由配置本身分开请求属于路由配置的虚拟主机。该 API 通常用于路由配置中有大量虚拟主机的部署中。 SRDS：作用域路由发现服务（SRDS）允许将路由表分解为多个部分。该 API 通常用于具有大量路由表的 HTTP 路由部署中。 LDS：监听器发现服务（LDS）可以让 Envoy 在运行时自动发现整个监听器。 SDS：密钥发现服务（SDS）可以让 Envoy 自动发现监听器的加密密钥（证书、私钥等）以及证书校验逻辑（受信任的根证书、吊销等）。 可以使用普通的文件来进行动态配置，也可以通过 REST-JSON 或者 gRPC 端点来提供。我们可以在 xDS 配置概述文档 中找到更多相关 API 的介绍。 在接下来的步骤中，我们将先更改配置来使用 EDS，让 Envoy 根据配置文件的数据来动态添加节点。 ","date":"5059-08-529","objectID":"/envoy_1/:11:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"Cluster ID¶ 首先我们这里定义了一个基本的 Envoy 配置文件，如下所示：(envoy.yaml) admin:access_log_path:\"/dev/null\"address:socket_address:address:0.0.0.0port_value:9901static_resources:listeners:- name:listener_0address:socket_address:{address: 0.0.0.0, port_value:10000}filter_chains:- filters:- name:envoy.http_connection_managerconfig:codec_type:autostat_prefix:ingress_httproute_config:name:local_routevirtual_hosts:- name:backenddomains:- \"*\"routes:- match:prefix:\"/\"route:cluster:targetClusterhttp_filters:- name:envoy.router 我们可以看到现在还没有配置 clusters 集群部分，这是因为我们要通过使用 EDS 来进行自动发现。 首先我们需要添加一个节点让 Envoy 来识别并应用这一个唯一的配置，将下面的配置放置在 envoy.yaml 文件的顶部区域： node:id:id_1cluster:test 除了 id 和 cluster 之外，我们还可以配置基于区域的一些位置信息来进行声明，比如 region、zone、sub_zone。 ","date":"5059-08-529","objectID":"/envoy_1/:11:1","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"2. EDS 配置¶ 接下来我们就可以来定义 EDS 配置了，可以来动态控制上游集群数据。在前面章节中，这部分的静态配置是这样的： clusters:- name:targetClusterconnect_timeout:0.25stype:STRICT_DNSdns_lookup_family:V4_ONLYlb_policy:ROUND_ROBINhosts:[{socket_address:{address: 172.17.0.3, port_value:80}},{socket_address:{address: 172.17.0.4, port_value:80}}] 现在我们将上面的静态配置转换成动态配置，首先需要转换为基于 EDS 的 eds_cluster_config 属性，并将类型更改为 EDS，将下面的集群配置添加到 Envoy 配置的末尾： clusters:- name:targetClusterconnect_timeout:0.25slb_policy:ROUND_ROBINtype:EDSeds_cluster_config:service_name:localservices # 可选，代替集群的名称，提供给 EDS 服务eds_config:# 集群的 EDS 更新源配置path:'/etc/envoy/eds.yaml' 上游的服务器 172.17.0.3 和 172.17.0.4 就将来自于 /etc/envoy/eds.yaml 文件，创建一个 eds.yaml 文件，内容如下所示： version_info:\"0\"resources:- \"@type\": \"type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\"cluster_name:\"localservices\"endpoints:- lb_endpoints:- endpoint:address:socket_address:address:\"172.17.0.3\"port_value:80 上面我们只定义了 172.17.0.3 这一个端点。 现在配置完成后，我们可以启动 Envoy 代理来进行测试。执行下面的命令启动 Envoy 容器： $ docker run --name=proxy-eds -d \\ -p 9901:9901 \\ -p 80:10000 \\ -v $(pwd)/manifests:/etc/envoy \\ envoyproxy/envoy:latest 然后同样和前面一样运行两个 HTTP 服务来作为上游服务器： $ docker run -d cnych/docker-http-server; docker run -d cnych/docker-http-server; $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b9f1955e2704 envoyproxy/envoy:latest \"/docker-entrypoint.…\" 4 seconds ago Up 2 seconds 0.0.0.0:9901-\u003e9901/tcp, 0.0.0.0:80-\u003e10000/tcp proxy-eds 4fd8eb3bd415 cnych/docker-http-server \"/app\" About a minute ago Up 59 seconds 80/tcp wonderful_hoover 73b616391920 cnych/docker-http-server \"/app\" About a minute ago Up About a minute 80/tcp pedantic_moser 根据上面的 EDS 配置，Envoy 将把所有的流量都发送到 172.17.0.3 这一个节点上去，我们可以使用 curl localhost 来测试下： $ curl localhost \u003ch1\u003eThis request was processed by host: 73b616391920\u003c/h1\u003e $ curl localhost \u003ch1\u003eThis request was processed by host: 73b616391920\u003c/h1\u003e 接下来我们来尝试更新上面的 EDS 配置添加上另外的一个节点，观察 Envoy 代理是否会自动生效。 由于我们这里使用的是 EDS 动态配置，所以当我们要扩展上游服务的时候，只需要将新的端点添加到上面我们指定的 eds.yaml 配置文件中即可，然后 Envoy 就会自动将新添加的端点包含进来。用上面同样的方式添加 172.17.0.4 这个端点，eds.yaml 内容如下所示： version_info:\"0\"resources:- \"@type\": \"type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\"cluster_name:\"localservices\"endpoints:- lb_endpoints:- endpoint:address:socket_address:address:\"172.17.0.3\"port_value:80- endpoint:address:socket_address:address:\"172.17.0.4\"port_value:80 由于我们这里是使用的 Docker 容器将配置文件挂载到容器中的，如果直接更改宿主机的配置文件，有时候可能不会立即触发文件变更，我们可以使用如下所示的命令来强制变更： $ mv manifests/eds.yaml tmp; mv tmp manifests/eds.yaml 这个时候正常情况下 Envoy 就会自动重新加载配置并将新的端点添加到负载均衡中去，这个时候我们再来访问代理： $ curl localhost \u003ch1\u003eThis request was processed by host: 73b616391920\u003c/h1\u003e $ curl localhost \u003ch1\u003eThis request was processed by host: 4fd8eb3bd415\u003c/h1\u003e 可以看到已经可以自动访问到另外的端点去了。 注意 我在测试阶段发现在 Mac 系统下面没有自动热加载，在 Linux 系统下面是可以正常重新加载的。 ","date":"5059-08-529","objectID":"/envoy_1/:12:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"3. CDS 配置¶ 现在已经配置好了 EDS，接下来我们就可以去扩大上游集群的规模了，如果我们想要能够动态添加新的域名和集群，就需要实现集群发现服务（CDS）API，在下面的示例中，我们将配置集群发现服务（CDS）和监听器发现服务（LDS）来进行动态配置。 创建一个名为 cds.yaml 的文件来配置集群服务发现的数据，文件内容如下所示： version_info:\"0\"resources:- \"@type\": \"type.googleapis.com/envoy.api.v2.Cluster\"name:targetClusterconnect_timeout:0.25slb_policy:ROUND_ROBINtype:EDSeds_cluster_config:service_name:localserviceseds_config:path:/etc/envoy/eds.yaml 此外，还需要创建一个名为 lds.yaml 的文件来放置监听器的配置，文件内容如下所示： version_info:\"0\"resources:- \"@type\": \"type.googleapis.com/envoy.api.v2.Listener\"name:listener_0address:socket_address:{address: 0.0.0.0, port_value:10000}filter_chanins:- filters:- name:envoy.http_connection_managerconfig:stat_prefix:ingress_httpcodec_type:AUTOroute_config:name:local_routevirtual_hosts:- name:local_servicedomains:[\"*\"]routes:- match:prefix:\"/\"route:cluster:targetClusterhttp_filters:- name:envoy.router 仔细观察可以发现 cds.yaml 和 lds.yaml 配置文件的内容基本上和上面的静态配置文件一致的。我们这里只是将集群和监听器拆分到外部文件中去，这个时候我们需要修改 Envoy 的配置来引用这些文件，我们可以通过将 static_resources 更改为 dynamic_resources 来进行配置。 重新新建一个 Envoy 配置文件，命名为 envoy1.yaml，内容如下所示： node:id:id_1cluster:testadmin:access_log_path:\"/dev/null\"address:socket_address:address:0.0.0.0port_value:9901# 动态配置dynamic_resources:lds_config:path:\"/etc/envoy/lds.yaml\"cds_config:path:\"/etc/envoy/cds.yaml\" 然后使用上面的配置文件重新启动一个新的 Envoy 代理，命令如下所示： $ docker run --name=proxy-xds -d \\ -p 9902:9901 \\ -p 81:10000 \\ -v $(pwd)/manifests:/etc/envoy \\ -v $(pwd)/manifests/envoy1.yaml:/etc/envoy/envoy.yaml \\ envoyproxy/envoy:latest 注意 为了避免和前面的 Envoy 实例端口冲突，这里我都修改了和宿主机上绑定的端口。 启动完成后，同样可以访问 Envoy 代理来测试是否生效了： $ curl localhost:81 \u003ch1\u003eThis request was processed by host: 4fd8eb3bd415\u003c/h1\u003e $ curl localhost:81 \u003ch1\u003eThis request was processed by host: 73b616391920\u003c/h1\u003e 现在我们基于上面配置的 CDS、LDS、EDS 的配置来动态添加一个新的集群。现在我们添加一个名为 newTargetCluster 的集群，内容如下所示： version_info:\"0\"resources:- \"@type\": \"type.googleapis.com/envoy.api.v2.Cluster\"name:targetClusterconnect_timeout:0.25slb_policy:ROUND_ROBINtype:EDSeds_cluster_config:service_name:localserviceseds_config:path:/etc/envoy/eds.yaml- \"@type\": \"type.googleapis.com/envoy.api.v2.Cluster\"name:newTargetClusterconnect_timeout:0.25slb_policy:ROUND_ROBINtype:EDSeds_cluster_config:service_name:localserviceseds_config:path:/etc/envoy/eds1.yaml 上面我们新增了一个新的集群，对应的 eds_config 配置文件是 eds1.yaml，所以我们同样需要去创建该文件去配置新的端点服务数据，内容如下所示： version_info:\"0\"resources:- \"@type\": \"type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\"cluster_name:\"localservices\"endpoints:- lb_endpoints:- endpoint:address:socket_address:address:\"172.17.0.6\"port_value:80- endpoint:address:socket_address:address:\"172.17.0.7\"port_value:80 这个时候新的集群添加上了，但是还没有任何路由来使用这个新集群，我们可以在 lds.yaml 中去配置，将之前配置的 targetCluster 替换成 newTargetCluster。 当然同样我们这里还需要运行两个简单的 HTTP 服务来作为上游服务提供服务，执行如下所示的命令： $ docker run -d cnych/docker-http-server; docker run -d cnych/docker-http-server; 上面的配置完成后，我们可以执行如下所示的命令来强制动态配置文件更新： $ mv manifests/cds.yaml tmp; mv tmp manifests/cds.yaml; mv manifests/lds.yaml tmp; mv tmp manifests/lds.yaml 这个时候 Envoy 应该就会自动重新加载并添加新的集群，我们同样可以执行 curl localhost:81 命令来验证： $ curl localhost:81 \u003ch1\u003eThis request was processed by host: f92b16426da5\u003c/h1\u003e $ curl localhost:81 \u003ch1\u003eThis request was processed by host: d89d590082dc\u003c/h1\u003e 可以看到已经变成了新的两个端点数据了。 基于 API 的动态端点发现¶ 当在 Envoy 配置中定义了上游集群后，Envoy 需要知道如何解析集群成员，这就是服务发现。端点发现服务（EDS）是 Envoy 基于 gRPC 或者用来获取集群成员的 REST-JSON API 服务的 xDS 管理服务。在本节我们将学习如何使用 REST-JSOn API 来配置端点的自动发现。 ","date":"5059-08-529","objectID":"/envoy_1/:13:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"1. 介绍¶ 在前面的章节中，我们使用文件来定义了静态和动态配置，在这里我们将介绍另外一种方式来进行动态配置：API 动态配置。 端点发现服务（EDS）是 Envoy 基于 gRPC 或者用来获取集群成员的 REST-JSON API 服务的 xDS 管理服务，集群成员在 Envoy 术语中成为端点，对于每个集群，Envoy 都从发现服务中获取端点。其中 EDS 就是最常用的服务发现机制，因为下面几个原因： Envoy 对每个上游主机都有一定的了解（相对于通过 DNS 解析的负载均衡器进行路由），可以做出更加智能的负载均衡策略。 发现 API 返回的每个主机的一些属性会将主机的负载均衡权重、金丝雀状态、区域等等告知 Envoy，这个额外的属性在负载均衡、统计数据收集等会被 Envoy 网格在全局中使用到 Envoy 项目在 Java 和 Golang 中都提供了 EDS 和其他服务发现的 gRPC 实现参考 接下来我们将更改配置来使用 EDS，从而允许基于来自 REST-JSON API 服务的数据进行动态添加节点。 ","date":"5059-08-529","objectID":"/envoy_1/:14:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"2. EDS 配置¶ 下面是提供的一个 Envoy 配置的初始配置 envoy.yaml，文件内容如下所示： admin:access_log_path:/dev/nulladdress:socket_address:address:127.0.0.1port_value:9000node:cluster:myclusterid:test-idstatic_resources:listeners:- name:listener_0address:socket_address:{address: 0.0.0.0, port_value:10000}filter_chains:- filters:- name:envoy.http_connection_managerconfig:stat_prefix:ingress_httpcodec_type:AUTOroute_config:name:local_routevirtual_hosts:- name:local_servicedomains:[\"*\"]routes:- match:{prefix:\"/\"}route:{cluster:targetCluster }http_filters:- name:envoy.router 接下来需要添加一个 EDS 类型的集群配置，并在 eds_config 中配置使用 REST API： clusters:- name:targetClustertype:EDSconnect_timeout:0.25seds_cluster_config:service_name:myserviceeds_config:api_config_source:api_type:REST cluster_names:[eds_cluster]refresh_delay:5s 然后需要定义 eds_cluster 的解析方式，这里我们可以使用静态配置： - name:eds_clustertype:STATICconnect_timeout:0.25shosts:[{socket_address:{address: 172.17.0.4, port_value:8080}}] 然后同样启动一个 Envoy 代理实例来进行测试： $ docker run --name=api-eds -d \\ -p 9901:9901 \\ -p 80:10000 \\ -v $(pwd)/manifests:/etc/envoy \\ envoyproxy/envoy:latest 然后启动一个如下所示的上游端点服务： $ docker run -p 8081:8081 -d -e EDS_SERVER_PORT='8081' cnych/docker-http-server:v4 启动完成后我们可以使用如下命令来测试上游的端点服务： $ curl http://localhost:8081 -i HTTP/1.0 200 OK Content-Type: text/html; charset=utf-8 Content-Length: 36 Server: Werkzeug/0.15.4 Python/2.7.16 Date: Tue, 14 Apr 2020 06:32:56 GMT 355d92db-9295-4a22-8b2c-fc0e5956ecf6 现在我们启动了 Envoy 代理和上游的服务集群，但是由于我们这里启动的服务并不是 eds_cluster 中配置的服务，所以还没有连接它们。这个时候我们去查看 Envoy 代理得日志，可以看到如下所示的一些错误： $ docker logs -f api-eds [2020-04-14 06:50:07.334][1][warning][config] [source/common/config/http_subscription_impl.cc:110] REST update for /v2/discovery:endpoints failed ...... ","date":"5059-08-529","objectID":"/envoy_1/:15:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"3. 启动 EDS¶ 为了让 Envoy 获取端点服务，我们需要启动 eds_cluster，我们这里将使用 python 实现的一个示例 eds_server。 使用如下所示的命令来启动 eds_server 服务： $ docker run -p 8080:8080 -d cnych/eds_server 服务启动后，可以在服务日志中查看到如下所示的日志信息，表明一个 Envoy 发现请求成功： * Serving Flask app \"main\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: on * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit) * Restarting with stat * Debugger is active! * Debugger PIN: 185-412-562 172.17.0.2 - - [14/Apr/2020 07:12:00] \"POST /v2/discovery:endpoints HTTP/1.1\" 200 - Inbound v2 request for discovery. POST payload: {u'node': {u'user_agent_name': u'envoy', u'cluster': u'mycluster', u'extensions': [{......}], u'user_agent_build_version': {u'version': {u'minor_number': 14, u'major_number': 1, u'patch': 1}, u'metadata': {u'ssl.version': u'BoringSSL', u'build.type': u'RELEASE', u'revision.status': u'Clean', u'revision.sha': u'3504d40f752eb5c20bc2883053547717bcb92fd8'}}, u'build_version': u'3504d40f752eb5c20bc2883053547717bcb92fd8/1.14.1/Clean/RELEASE/BoringSSL', u'id': u'test-id'}, u'type_url': u'type.googleapis.com/envoy.api.v2.ClusterLoadAssignment', u'resource_names': [u'myservice'], u'version_info': u'v1'} 172.17.0.2 - - [14/Apr/2020 07:12:08] \"POST /v2/discovery:endpoints HTTP/1.1\" 200 - 现在我们就可以将上游的服务配置添加到 EDS 服务中去了，这样可以让 Envoy 来自动发现上游服务。 我们在 Envoy 配置中将服务定义为了 myservice，所以我们需要针对该服务注册一个端点： $ curl -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' -d '{ \"hosts\": [ { \"ip_address\": \"172.17.0.3\", \"port\": 8081, \"tags\": { \"az\": \"cn-beijing-a\", \"canary\": false, \"load_balancing_weight\": 50 } } ] }' http://localhost:8080/edsservice/myservice 由于我们已经启动了上面注册的上游服务，所以现在我们可以通过 Envoy 代理访问到它了： $ curl -i http://localhost HTTP/1.1 200 OK content-type: text/html; charset=utf-8 content-length: 36 server: envoy date: Tue, 14 Apr 2020 07:33:04 GMT x-envoy-upstream-service-time: 4 355d92db-9295-4a22-8b2c-fc0e5956ecf6 接下来我们在上游集群中运行更多的节点，并调用 API 来进行动态注册，使用如下所示的命令来向上游集群再添加4个节点： for i in 8082 8083 8084 8085 do docker run -d -e EDS_SERVER_PORT=$i cnych/docker-http-server:v4; sleep .5 done 然后将上面的4个节点注册到 EDS 服务上面去，同样使用如下所示的 API 接口调用： $ curl -X PUT --header 'Content-Type: application/json' --header 'Accept: application/json' -d '{ \"hosts\": [ { \"ip_address\": \"172.17.0.3\", \"port\": 8081, \"tags\": { \"az\": \"cn-beijing-a\", \"canary\": false, \"load_balancing_weight\": 50 } }, { \"ip_address\": \"172.17.0.5\", \"port\": 8082, \"tags\": { \"az\": \"cn-beijing-a\", \"canary\": false, \"load_balancing_weight\": 50 } }, { \"ip_address\": \"172.17.0.6\", \"port\": 8083, \"tags\": { \"az\": \"cn-beijing-a\", \"canary\": false, \"load_balancing_weight\": 50 } }, { \"ip_address\": \"172.17.0.7\", \"port\": 8084, \"tags\": { \"az\": \"cn-beijing-a\", \"canary\": false, \"load_balancing_weight\": 50 } }, { \"ip_address\": \"172.17.0.8\", \"port\": 8085, \"tags\": { \"az\": \"cn-beijing-a\", \"canary\": false, \"load_balancing_weight\": 50 } } ] }' http://localhost:8080/edsservice/myservice 注册成功后，我们可以通过如下所示的命令来验证网络请求是否与注册的节点之间是均衡的： $ while true; do curl http://localhost; sleep .5; printf '\\n'; done d671262d-39b5-4150-9e25-94fb4f733959 dd1519ef-e03a-4708-bcd1-71890d38e40c b0c218f0-99f4-43e4-87fc-8989d49fccec 355d92db-9295-4a22-8b2c-fc0e5956ecf6 d671262d-39b5-4150-9e25-94fb4f733959 34690963-0887-4d36-8776-c35cf37fa901 ...... 根据上面的输出结果可以看到每次请求的服务是不同的响应，我们一共注册了5个端点服务。 现在我们来通过 API 删除 EDS 服务上面注册的主机来测试下，执行如下所示的命令清空 hosts： $ curl -X PUT --header 'Content-Type: application/json' --header 'Accept: application/json' -d '{ \"hosts\": [] }' http://localhost:8080/edsservice/myservice 现在如果我们尝试向 Envoy 发送请求，我们将会看到如下所示的不健康的日志信息： $ curl -v http://localhost * Rebuilt URL to: http://localhost/ * Trying ::1... * TCP_NODELAY set * Connected to localhost (::1) port 80 (#0) \u003e GET / HTTP/1.1 \u003e Host: localhost \u003e User-Agent: curl/7.54.0 \u003e Accept: */* \u003e \u003c HTTP/1.1 503 Service Unavailable \u003c content-length: 19 \u003c content-type: text/plain \u003c date: Tue, 14 Apr 2020 07:50:06 GMT \u003c server: envoy \u003c * Conne","date":"5059-08-529","objectID":"/envoy_1/:16:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"1. 代理配置¶ 首先创建一个 Envoy 配置文件 envoy.yaml，配置将任何域名的请求都代理到 172.17.0.3 和 172.17.0.4 这两个上游服务去。完整的配置如下所示： static_resources:listeners:- name:listener_0address:socket_address:{address: 0.0.0.0, port_value:8080}filter_chains:- filters:- name:envoy.http_connection_managerconfig:codec_type:autostat_prefix:ingress_httproute_config:name:local_routevirtual_hosts:- name:backenddomains:- \"*\"routes:- match:prefix:\"/\"route:cluster:targetClusterhttp_filters:- name:envoy.routerclusters:- name:targetClusterconnect_timeout:0.25stype:STRICT_DNSdns_lookup_family:V4_ONLYlb_policy:ROUND_ROBINhosts:[{socket_address:{address: 172.17.0.3, port_value:80}},{socket_address:{address: 172.17.0.4, port_value:80}}] 假设目前 172.17.0.3 这个上游服务出现了故障，现在的 Envoy 代理还是会继续向该服务转发流量过来的，这样当用户访问服务的时候就会遇到不可用的情况。对于这种情况我们更希望的是 Envoy 能够检测到服务不可用的时候自动将其从节点中移除掉，这其实就可以通过向集群中添加健康检查来完成。 ","date":"5059-08-529","objectID":"/envoy_1/:17:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"2. 添加健康检查¶ 健康检查可以添加到 Envoy 的集群配置中，如下所示的配置将在定义的每个节点内使用 /health 端点来进行健康检查，Envoy 会根据端点返回的 HTTP 状态来确定其是否健康。 health_checks:- timeout:1sinterval:10sinterval_jitter:1sunhealthy_threshold:6healthy_threshold:1http_health_check:path:\"/health\" 这里我们简单对上面配置的健康检查的关键字段进行下说明： interval：执行一次健康检查的时间间隔 unhealthy_threshold：将主机标记为不健康状态之前需要进行的不健康状态检查数量（相当于就是检测到几次不健康就认为是不健康的） healthy_threshold：将主机标记为健康状态之前需要进行的健康状态检查数量（相当于就是检测到几次健康就认为是健康的） http_health_check.path：用于健康检查请求的路径 关于健康检查的更多字段介绍可以查看官方的文档说明：https://www.envoyproxy.io/docs/envoy/latest/api-v2/api/v2/core/health_check.proto ","date":"5059-08-529","objectID":"/envoy_1/:18:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"3. 启动代理¶ 添加了健康检查之后，Envoy 将检查集群中定义的每个节点的运行状况。同样使用如下所示的命令启动 Envoy 代理： $ docker run -d --name proxy1 -p 80:8080 -v $(pwd)/manifests:/etc/envoy envoyproxy/envoy:latest 然后启动两个节点，都处于正常运行状态： $ docker run -d cnych/docker-http-server:healthy; docker run -d cnych/docker-http-server:healthy; 启动完成后，我们可以向 Envoy 发送请求，正常都可以从上面的两个上游服务中返回正常的请求： $ curl localhost -i HTTP/1.1 200 OK date: Wed, 15 Apr 2020 04:13:01 GMT content-length: 63 content-type: text/html; charset=utf-8 x-envoy-upstream-service-time: 0 server: envoy \u003ch1\u003eA healthy request was processed by host: b6336e79951d\u003c/h1\u003e $ curl localhost -i HTTP/1.1 200 OK date: Wed, 15 Apr 2020 04:13:02 GMT content-length: 63 content-type: text/html; charset=utf-8 x-envoy-upstream-service-time: 0 server: envoy \u003ch1\u003eA healthy request was processed by host: 9a4c07cc4306\u003c/h1\u003e ","date":"5059-08-529","objectID":"/envoy_1/:19:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"4. 测试¶ 接下来我们来测试下 Envoy 是如何处理不正常的节点的。在一个独立的命令行终端中，启动一个循环来发送请求，可以让我们来观察状态变化： $ while true; do curl localhost; sleep .5; done ...... 然后使用如下命令，我们可以来确定哪个 Docker 容器的 IP 为 172.17.0.3，然后将这个节点变成不健康的，然后 Envoy 就会自动将其从负载均衡中移除掉。 $ docker ps -q | xargs -n 1 docker inspect --format '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}} {{ .Config.Hostname }}' | sed 's/ \\// /' 172.17.0.4 b6336e79951d 172.17.0.3 9a4c07cc4306 172.17.0.2 6928df882c4f 要让该一个节点变成不健康的状态，我们可以直接请求 unhealthy 的端点： $ curl 172.18.0.3/unhealthy 这个时候可以看到另外一个终端中循环请求的日志信息中就出现了 unhealthy 的相关信息： ...... \u003ch1\u003eA unhealthy request was processed by host: 9a4c07cc4306\u003c/h1\u003e \u003ch1\u003eA unhealthy request was processed by host: 9a4c07cc4306\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: b6336e79951d\u003c/h1\u003e \u003ch1\u003eA unhealthy request was processed by host: 9a4c07cc4306\u003c/h1\u003e ...... 这个时候访问该容器就会返回 500 状态码了： $ curl 172.18.0.3 -i HTTP/1.1 500 Internal Server Error Date: Wed, 15 Apr 2020 04:23:59 GMT Content-Length: 65 Content-Type: text/html; charset=utf-8 \u003ch1\u003eA unhealthy request was processed by host: 9a4c07cc4306\u003c/h1\u003e 在这段时间内，Envoy会将请求发送到健康检查的端点。如果健康检查的端点发生了故障，它将继续向该服务发送流量，直到达到 unhealthy_threshold 这么多次不健康的请求，此时，Envoy 将从负载均衡器中将其删除。这个时候可以看到另外一个终端中循环请求的日志信息中就只有一个容器的信息了： ...... \u003ch1\u003eA healthy request was processed by host: b6336e79951d\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: b6336e79951d\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: b6336e79951d\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: b6336e79951d\u003c/h1\u003e ...... 与此同时，Envoy 还会继续检查健康状态的端点，来查看它是否再次变得可用了，一旦可用，它将又会被添加回到 Envoy 的上游服务器集群中去。 我们可以访问下上面不健康容器的 healthy 端点让其变成正常运行状态： $ curl 172.17.0.3/healthy 我们健康检查的间隔是10s，healthy_threshold 阈值是1，所以检测到成功后 Envoy 就会将该容器再次添加回来。这个时候可以看到另外一个终端中循环请求的日志信息中就又出现了两个容器的信息： ...... \u003ch1\u003eA healthy request was processed by host: 9a4c07cc4306\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 9a4c07cc4306\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: b6336e79951d\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: b6336e79951d\u003c/h1\u003e ...... 接下来我们再来测试下所有服务均不可用时发生的情况。目前已经有两个运行正常的上游服务器，Envoy 代理会在它们之间进行负载均衡。 和上面方法一样，对两个上游服务访问 unhealthy 端点，这样就可以将两个服务变成不健康的状态： $ curl 172.18.0.3/unhealthy $ curl 172.18.0.4/unhealthy 现在两个上游服务都已经不健康了，所以当我们请求 Envoy 时，将得到如下所示的信息： $ curl localhost -i HTTP/1.1 500 Internal Server Error date: Wed, 15 Apr 2020 06:19:01 GMT content-length: 65 content-type: text/html; charset=utf-8 x-envoy-upstream-service-time: 0 server: envoy \u003ch1\u003eA unhealthy request was processed by host: b6336e79951d\u003c/h1\u003e ","date":"5059-08-529","objectID":"/envoy_1/:20:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"5. 被动健康检查¶ 和前面的主动健康检查不同，被动健康检查从真实的请求响应来确定端点是否健康。一旦端点被删除后，Envoy 将使用基于超时的方法进行重新插入，使用该方法可以通过配置 interval 将不正常的主机重新添加到集群中去，后续的每次删除都会增加一定的时间间隔，这样的话不健康的端点对用户的流量影响就会尽可能小。 和前面的主动健康检查一样，被动健康检查也需要针对每个集群进行配置。如下所示的配置表示返回3个连续的5xx错误时，该配置会将主机删除30s： outlier_detection:consecutive_5xx:\"3\"base_ejection_time:\"30s\" consecutive_5xx：表示上游主机返回一定数量的连续 5xx 状态，则将其移除。需要注意的是在这种情况下，5xx表示实际的5xx响应码值，或者是一个导致 HTTP 路由器返回一个上游的事件行为（比如重置、连接失败等） base_ejection_time：表示移除主机的基准时间。真实的时间等于基准时间乘以主机移除的次数，默认为 30000ms 或 30s。 当启用被动健康检查过后，Envoy 会根据实际的请求响应来删除主机。同样首先我们先运行两个新的上游节点: $ docker run -d cnych/docker-http-server:healthy; docker run -d cnych/docker-http-server:healthy; 然后启动一个新的 Envoy 代理，对应的配置文件为 envoy1.yaml，内容如下所示： static_resources:listeners:- name:listener_0address:socket_address:{address: 0.0.0.0, port_value:8080}filter_chains:- filters:- name:envoy.http_connection_managerconfig:codec_type:autostat_prefix:ingress_httproute_config:name:local_routevirtual_hosts:- name:backenddomains:- \"*\"routes:- match:prefix:\"/\"route:cluster:targetClusterhttp_filters:- name:envoy.routerclusters:- name:targetClusterconnect_timeout:0.25stype:STRICT_DNSdns_lookup_family:V4_ONLYlb_policy:ROUND_ROBINhosts:[{socket_address:{address: 172.17.0.5, port_value:80}},{socket_address:{address: 172.17.0.6, port_value:80}}]outlier_detection:consecutive_5xx:\"3\"base_ejection_time:\"30s\" 然后执行如下命令启动 Envoy 代理： $ docker run -d --name proxy2 -p 81:8080 \\ -v $(pwd)/manifests/envoy1.yaml:/etc/envoy/envoy.yaml \\ envoyproxy/envoy 启动完成后，在单独的一个命令行终端中，执行下面的命令来循环发送请求观察状态的变化： $ while true; do curl localhost:81; sleep .5; done 然后我们将 172.17.0.5 这个端点变成不健康的状态： $ curl 172.17.0.5/unhealthy 该命令会将该端点的所有请求变成 500 错误： $ curl 172.17.0.5 -i HTTP/1.1 500 Internal Server Error Date: Wed, 15 Apr 2020 06:55:02 GMT Content-Length: 65 Content-Type: text/html; charset=utf-8 \u003ch1\u003eA unhealthy request was processed by host: 55e0950029b8\u003c/h1\u003e 然后我们会在循环的终端中看到会收到3个不健康的请求，然后 Envoy 就会将该上游服务给移除掉： ...... \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 55e0950029b8\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e \u003ch1\u003eA unhealthy request was processed by host: 55e0950029b8\u003c/h1\u003e \u003ch1\u003eA unhealthy request was processed by host: 55e0950029b8\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e \u003ch1\u003eA unhealthy request was processed by host: 55e0950029b8\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e ...... 然后我们也可以再次将 172.17.0.5 标记为健康，执行如下命令即可： $ curl 172.17.0.5/healthy 然后差不多 30s 过后，我们查看 Envoy 又将该端点添加回来参与负载均衡了： ...... \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 55e0950029b8\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 55e0950029b8\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e ...... 到这里我们就完成了在 Envoy 中的健康检查相关的配置。 ","date":"5059-08-529","objectID":"/envoy_1/:21:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"}]