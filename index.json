[{"categories":["calico"],"content":"calico 网络是怎么流通的，抓包如何，一点点带你走进 calico.","date":"212111-08-2151","objectID":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/","tags":["kubernetes","calico"],"title":"calico - 网络流通以及抓包实战","uri":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/"},{"categories":["calico"],"content":"前言 本文主要分析k8s中网络组件calico的 IPIP网络模式。旨在理解IPIP网络模式下产生的calixxxx，tunl0等设备以及跨节点网络通信方式。可能看着有点枯燥，但是请花几分钟时间坚持看完，如果看到后面忘了前面，请反复看两遍，这几分钟时间一定你会花的很值。 ","date":"212111-08-2151","objectID":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/:0:1","tags":["kubernetes","calico"],"title":"calico - 网络流通以及抓包实战","uri":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/"},{"categories":["calico"],"content":"一、calico介绍 Calico是Kubernetes生态系统中另一种流行的网络选择。虽然Flannel被公认为是最简单的选择，但Calico以其性能、灵活性而闻名。Calico的功能更为全面，不仅提供主机和pod之间的网络连接，还涉及网络安全和管理。Calico CNI插件在CNI框架内封装了Calico的功能。 Calico是一个基于BGP的纯三层的网络方案，与OpenStack、Kubernetes、AWS、GCE等云平台都能够良好地集成。Calico在每个计算节点(every node)都利用Linux Kernel实现了一个高效的虚拟路由器vRouter来负责数据转发。每个vRouter都通过BGP协议把在本节点上运行的容器的路由信息向整个Calico网络广播，并自动设置到达其他节点的路由转发规则。Calico保证所有容器之间的数据流量都是通过IP路由的方式完成互联互通的。Calico节点组网时可以直接利用数据中心的网络结构(L2或者L3)，不需要额外的NAT、隧道或者Overlay Network，没有额外的封包解包，能够节约CPU运算，提高网络效率。 此外，Calico基于iptables还提供了丰富的网络策略，实现了Kubernetes的Network Policy策略，提供容器间网络可达性限制的功能。 **calico官网：**https://www.projectcalico.org/ ","date":"212111-08-2151","objectID":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/:0:2","tags":["kubernetes","calico"],"title":"calico - 网络流通以及抓包实战","uri":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/"},{"categories":["calico"],"content":"二、calico架构及核心组件 架构图如下： calico核心组件： Felix (菲利克斯)：运行在每个需要运行workload的节点上的agent进程。主要负责配置路由及 ACLs(访问控制列表) 等信息来确保 endpoint 的连通状态，保证跨主机容器的网络互通; etcd：强一致性、高可用的键值存储，持久存储calico数据的存储管理系统。主要负责网络元数据一致性，确保Calico网络状态的准确性; BGP Client(BIRD)：读取Felix设置的内核路由状态，在数据中心分发状态。 BGP Route Reflector(BIRD)：BGP路由反射器，在较大规模部署时使用。如果仅使用BGP Client形成mesh全网互联就会导致规模限制，因为所有BGP client节点之间两两互联，需要建立N^2个连接，拓扑也会变得复杂。因此使用reflector来负责client之间的连接，防止节点两两相连。 ","date":"212111-08-2151","objectID":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/:0:3","tags":["kubernetes","calico"],"title":"calico - 网络流通以及抓包实战","uri":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/"},{"categories":["calico"],"content":"三、calico工作原理 ​ Calico把每个操作系统的协议栈认为是一个路由器，然后把所有的容器认为是连在这个路由器上的网络终端，在路由器之间跑标准的路由协议——BGP的协议，然后让它们自己去学习这个网络拓扑该如何转发。所以Calico方案其实是一个纯三层的方案，也就是说让每台机器的协议栈的三层去确保两个容器，跨主机容器之间的三层连通性。 ","date":"212111-08-2151","objectID":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/:0:4","tags":["kubernetes","calico"],"title":"calico - 网络流通以及抓包实战","uri":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/"},{"categories":["calico"],"content":"四、calico的两种网络方式 ","date":"212111-08-2151","objectID":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/:0:5","tags":["kubernetes","calico"],"title":"calico - 网络流通以及抓包实战","uri":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/"},{"categories":["calico"],"content":"1)IPIP ​ 把 IP 层封装到 IP 层的一个 tunnel。它的作用其实基本上就相当于一个基于IP层的网桥!一般来说，普通的网桥是基于mac层的，根本不需 IP，而这个 ipip 则是通过两端的路由做一个 tunnel，把两个本来不通的网络通过点对点连接起来。ipip 的源代码在内核 net/ipv4/ipip.c 中可以找到。 ","date":"212111-08-2151","objectID":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/:0:6","tags":["kubernetes","calico"],"title":"calico - 网络流通以及抓包实战","uri":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/"},{"categories":["calico"],"content":"2)BGP 边界网关协议(Border Gateway Protocol, BGP)是互联网上一个核心的去中心化自治路由协议。它通过维护IP路由表或‘前缀’表来实现自治系统(AS)之间的可达性，属于矢量路由协议。BGP不使用传统的内部网关协议(IGP)的指标，而使用基于路径、网络策略或规则集来决定路由。因此，它更适合被称为矢量性协议，而不是路由协议。 ","date":"212111-08-2151","objectID":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/:0:7","tags":["kubernetes","calico"],"title":"calico - 网络流通以及抓包实战","uri":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/"},{"categories":["calico"],"content":"五、IPIP网络模式分析 由于个人环境中使用的是IPIP模式，因此接下来这里分析一下这种模式。 # kubectl get po -o wide -n paas | grep hello demo-hello-perf-d84bffcb8-7fxqj 1/1 Running 0 9d 10.20.105.215 node2.perf \u003cnone\u003e \u003cnone\u003e demo-hello-sit-6d5c9f44bc-ncpql 1/1 Running 0 9d 10.20.42.31 node1.sit \u003cnone\u003e \u003cnone\u003e 进行ping测试 这里在demo-hello-perf这个pod中ping demo-hello-sit这个pod。 root@demo-hello-perf-d84bffcb8-7fxqj:/# ping 10.20.42.31 PING 10.20.42.31 (10.20.42.31) 56(84) bytes of data. 64 bytes from 10.20.42.31: icmp_seq=1 ttl=62 time=5.60 ms 64 bytes from 10.20.42.31: icmp_seq=2 ttl=62 time=1.66 ms 64 bytes from 10.20.42.31: icmp_seq=3 ttl=62 time=1.79 ms ^C --- 10.20.42.31 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 6ms rtt min/avg/max/mdev = 1.662/3.015/5.595/1.825 ms 进入pod demo-hello-perf中查看这个pod中的路由信息 root@demo-hello-perf-d84bffcb8-7fxqj:/# route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 169.254.1.1 0.0.0.0 UG 0 0 0 eth0 169.254.1.1 0.0.0.0 255.255.255.255 UH 0 0 0 eth0 根据路由信息，ping 10.20.42.31，会匹配到第一条。 **第一条路由的意思是：**去往任何网段的数据包都发往网关169.254.1.1，然后从eth0网卡发送出去。 demo-hello-perf所在的node node2.perf 宿主机上路由信息如下： # node2 # route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 172.16.36.1 0.0.0.0 UG 100 0 0 eth0 10.20.42.0 172.16.35.4 255.255.255.192 UG 0 0 0 tunl0 10.20.105.196 0.0.0.0 255.255.255.255 UH 0 0 0 cali4bb1efe70a2 169.254.169.254 172.16.36.2 255.255.255.255 UGH 100 0 0 eth0 172.16.36.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 可以看到一条Destination为 10.20.42.0的路由。 该路由的意思是：去往10.20.42.0/26的网段的数据包都发往网关172.16.35.4(其实 这个 ip 就是 对端 node 的 ip)。因为demo-hello-perf的pod在172.16.36.5上，demo-hello-sit的pod在172.16.35.4上。所以数据包就通过设备tunl0发往到node节点上。 demo-hello-sit所在的node node1.sit 宿主机上路由信息如下： # node1 # route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 172.16.35.1 0.0.0.0 UG 100 0 0 eth0 10.20.15.64 172.16.36.4 255.255.255.192 UG 0 0 0 tunl0 10.20.42.31 0.0.0.0 255.255.255.255 UH 0 0 0 cali04736ec14ce 10.20.105.192 172.16.36.5 255.255.255.192 UG 0 0 0 tunl0 当node节点网卡收到数据包之后，发现发往的目的ip为10.20.42.31，于是匹配到Destination为10.20.42.31的路由。 该路由的意思是：10.20.42.31是本机直连设备，去往设备的数据包发往cali04736ec14ce 为什么这么奇怪会有一个名为cali04736ec14ce的设备呢?这是个啥玩意儿呢? 其实这个设备就是veth pair的一端。在创建demo-hello-sit 时calico会给demo-hello-sit创建一个veth pair设备。一端是demo-hello-sit 的网卡，另一端就是我们看到的cali04736ec14ce 接着验证一下。我们进入demo-hello-sit 的pod，查看到 4 号设备后面的编号是：122964 root@demo-hello-sit--6d5c9f44bc-ncpql:/# ip a 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: tunl0@NONE: \u003cNOARP\u003e mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 4: eth0@if122964: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1380 qdisc noqueue state UP group default link/ether 9a:7d:b2:26:9b:17 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.20.42.31/32 brd 10.20.42.31 scope global eth0 # 这个 ip 就是 pod ip valid_lft forever preferred_lft forever 然后我们登录到demo-hello-sit这个pod所在的宿主机查看 # node1 宿主机 # ip a | grep -A 5 \"cali04736ec14ce\" 122964: cali04736ec14ce@if4: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1380 qdisc noqueue state UP group default link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 16 inet6 fe80::ecee:eeff:feee:eeee/64 scope link valid_lft forever preferred_lft forever 120918: calidd1cafcd275@if4: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1380 qdisc noqueue state UP group default link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 2 发现pod demo-hello-sit中 的另一端设备编号和这里在node上看到的cali04736ec14ce编号122964是一样的 所以，node上的路由，发送cali04736ec14ce网卡设备的数据其实就是发送到了demo-hello-sit的这个pod中去了。到这里ping包就到了目的地。 注意看 demo-hello-sit这个pod所在的宿主机的路由，有一条 Destination为10.20.105.192的路由 # node1 # route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface ... 0.0.0.0 172.16.35.1 0.0.0.0 UG 100 0","date":"212111-08-2151","objectID":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/:0:8","tags":["kubernetes","calico"],"title":"calico - 网络流通以及抓包实战","uri":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/"},{"categories":["calico"],"content":"六、抓包分析 这里我使用了httpbin 服务和 sleep 服务来演示抓包过程 在 node1 sleep这个 pod 中去 curl node2 httpbin 服务，接着在 httpbin 所在node2 上tcpdump抓包 # 先在 nodes2 上 启动抓包 $ tcpdump -i ens160 -nn -w httpbin_ens160.cap tcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes # 然后在 node1 上发起 curl 请求 $ kubectl exec sleep-76c9b748f4-p964r -c sleep -n zhangji -- curl httpbin.httpbin:8000/headers % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 565 100 565 0 0 137k 0 --:--:-- --:--:-- --:--:-- 137k { \"headers\": { \"Accept\": \"*/*\", \"Content-Length\": \"0\", \"Host\": \"httpbin.httpbin:8000\", \"User-Agent\": \"curl/7.76.1-DEV\", \"X-B3-Parentspanid\": \"365864bb263d7b45\", \"X-B3-Sampled\": \"0\", \"X-B3-Spanid\": \"65ee1633796944f9\", \"X-B3-Traceid\": \"a36a2e412ab0d22b365864bb263d7b45\", \"X-Envoy-Attempt-Count\": \"1\", \"X-Forwarded-Client-Cert\": \"By=spiffe://cluster.local/ns/httpbin/sa/httpbin;Hash=cdd5655c5735fa86c8ad8636fdd91027fa288a2c31b6cb18d7feae0d563b009f;Subject=\\\"\\\";URI=spiffe://cluster.local/ns/zhangji/sa/default\" } } 结束抓包后下载httpbin_ens160.cap到本地wireshark进行抓包分析 能看到该数据包一共5层，其中IP(Internet Protocol)所在的网络层有两个，分别是pod之间的网络和主机之间的网络封装。 红色框选的是两个pod所在的宿主机，黄色框选的是两个pod的ip，src表示发起ping操作的pod所在的宿主机ip以及发起curl操作的pod的ip，dst表示被curl的pod所在的宿主机ip及被curl的pod的ip 封包顺序如下所示： 可以看到每个数据报文共有两个IP网络层,内层是Pod容器之间的IP网络报文,外层是宿主机节点的网络报文(2个node节点)。之所以要这样做是因为tunl0是一个隧道端点设备，在数据到达时要加上一层封装，便于发送到对端隧道设备中。 两层封包的具体内容如下： Pod间的通信经由IPIP的三层隧道转发,相比较VxLAN的二层隧道来说，IPIP隧道的开销较小，但其安全性也更差一些。 ","date":"212111-08-2151","objectID":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/:0:9","tags":["kubernetes","calico"],"title":"calico - 网络流通以及抓包实战","uri":"/%E4%B8%80%E6%96%87%E6%98%8E%E7%99%BDcalico%E7%9A%84ipip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/"},{"categories":["docker"],"content":"Dockerfile 手把手教你怎么写.","date":"212111-08-2151","objectID":"/docker_dockerbuilder/","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"[TOC] 一 使用 Dockerfile 定制镜像 镜像的定制实际上就是定制每一层所添加的配置、文件。如果我们可以把每一层修改、安装、构建、操作的命令都写入一个脚本，用这个脚本来构建、定制镜像，那么之前提及的无法重复的问题、镜像构建透明性的问题、体积的问题就都会解决。这个脚本就是 Dockerfile。 Dockerfile 是一个文本文件，其内包含了一条条的 指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。 以定制 nginx 镜像为例，这次我们使用 Dockerfile 来定制。 在一个空白目录中，建立一个文本文件，并命名为 Dockerfile： $ mkdir mynginx $ cd mynginx $ touch Dockerfile 其内容为： FROMnginxRUN echo '\u003ch1\u003eHello, Docker!\u003c/h1\u003e' \u003e /usr/share/nginx/html/index.html 这个 Dockerfile 很简单，一共就两行。涉及到了两条指令，FROM 和 RUN。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:0:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"FROM 指定基础镜像 所谓定制镜像，那一定是以一个镜像为基础，在其上进行定制。基础镜像是必须指定的。而 FROM 就是指定 基础镜像，因此一个 Dockerfile 中 FROM 是必备的指令，并且必须是第一条指令。 在 Docker Hub 上有非常多的高质量的官方镜像，有可以直接拿来使用的服务类的镜像，如 nginx、redis、mongo、mysql、httpd、php、tomcat 等；也有一些方便开发、构建、运行各种语言应用的镜像，如 node、openjdk、python、ruby、golang 等。可以在其中寻找一个最符合我们最终目标的镜像为基础镜像进行定制。 如果没有找到对应服务的镜像，官方镜像中还提供了一些更为基础的操作系统镜像，如 ubuntu、debian、centos、fedora、alpine 等，这些操作系统的软件库为我们提供了更广阔的扩展空间。 除了选择现有镜像为基础镜像外，Docker 还存在一个特殊的镜像，名为 scratch。这个镜像是虚拟的概念，并不实际存在，它表示一个空白的镜像。 FROMscratch... 如果你以 scratch 为基础镜像的话，意味着你不以任何镜像为基础，接下来所写的指令将作为镜像第一层开始存在。 不以任何系统为基础，直接将可执行文件复制进镜像的做法并不罕见. 对于 Linux 下静态编译的程序来说，并不需要有操作系统提供运行时支持，所需的一切库都已经在可执行文件里了，因此直接 FROM scratch 会让镜像体积更加小巧。 使用 Go 语言 开发的应用很多会使用这种方式来制作镜像，这也是为什么有人认为 Go 是特别适合容器微服务架构的语言的原因之一。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:1:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"RUN 执行命令 RUN 指令是用来执行命令行命令的。由于命令行的强大能力，RUN 指令在定制镜像时是最常用的指令之一。其格式有两种： shell 格式：RUN \u003c命令\u003e，就像直接在命令行中输入的命令一样。刚才写的 Dockerfile 中的 RUN 指令就是这种格式。 RUN echo '\u003ch1\u003eHello, Docker!\u003c/h1\u003e' \u003e /usr/share/nginx/html/index.html exec 格式：RUN [\"可执行文件\", \"参数1\", \"参数2\"]，这更像是函数调用中的格式。 既然 RUN 就像 Shell 脚本一样可以执行命令，那么我们是否就可以像 Shell 脚本一样把每个命令对应一个 RUN 呢？比如这样： FROMdebian:stretchRUN apt-get updateRUN apt-get install -y gcc libc6-dev make wgetRUN wget -O redis.tar.gz \"http://download.redis.io/releases/redis-5.0.3.tar.gz\"RUN mkdir -p /usr/src/redisRUN tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1RUN make -C /usr/src/redisRUN make -C /usr/src/redis install 之前说过，Dockerfile 中每一个指令都会建立一层，RUN 也不例外。每一个 RUN 的行为： 新建立一层，在其上执行这些命令 执行结束后，commit 这一层的修改，构成新的镜像。 而上面的这种写法，创建了 7 层镜像。这是完全没有意义的，而且很多运行时不需要的东西，都被装进了镜像里，比如编译环境、更新的软件包等等。结果就是产生非常臃肿、非常多层的镜像，不仅仅增加了构建部署的时间，也很容易出错。 这是很多初学 Docker 的人常犯的一个错误。 Union FS 是有最大层数限制的，比如 AUFS，曾经是最大不得超过 42 层，现在是不得超过 127 层。 上面的 Dockerfile 正确的写法应该是这样： FROMdebian:stretchRUN set -x; buildDeps='gcc libc6-dev make wget' \\ \u0026\u0026 apt-get update \\ \u0026\u0026 apt-get install -y $buildDeps \\ \u0026\u0026 wget -O redis.tar.gz \"http://download.redis.io/releases/redis-5.0.3.tar.gz\" \\ \u0026\u0026 mkdir -p /usr/src/redis \\ \u0026\u0026 tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \\ \u0026\u0026 make -C /usr/src/redis \\ \u0026\u0026 make -C /usr/src/redis install \\ \u0026\u0026 rm -rf /var/lib/apt/lists/* \\ \u0026\u0026 rm redis.tar.gz \\ \u0026\u0026 rm -r /usr/src/redis \\ \u0026\u0026 apt-get purge -y --auto-remove $buildDeps 首先，之前所有的命令只有一个目的，就是编译、安装 redis 可执行文件。因此没有必要建立很多层，这只是一层的事情。因此，这里没有使用很多个 RUN 一一对应不同的命令，而是仅仅使用一个 RUN 指令，并使用 \u0026\u0026 将各个所需命令串联起来。将之前的 7 层，简化为了 1 层。在撰写 Dockerfile 的时候，要经常提醒自己，这并不是在写 Shell 脚本，而是在定义每一层该如何构建。 并且，这里为了格式化还进行了换行。Dockerfile 支持 Shell 类的行尾添加 \\ 的命令换行方式，以及行首 # 进行注释的格式。良好的格式，比如换行、缩进、注释等，会让维护、排障更为容易，这是一个比较好的习惯。 此外，还可以看到这一组命令的最后添加了清理工作的命令，删除了为了编译构建所需要的软件，清理了所有下载、展开的文件，并且还清理了 apt 缓存文件。这是很重要的一步，我们之前说过，镜像是多层存储，每一层的东西并不会在下一层被删除，会一直跟随着镜像。因此镜像构建时，一定要确保每一层只添加真正需要添加的东西，任何无关的东西都应该清理掉。 很多人初学 Docker 制作出了很臃肿的镜像的原因之一，就是忘记了每一层构建的最后一定要清理掉无关文件。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:2:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"构建镜像 好了，让我们再回到之前定制的 nginx 镜像的 Dockerfile 来。现在我们明白了这个 Dockerfile 的内容，那么让我们来构建这个镜像吧。 在 Dockerfile 文件所在目录执行： $ docker build -t nginx:v3 . Sending build context to Docker daemon 2.048 kB Step 1 : FROM nginx ---\u003e e43d811ce2f4 Step 2 : RUN echo '\u003ch1\u003eHello, Docker!\u003c/h1\u003e' \u003e /usr/share/nginx/html/index.html ---\u003e Running in 9cdc27646c7b ---\u003e 44aa4490ce2c Removing intermediate container 9cdc27646c7b Successfully built 44aa4490ce2c 从命令的输出结果中，我们可以清晰的看到镜像的构建过程。在 Step 2 中，如同我们之前所说的那样，RUN 指令启动了一个容器 9cdc27646c7b，执行了所要求的命令，并最后提交了这一层 44aa4490ce2c，随后删除了所用到的这个容器 9cdc27646c7b。 这里我们使用了 docker build 命令进行镜像构建。其格式为： docker build [选项] \u003c上下文路径/URL/-\u003e 在这里我们指定了最终镜像的名称 -t nginx:v3，构建成功后，会生成一个新的nginx镜像。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:3:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"镜像构建上下文（Context） 如果注意，会看到 docker build 命令最后有一个 .。. 表示当前目录，而 Dockerfile 就在当前目录，因此不少初学者以为这个路径是在指定 Dockerfile 所在路径，这么理解其实是不准确的。如果对应上面的命令格式，你可能会发现，这是在指定 上下文路径。那么什么是上下文呢？ 首先我们要理解 docker build 的工作原理。Docker 在运行时分为 Docker 引擎（也就是服务端守护进程）和客户端工具。Docker 的引擎提供了一组 REST API，被称为 Docker Remote API，而如 docker 命令这样的客户端工具，则是通过这组 API 与 Docker 引擎交互，从而完成各种功能。因此，虽然表面上我们好像是在本机执行各种 docker 功能，但实际上，一切都是使用的远程调用形式在服务端（Docker 引擎）完成。也因为这种 C/S 设计，让我们操作远程服务器的 Docker 引擎变得轻而易举。 当我们进行镜像构建的时候，并非所有定制都会通过 RUN 指令完成，经常会需要将一些本地文件复制进镜像，比如通过 COPY 指令、ADD 指令等。而 docker build 命令构建镜像，其实并非在本地构建，而是在服务端，也就是 Docker 引擎中构建的。那么在这种客户端/服务端的架构中，如何才能让服务端获得本地文件呢？ 这就引入了上下文的概念。当构建的时候，用户会指定构建镜像上下文的路径，docker build 命令得知这个路径后，会将路径下的所有内容打包，然后上传给 Docker 引擎。这样 Docker 引擎收到这个上下文包后，展开就会获得构建镜像所需的一切文件。 如果在 Dockerfile 中这么写： COPY ./package.json /app/ 这并不是要复制执行 docker build 命令所在的目录下的 package.json，也不是复制 Dockerfile 所在目录下的 package.json，而是复制 上下文（context） 目录下的 package.json。 因此，COPY 这类指令中的源文件的路径都是相对路径。这也是初学者经常会问的为什么 COPY ../package.json /app 或者 COPY /opt/xxxx /app 无法工作的原因，因为这些路径已经超出了上下文的范围，Docker 引擎无法获得这些位置的文件。如果真的需要那些文件，应该将它们复制到上下文目录中去。 现在就可以理解刚才的命令 docker build -t nginx:v3 . 中的这个 .，实际上是在指定上下文的目录，docker build 命令会将该目录下的内容打包交给 Docker 引擎以帮助构建镜像。 如果观察 docker build 输出，我们其实已经看到了这个发送上下文的过程： $ docker build -t nginx:v3 . Sending build context to Docker daemon 2.048 kB ... 理解构建上下文对于镜像构建是很重要的，避免犯一些不应该的错误。比如有些初学者在发现 COPY /opt/xxxx /app 不工作后，于是干脆将 Dockerfile 放到了硬盘根目录去构建，结果发现 docker build 执行后，在发送一个几十 GB 的东西，极为缓慢而且很容易构建失败。那是因为这种做法是在让 docker build 打包整个硬盘，这显然是使用错误。 一般来说，应该会将 Dockerfile 置于一个空目录下，或者项目根目录下。如果该目录下没有所需文件，那么应该把所需文件复制一份过来。如果目录下有些东西确实不希望构建时传给 Docker 引擎，那么可以用 .gitignore 一样的语法写一个 .dockerignore，该文件是用于剔除不需要作为上下文传递给 Docker 引擎的。 那么为什么会有人误以为 . 是指定 Dockerfile 所在目录呢？这是因为在默认情况下，如果不额外指定 Dockerfile 的话，会将上下文目录下的名为 Dockerfile 的文件作为 Dockerfile。 这只是默认行为，实际上 Dockerfile 的文件名并不要求必须为 Dockerfile，而且并不要求必须位于上下文目录中，比如可以用 -f ../Dockerfile.php 参数指定某个文件作为 Dockerfile。 当然，一般大家习惯性的会使用默认的文件名 Dockerfile，以及会将其置于镜像构建上下文目录中。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:4:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"其它 docker build 的用法 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:5:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"直接用 Git repo 进行构建 或许你已经注意到了，docker build 还支持从 URL 构建，比如可以直接从 Git repo 中构建： # $env:DOCKER_BUILDKIT=0 # export DOCKER_BUILDKIT=0 $ docker build -t hello-world https://github.com/docker-library/hello-world.git#master:amd64/hello-world Step 1/3 : FROM scratch ---\u003e Step 2/3 : COPY hello / ---\u003e ac779757d46e Step 3/3 : CMD [\"/hello\"] ---\u003e Running in d2a513a760ed Removing intermediate container d2a513a760ed ---\u003e 038ad4142d2b Successfully built 038ad4142d2b 这行命令指定了构建所需的 Git repo，并且指定分支为 master，构建目录为 /amd64/hello-world/，然后 Docker 就会自己去 git clone 这个项目、切换到指定分支、并进入到指定目录后开始构建。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:5:1","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"用给定的 tar 压缩包构建 $ docker build http://server/context.tar.gz 如果所给出的 URL 不是个 Git repo，而是个 tar 压缩包，那么 Docker 引擎会下载这个包，并自动解压缩，以其作为上下文，开始构建。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:5:2","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"从标准输入中读取 Dockerfile 进行构建 docker build - \u003c Dockerfile 或 cat Dockerfile | docker build - 如果标准输入传入的是文本文件，则将其视为 Dockerfile，并开始构建。这种形式由于直接从标准输入中读取 Dockerfile 的内容，它没有上下文，因此不可以像其他方法那样可以将本地文件 COPY 进镜像之类的事情。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:5:3","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"从标准输入中读取上下文压缩包进行构建 $ docker build - \u003c context.tar.gz 如果发现标准输入的文件格式是 gzip、bzip2 以及 xz 的话，将会使其为上下文压缩包，直接将其展开，将里面视为上下文，并开始构建。 二 指令详解 我们已经介绍了 FROM，RUN，还提及了 COPY, ADD，其实 Dockerfile 功能很强大，它提供了十多个指令。下面我们继续讲解其他的指令。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:5:4","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"COPY 复制文件 格式： COPY [--chown=\u003cuser\u003e:\u003cgroup\u003e] \u003c源路径\u003e... \u003c目标路径\u003e COPY [--chown=\u003cuser\u003e:\u003cgroup\u003e] [\"\u003c源路径1\u003e\",... \"\u003c目标路径\u003e\"] 和 RUN 指令一样，也有两种格式，一种类似于命令行，一种类似于函数调用。 COPY 指令将从构建上下文目录中 \u003c源路径\u003e 的文件/目录复制到新的一层的镜像内的 \u003c目标路径\u003e 位置。比如： COPY package.json /usr/src/app/ \u003c源路径\u003e 可以是多个，甚至可以是通配符，其通配符规则要满足 Go 的 filepath.Match 规则，如： COPY hom* /mydir/COPY hom?.txt /mydir/ \u003c目标路径\u003e 可以是容器内的绝对路径，也可以是相对于工作目录的相对路径（工作目录可以用 WORKDIR 指令来指定）。目标路径不需要事先创建，如果目录不存在会在复制文件前先行创建缺失目录。 此外，还需要注意一点，使用 COPY 指令，源文件的各种元数据都会保留。比如读、写、执行权限、文件变更时间等。这个特性对于镜像定制很有用。特别是构建相关文件都在使用 Git 进行管理的时候。 在使用该指令的时候还可以加上 --chown=\u003cuser\u003e:\u003cgroup\u003e 选项来改变文件的所属用户及所属组。 COPY --chown=55:mygroup files* /mydir/COPY --chown=bin files* /mydir/COPY --chown=1 files* /mydir/COPY --chown=10:11 files* /mydir/ 注意事项：如果源路径为文件夹，复制的时候不是直接复制该文件夹，而是将文件夹中的内容复制到目标路径。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:6:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"ADD 更高级的复制文件 ADD 指令和 COPY 的格式和性质基本一致。但是在 COPY 基础上增加了一些功能。 比如 \u003c源路径\u003e 可以是一个 URL，这种情况下，Docker 引擎会试图去下载这个链接的文件放到 \u003c目标路径\u003e 去。下载后的文件权限自动设置为 600，如果这并不是想要的权限，那么还需要增加额外的一层 RUN 进行权限调整，另外，如果下载的是个压缩包，需要解压缩，也一样还需要额外的一层 RUN 指令进行解压缩。所以不如直接使用 RUN 指令，然后使用 wget 或者 curl 工具下载，处理权限、解压缩、然后清理无用文件更合理。因此，这个功能其实并不实用，而且不推荐使用。 如果 \u003c源路径\u003e 为一个 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，ADD 指令将会自动解压缩这个压缩文件到 \u003c目标路径\u003e 去。 在某些情况下，这个自动解压缩的功能非常有用，比如官方镜像 ubuntu 中： FROMscratchADD ubuntu-xenial-core-cloudimg-amd64-root.tar.gz /... 但在某些情况下，如果我们真的是希望复制个压缩文件进去，而不解压缩，这时就不可以使用 ADD 命令了。 在 Docker 官方的 Dockerfile 最佳实践文档 中要求，尽可能的使用 COPY，因为 COPY 的语义很明确，就是复制文件而已，而 ADD 则包含了更复杂的功能，其行为也不一定很清晰。最适合使用 ADD 的场合，就是所提及的需要自动解压缩的场合。 另外需要注意的是，ADD 指令会令镜像构建缓存失效，从而可能会令镜像构建变得比较缓慢。 因此在 COPY 和 ADD 指令中选择的时候，可以遵循这样的原则： 所有的文件复制均使用 COPY 指令，仅在需要自动解压缩的场合使用 ADD。 在使用该指令的时候还可以加上 --chown=\u003cuser\u003e:\u003cgroup\u003e 选项来改变文件的所属用户及所属组。 ADD --chown=55:mygroup files* /mydir/ADD --chown=bin files* /mydir/ADD --chown=1 files* /mydir/ADD --chown=10:11 files* /mydir/ ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:7:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"CMD 容器启动命令 CMD 指令的格式和 RUN 相似，也是两种格式： shell 格式：CMD \u003c命令\u003e exec 格式：CMD [\"可执行文件\", \"参数1\", \"参数2\"...] 参数列表格式：CMD [\"参数1\", \"参数2\"...]。在指定了 ENTRYPOINT 指令后，用 CMD 指定具体的参数。 Docker 不是虚拟机，容器就是进程。既然是进程，那么在启动容器的时候，需要指定所运行的程序及参数。 功能：CMD 指令就是用于指定默认的容器主进程的启动命令的。 在运行时可以指定新的命令来替代镜像设置中的这个默认命令，比如，ubuntu 镜像默认的 CMD 是 /bin/bash，如果我们直接 docker run -it ubuntu 的话，会直接进入 bash。我们也可以在运行时指定运行别的命令，如 docker run -it ubuntu cat /etc/os-release。这就是用 cat /etc/os-release 命令替换了默认的 /bin/bash 命令了，输出了系统版本信息。 在指令格式上，一般推荐使用 exec 格式，这类格式在解析时会被解析为 JSON 数组，因此一定要使用双引号 \"，而不要使用单引号。 如果使用 shell 格式的话，实际的命令会被包装为 sh -c 的参数的形式进行执行。比如： CMD echo $HOME 在实际执行中，会将其变更为： CMD [ \"sh\", \"-c\", \"echo $HOME\" ] 这就是为什么我们可以使用环境变量的原因，因为这些环境变量会被 shell 进行解析处理。 提到 CMD 就不得不提容器中应用在前台执行和后台执行的问题。这是初学者常出现的一个混淆。 Docker 不是虚拟机，容器中的应用都应该以前台执行，而不是像虚拟机、物理机里面那样，用 systemd 去启动后台服务，容器内没有后台服务的概念。 一些初学者将 CMD 写为： CMD service nginx start 然后发现容器执行后就立即退出了。甚至在容器内去使用 systemctl 命令结果却发现根本执行不了。这就是因为没有搞明白前台、后台的概念，没有区分容器和虚拟机的差异，依旧在以传统虚拟机的角度去理解容器。 对于容器而言，其启动程序就是容器应用进程，容器就是为了主进程而存在的，主进程退出，容器就失去了存在的意义，从而退出，其它辅助进程不是它需要关心的东西。 而使用 service nginx start 命令，则是希望 upstart 来以后台守护进程形式启动 nginx 服务。而刚才说了 CMD service nginx start 会被理解为 CMD [ \"sh\", \"-c\", \"service nginx start\"]，因此主进程实际上是 sh。那么当 service nginx start 命令结束后，sh 也就结束了，sh 作为主进程退出了，自然就会令容器退出。 正确的做法是直接执行 nginx 可执行文件，并且要求以前台形式运行。比如： CMD [\"nginx\", \"-g\", \"daemon off;\"] ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:8:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"ENTRYPOINT 入口点 ENTRYPOINT 的格式和 RUN 指令格式一样，分为 exec 格式和 shell 格式。 ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数。ENTRYPOINT 在运行时也可以替代，不过比 CMD 要略显繁琐，需要通过 docker run 的参数 --entrypoint 来指定。 当指定了 ENTRYPOINT 后，CMD 的含义就发生了改变，不再是直接的运行其命令，而是将 CMD 的内容作为参数传给 ENTRYPOINT 指令，换句话说实际执行时，将变为： \u003cENTRYPOINT\u003e \"\u003cCMD\u003e\" 那么有了 CMD 后，为什么还要有 ENTRYPOINT 呢？这种 \u003cENTRYPOINT\u003e \"\u003cCMD\u003e\" 有什么好处么？让我们来看几个场景。 场景一：让镜像变成像命令一样使用 假设我们需要一个得知自己当前公网 IP 的镜像，那么可以先用 CMD 来实现： FROMubuntu:18.04RUN apt-get update \\ \u0026\u0026 apt-get install -y curl \\ \u0026\u0026 rm -rf /var/lib/apt/lists/*CMD [ \"curl\", \"-s\", \"http://myip.ipip.net\" ] 假如我们使用 docker build -t myip . 来构建镜像的话，如果我们需要查询当前公网 IP，只需要执行： $ docker run myip 当前 IP：61.148.226.66 来自：北京市 联通 嗯，这么看起来好像可以直接把镜像当做命令使用了，不过命令总有参数，如果我们希望加参数呢？比如从上面的 CMD 中可以看到实质的命令是 curl，那么如果我们希望显示 HTTP 头信息，就需要加上 -i 参数。那么我们可以直接加 -i 参数给 docker run myip 么？ $ docker run myip -i docker: Error response from daemon: invalid header field value \"oci runtime error: container_linux.go:247: starting container process caused \\\"exec: \\\\\\\"-i\\\\\\\": executable file not found in $PATH\\\"\\n\". 我们可以看到可执行文件找不到的报错，executable file not found。之前我们说过，跟在镜像名后面的是 command，运行时会替换 CMD 的默认值。因此这里的 -i 替换了原来的 CMD，而不是添加在原来的 curl -s http://myip.ipip.net 后面。而 -i 根本不是命令，所以自然找不到。 那么如果我们希望加入 -i 这参数，我们就必须重新完整的输入这个命令： $ docker run myip curl -s http://myip.ipip.net -i 这显然不是很好的解决方案，而使用 ENTRYPOINT 就可以解决这个问题。现在我们重新用 ENTRYPOINT 来实现这个镜像： FROMubuntu:18.04RUN apt-get update \\ \u0026\u0026 apt-get install -y curl \\ \u0026\u0026 rm -rf /var/lib/apt/lists/*ENTRYPOINT [ \"curl\", \"-s\", \"http://myip.ipip.net\" ] 这次我们再来尝试直接使用 docker run myip -i： $ docker run myip 当前 IP：61.148.226.66 来自：北京市 联通 $ docker run myip -i HTTP/1.1 200 OK Server: nginx/1.8.0 Date: Tue, 22 Nov 2016 05:12:40 GMT Content-Type: text/html; charset=UTF-8 Vary: Accept-Encoding X-Powered-By: PHP/5.6.24-1~dotdeb+7.1 X-Cache: MISS from cache-2 X-Cache-Lookup: MISS from cache-2:80 X-Cache: MISS from proxy-2_6 Transfer-Encoding: chunked Via: 1.1 cache-2:80, 1.1 proxy-2_6:8006 Connection: keep-alive 当前 IP：61.148.226.66 来自：北京市 联通 可以看到，这次成功了。这是因为当存在 ENTRYPOINT 后，CMD 的内容将会作为参数传给 ENTRYPOINT，而这里 -i 就是新的 CMD，因此会作为参数传给 curl，从而达到了我们预期的效果。 场景二：应用运行前的准备工作 启动容器就是启动主进程，但有些时候，启动主进程前，需要一些准备工作。 比如 mysql 类的数据库，可能需要一些数据库配置、初始化的工作，这些工作要在最终的 mysql 服务器运行之前解决。 此外，可能希望避免使用 root 用户去启动服务，从而提高安全性，而在启动服务前还需要以 root 身份执行一些必要的准备工作，最后切换到服务用户身份启动服务。或者除了服务外，其它命令依旧可以使用 root 身份执行，方便调试等。 这些准备工作是和容器 CMD 无关的，无论 CMD 为什么，都需要事先进行一个预处理的工作。这种情况下，可以写一个脚本，然后放入 ENTRYPOINT 中去执行，而这个脚本会将接到的参数（也就是 \u003cCMD\u003e）作为命令，在脚本最后执行。比如官方镜像 redis 中就是这么做的： FROMalpine:3.4...RUN addgroup -S redis \u0026\u0026 adduser -S -G redis redis...ENTRYPOINT [\"docker-entrypoint.sh\"]EXPOSE6379CMD [ \"redis-server\" ] 可以看到其中为了 redis 服务创建了 redis 用户，并在最后指定了 ENTRYPOINT 为 docker-entrypoint.sh 脚本。 #!/bin/sh ... # allow the container to be started with `--user` if [ \"$1\" = 'redis-server' -a \"$(id -u)\" = '0' ]; then find . \\! -user redis -exec chown redis '{}' + exec gosu redis \"$0\" \"$@\" fi exec \"$@\" 该脚本的内容就是根据 CMD 的内容来判断，如果是 redis-server 的话，则切换到 redis 用户身份启动服务器，否则依旧使用 root 身份执行。比如： $ docker run -it redis id uid=0(root) gid=0(root) groups=0(root) ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:9:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"ENV 设置环境变量 格式有两种： ENV \u003ckey\u003e \u003cvalue\u003e ENV \u003ckey1\u003e=\u003cvalue1\u003e \u003ckey2\u003e=\u003cvalue2\u003e... 这个指令很简单，就是设置环境变量而已，无论是后面的其它指令，如 RUN，还是运行时的应用，都可以直接使用这里定义的环境变量。 ENV VERSION=1.0 DEBUG=on \\ NAME=\"Happy Feet\" 这个例子中演示了如何换行，以及对含有空格的值用双引号括起来的办法，这和 Shell 下的行为是一致的。 定义了环境变量，那么在后续的指令中，就可以使用这个环境变量。比如在官方 node 镜像 Dockerfile 中，就有类似这样的代码： ENV NODE_VERSION 7.2.0RUN curl -SLO \"https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION-linux-x64.tar.xz\" \\ \u0026\u0026 curl -SLO \"https://nodejs.org/dist/v$NODE_VERSION/SHASUMS256.txt.asc\" \\ \u0026\u0026 gpg --batch --decrypt --output SHASUMS256.txt SHASUMS256.txt.asc \\ \u0026\u0026 grep \" node-v$NODE_VERSION-linux-x64.tar.xz\\$\" SHASUMS256.txt | sha256sum -c - \\ \u0026\u0026 tar -xJf \"node-v$NODE_VERSION-linux-x64.tar.xz\" -C /usr/local --strip-components=1 \\ \u0026\u0026 rm \"node-v$NODE_VERSION-linux-x64.tar.xz\" SHASUMS256.txt.asc SHASUMS256.txt \\ \u0026\u0026 ln -s /usr/local/bin/node /usr/local/bin/nodejs 在这里先定义了环境变量 NODE_VERSION，其后的 RUN 这层里，多次使用 $NODE_VERSION 来进行操作定制。可以看到，将来升级镜像构建版本的时候，只需要更新 7.2.0 即可，Dockerfile 构建维护变得更轻松了。 下列指令可以支持环境变量展开： ADD、COPY、ENV、EXPOSE、FROM、LABEL、USER、WORKDIR、VOLUME、STOPSIGNAL、ONBUILD、RUN。 可以从这个指令列表里感觉到，环境变量可以使用的地方很多，很强大。通过环境变量，我们可以让一份 Dockerfile 制作更多的镜像，只需使用不同的环境变量即可。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:10:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"ARG 构建参数 格式：ARG \u003c参数名\u003e[=\u003c默认值\u003e] 构建参数和 ENV 的效果一样，都是设置环境变量。所不同的是，ARG 所设置的构建环境的环境变量，在将来容器运行时是不会存在这些环境变量的。但是不要因此就使用 ARG 保存密码之类的信息，因为 docker history 还是可以看到所有值的。 Dockerfile 中的 ARG 指令是定义参数名称，以及定义其默认值。该默认值可以在构建命令 docker build 中用 --build-arg \u003c参数名\u003e=\u003c值\u003e 来覆盖。 灵活的使用 ARG 指令，能够在不修改 Dockerfile 的情况下，构建出不同的镜像。 ARG 指令有生效范围，如果在 FROM 指令之前指定，那么只能用于 FROM 指令中。 ARG DOCKER_USERNAME=library FROM${DOCKER_USERNAME}/alpineRUN set -x ; echo ${DOCKER_USERNAME} 使用上述 Dockerfile 会发现无法输出 ${DOCKER_USERNAME} 变量的值，要想正常输出，你必须在 FROM 之后再次指定 ARG # 只在 FROM 中生效ARG DOCKER_USERNAME=library FROM${DOCKER_USERNAME}/alpine# 要想在 FROM 之后使用，必须再次指定ARG DOCKER_USERNAME=library RUN set -x ; echo ${DOCKER_USERNAME} 对于多阶段构建，尤其要注意这个问题 # 这个变量在每个 FROM 中都生效ARG DOCKER_USERNAME=library FROM${DOCKER_USERNAME}/alpineRUN set -x ; echo 1FROM${DOCKER_USERNAME}/alpineRUN set -x ; echo 2 对于上述 Dockerfile 两个 FROM 指令都可以使用 ${DOCKER_USERNAME}，对于在各个阶段中使用的变量都必须在每个阶段分别指定： ARG DOCKER_USERNAME=library FROM${DOCKER_USERNAME}/alpine# 在FROM 之后使用变量，必须在每个阶段分别指定ARG DOCKER_USERNAME=library RUN set -x ; echo ${DOCKER_USERNAME}FROM${DOCKER_USERNAME}/alpine# 在FROM 之后使用变量，必须在每个阶段分别指定ARG DOCKER_USERNAME=library RUN set -x ; echo ${DOCKER_USERNAME} ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:11:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"VOLUME 定义匿名卷 格式为： VOLUME [\"\u003c路径1\u003e\", \"\u003c路径2\u003e\"...] VOLUME \u003c路径\u003e 之前我们说过，容器运行时应该尽量保持容器存储层不发生写操作，对于数据库类需要保存动态数据的应用，其数据库文件应该保存于卷(volume)中，后面的章节我们会进一步介绍 Docker 卷的概念。为了防止运行时用户忘记将动态文件所保存目录挂载为卷，在 Dockerfile 中，我们可以事先指定某些目录挂载为匿名卷，这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据。 VOLUME/data 这里的 /data 目录就会在容器运行时自动挂载为匿名卷，任何向 /data 中写入的信息都不会记录进容器存储层，从而保证了容器存储层的无状态化。当然，运行容器时可以覆盖这个挂载设置。比如： $ docker run -d -v mydata:/data xxxx 在这行命令中，就使用了 mydata 这个命名卷挂载到了 /data 这个位置，替代了 Dockerfile 中定义的匿名卷的挂载配置。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:12:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"EXPOSE 暴露端口 格式为 EXPOSE \u003c端口1\u003e [\u003c端口2\u003e...]。 EXPOSE 指令是声明容器运行时提供服务的端口，这只是一个声明，在容器运行时并不会因为这个声明应用就会开启这个端口的服务。在 Dockerfile 中写入这样的声明有两个好处： 一个是帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射； 另一个用处则是在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口。 要将 EXPOSE 和在运行时使用 -p \u003c宿主端口\u003e:\u003c容器端口\u003e 区分开来。-p，是映射宿主端口和容器端口，换句话说，就是将容器的对应端口服务公开给外界访问，而 EXPOSE 仅仅是声明容器打算使用什么端口而已，并不会自动在宿主进行端口映射。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:13:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"WORKDIR 指定工作目录 格式为 WORKDIR \u003c工作目录路径\u003e。 使用 WORKDIR 指令可以来指定工作目录（或者称为当前目录），以后各层的当前目录就被改为指定的目录，如该目录不存在，WORKDIR 会帮你建立目录。 之前提到一些初学者常犯的错误是把 Dockerfile 等同于 Shell 脚本来书写，这种错误的理解还可能会导致出现下面这样的错误： RUN cd /appRUN echo \"hello\" \u003e world.txt 如果将这个 Dockerfile 进行构建镜像运行后，会发现找不到 /app/world.txt 文件，或者其内容不是 hello。原因其实很简单： 在 Shell 中，连续两行是同一个进程执行环境，因此前一个命令修改的内存状态，会直接影响后一个命令； 而在 Dockerfile 中，这两行 RUN 命令的执行环境根本不同，是两个完全不同的容器。这就是对 Dockerfile 构建分层存储的概念不了解所导致的错误。 之前说过每一个 RUN 都是启动一个容器、执行命令、然后提交存储层文件变更。第一层 RUN cd /app 的执行仅仅是当前进程的工作目录变更，一个内存上的变化而已，其结果不会造成任何文件变更。而到第二层的时候，启动的是一个全新的容器，跟第一层的容器更完全没关系，自然不可能继承前一层构建过程中的内存变化。 因此如果需要改变以后各层的工作目录的位置，那么应该使用 WORKDIR 指令。 WORKDIR/appRUN echo \"hello\" \u003e world.txt 如果你的 WORKDIR 指令使用的相对路径，那么所切换的路径与之前的 WORKDIR 有关： WORKDIR/aWORKDIRbWORKDIRcRUN pwd RUN pwd 的工作目录为 /a/b/c。 ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:14:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"USER 指定当前用户 格式：USER \u003c用户名\u003e[:\u003c用户组\u003e] USER 指令和 WORKDIR 相似，都是改变环境状态并影响以后的层。WORKDIR 是改变工作目录，USER 则是改变之后层的执行 RUN, CMD 以及 ENTRYPOINT 这类命令的身份。 注意，USER 只是帮助你切换到指定用户而已，这个用户必须是事先建立好的，否则无法切换。 RUN groupadd -r redis \u0026\u0026 useradd -r -g redis redisUSERredisRUN [ \"redis-server\" ] 如果以 root 执行的脚本，在执行期间希望改变身份，比如希望以某个已经建立好的用户来运行某个服务进程，不要使用 su 或者 sudo，这些都需要比较麻烦的配置，而且在 TTY 缺失的环境下经常出错。建议使用 gosu。 # 建立 redis 用户，并使用 gosu 换另一个用户执行命令RUN groupadd -r redis \u0026\u0026 useradd -r -g redis redis# 下载 gosuRUN wget -O /usr/local/bin/gosu \"https://github.com/tianon/gosu/releases/download/1.12/gosu-amd64\" \\ \u0026\u0026 chmod +x /usr/local/bin/gosu \\ \u0026\u0026 gosu nobody true# 设置 CMD，并以另外的用户执行CMD [ \"exec\", \"gosu\", \"redis\", \"redis-server\" ] ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:15:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"HEALTHCHECK 健康检查 格式： HEALTHCHECK [选项] CMD \u003c命令\u003e：设置检查容器健康状况的命令 HEALTHCHECK NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉其健康检查指令 HEALTHCHECK 指令是告诉 Docker 应该如何进行判断容器的状态是否正常，这是 Docker 1.12 引入的新指令。 在没有 HEALTHCHECK 指令前，Docker 引擎只可以通过容器内主进程是否退出来判断容器是否状态异常。很多情况下这没问题，但是如果程序进入死锁状态，或者死循环状态，应用进程并不退出，但是该容器已经无法提供服务了。在 1.12 以前，Docker 不会检测到容器的这种状态，从而不会重新调度，导致可能会有部分容器已经无法提供服务了却还在接受用户请求。 而自 1.12 之后，Docker 提供了 HEALTHCHECK 指令，通过该指令指定一行命令，用这行命令来判断容器主进程的服务状态是否还正常，从而比较真实的反应容器实际状态。 当在一个镜像指定了 HEALTHCHECK 指令后，用其启动容器，初始状态会为 starting，在 HEALTHCHECK 指令检查成功后变为 healthy，如果连续一定次数失败，则会变为 unhealthy。 HEALTHCHECK 支持下列选项： --interval=\u003c间隔\u003e：两次健康检查的间隔，默认为 30 秒； --timeout=\u003c时长\u003e：健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认 30 秒； --retries=\u003c次数\u003e：当连续失败指定次数后，则将容器状态视为 unhealthy，默认 3 次。 和 CMD, ENTRYPOINT 一样，HEALTHCHECK 只可以出现一次，如果写了多个，只有最后一个生效。 在 HEALTHCHECK [选项] CMD 后面的命令，格式和 ENTRYPOINT 一样，分为 shell 格式，和 exec 格式。命令的返回值决定了该次健康检查的成功与否：0：成功；1：失败；2：保留，不要使用这个值。 假设我们有个镜像是个最简单的 Web 服务，我们希望增加健康检查来判断其 Web 服务是否在正常工作，我们可以用 curl 来帮助判断，其 Dockerfile 的 HEALTHCHECK 可以这么写： FROMnginxRUN apt-get update \u0026\u0026 apt-get install -y curl \u0026\u0026 rm -rf /var/lib/apt/lists/*HEALTHCHECK --interval=5s --timeout=3s \\ CMD curl -fs http://localhost/ || exit 1 这里我们设置了每 5 秒检查一次（这里为了试验所以间隔非常短，实际应该相对较长），如果健康检查命令超过 3 秒没响应就视为失败，并且使用 curl -fs http://localhost/ || exit 1 作为健康检查命令。 使用 docker build 来构建这个镜像： $ docker build -t myweb:v1 . 构建好了后，我们启动一个容器： $ docker run -d --name web -p 80:80 myweb:v1 当运行该镜像后，可以通过 docker container ls 看到最初的状态为 (health: starting)： $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 03e28eb00bd0 myweb:v1 \"nginx -g 'daemon off\" 3 seconds ago Up 2 seconds (health: starting) 80/tcp, 443/tcp web 在等待几秒钟后，再次 docker container ls，就会看到健康状态变化为了 (healthy)： $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 03e28eb00bd0 myweb:v1 \"nginx -g 'daemon off\" 18 seconds ago Up 16 seconds (healthy) 80/tcp, 443/tcp web 如果健康检查连续失败超过了重试次数，状态就会变为 (unhealthy)。 为了帮助排障，健康检查命令的输出（包括 stdout 以及 stderr）都会被存储于健康状态里，可以用 docker inspect 来查看。 $ docker inspect --format '{{json .State.Health}}' web | python -m json.tool { \"FailingStreak\": 0, \"Log\": [ { \"End\": \"2016-11-25T14:35:37.940957051Z\", \"ExitCode\": 0, \"Output\": \"\u003c!DOCTYPE html\u003e\\n\u003chtml\u003e\\n\u003chead\u003e\\n\u003ctitle\u003eWelcome to nginx!\u003c/title\u003e\\n\u003cstyle\u003e\\n body {\\n width: 35em;\\n margin: 0 auto;\\n font-family: Tahoma, Verdana, Arial, sans-serif;\\n }\\n\u003c/style\u003e\\n\u003c/head\u003e\\n\u003cbody\u003e\\n\u003ch1\u003eWelcome to nginx!\u003c/h1\u003e\\n\u003cp\u003eIf you see this page, the nginx web server is successfully installed and\\nworking. Further configuration is required.\u003c/p\u003e\\n\\n\u003cp\u003eFor online documentation and support please refer to\\n\u003ca href=\\\"http://nginx.org/\\\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e\\nCommercial support is available at\\n\u003ca href=\\\"http://nginx.com/\\\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e\\n\\n\u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e\\n\u003c/body\u003e\\n\u003c/html\u003e\\n\", \"Start\": \"2016-11-25T14:35:37.780192565Z\" } ], \"Status\": \"healthy\" } ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:16:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"ONBUILD 为他人作嫁衣裳 格式：ONBUILD \u003c其它指令\u003e。 ONBUILD 是一个特殊的指令，它后面跟的是其它指令，比如 RUN, COPY 等，而这些指令，在当前镜像构建时并不会被执行。只有当以当前镜像为基础镜像，去构建下一级镜像的时候才会被执行。 Dockerfile 中的其它指令都是为了定制当前镜像而准备的，唯有 ONBUILD 是为了帮助别人定制自己而准备的。 假设我们要制作 Node.js 所写的应用的镜像。我们都知道 Node.js 使用 npm 进行包管理，所有依赖、配置、启动信息等会放到 package.json 文件里。在拿到程序代码后，需要先进行 npm install 才可以获得所有需要的依赖。然后就可以通过 npm start 来启动应用。因此，一般来说会这样写 Dockerfile： FROMnode:slimRUN mkdir /appWORKDIR/appCOPY ./package.json /appRUN [ \"npm\", \"install\" ]COPY . /app/CMD [ \"npm\", \"start\" ] 把这个 Dockerfile 放到 Node.js 项目的根目录，构建好镜像后，就可以直接拿来启动容器运行。但是如果我们还有第二个 Node.js 项目也差不多呢？好吧，那就再把这个 Dockerfile 复制到第二个项目里。那如果有第三个项目呢？再复制么？文件的副本越多，版本控制就越困难，让我们继续看这样的场景维护的问题。 如果第一个 Node.js 项目在开发过程中，发现这个 Dockerfile 里存在问题，比如敲错字了、或者需要安装额外的包，然后开发人员修复了这个 Dockerfile，再次构建，问题解决。\u0008第一个项目没问题了，但是第二个项目呢？虽然最初 Dockerfile 是复制、粘贴自第一个项目的，但是并不会因为第一个项目修复了他们的 Dockerfile，而第二个项目的 Dockerfile 就会被自动修复。 那么我们可不可以做一个基础镜像，然后各个项目使用这个基础镜像呢？这样基础镜像更新，各个项目不用同步 Dockerfile 的变化，重新构建后就继承了基础镜像的更新？好吧，可以，让我们看看这样的结果。那么上面的这个 Dockerfile 就会变为： FROMnode:slimRUN mkdir /appWORKDIR/appCMD [ \"npm\", \"start\" ] 这里我们把项目相关的构建指令拿出来，放到子项目里去。假设这个基础镜像的名字为 my-node 的话，各个项目内的自己的 Dockerfile 就变为： FROMmy-nodeCOPY ./package.json /appRUN [ \"npm\", \"install\" ]COPY . /app/ 基础镜像变化后，各个项目都用这个 Dockerfile 重新构建镜像，会继承基础镜像的更新。 那么，问题解决了么？没有。准确说，只解决了一半。如果这个 Dockerfile 里面有些东西需要调整呢？比如 npm install 都需要加一些参数，那怎么办？这一行 RUN 是不可能放入基础镜像的，因为涉及到了当前项目的 ./package.json，难道又要一个个修改么？所以说，这样制作基础镜像，只解决了原来的 Dockerfile 的前4条指令的变化问题，而后面三条指令的变化则完全没办法处理。 ONBUILD 可以解决这个问题。让我们用 ONBUILD 重新写一下基础镜像的 Dockerfile: FROMnode:slimRUN mkdir /appWORKDIR/appONBUILD COPY ./package.json /appONBUILD RUN [ \"npm\", \"install\" ]ONBUILD COPY . /app/CMD [ \"npm\", \"start\" ] 这次我们回到原始的 Dockerfile，但是这次将项目相关的指令加上 ONBUILD，这样在构建基础镜像的时候，这三行并不会被执行。然后各个项目的 Dockerfile 就变成了简单地： FROMmy-node 是的，只有这么一行。当在各个项目目录中，用这个只有一行的 Dockerfile 构建镜像时，之前基础镜像的那三行 ONBUILD 就会开始执行，成功的将当前项目的代码复制进镜像、并且针对本项目执行 npm install，生成应用镜像。 LABEL 为镜像添加元数据 LABEL 指令用来给镜像以键值对的形式添加一些元数据（metadata）。 LABEL \u003ckey\u003e=\u003cvalue\u003e \u003ckey\u003e=\u003cvalue\u003e \u003ckey\u003e=\u003cvalue\u003e ... 我们还可以用一些标签来申明镜像的作者、文档地址等： LABEL org.opencontainers.image.authors=\"yeasy\"LABEL org.opencontainers.image.documentation=\"https://yeasy.gitbooks.io\" 具体可以参考 https://github.com/opencontainers/image-spec/blob/master/annotations.md ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:17:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"SHELL 指令 格式：SHELL [\"executable\", \"parameters\"] SHELL` 指令可以指定 `RUN` `ENTRYPOINT` `CMD` 指令的 shell，Linux 中默认为 `[\"/bin/sh\", \"-c\"] SHELL [\"/bin/sh\", \"-c\"]RUN lll ; lsSHELL [\"/bin/sh\", \"-cex\"]RUN lll ; ls 两个 RUN 运行同一命令，第二个 RUN 运行的命令会打印出每条命令并当遇到错误时退出。 当 ENTRYPOINT CMD 以 shell 格式指定时，SHELL 指令所指定的 shell 也会成为这两个指令的 shell SHELL [\"/bin/sh\", \"-cex\"]# /bin/sh -cex \"nginx\"ENTRYPOINT nginx SHELL [\"/bin/sh\", \"-cex\"]# /bin/sh -cex \"nginx\"CMD nginx 参考文档 Dockerfie 官方文档：https://docs.docker.com/engine/reference/builder/ Dockerfile 最佳实践文档：https://docs.docker.com/develop/develop-images/dockerfile_best-practices/ Docker 官方镜像 Dockerfile：https://github.com/docker-library/docs docker优质博客教程 https://yeasy.gitbook.io/docker_practice/image ","date":"212111-08-2151","objectID":"/docker_dockerbuilder/:18:0","tags":["docker"],"title":"docker - Dockerfile","uri":"/docker_dockerbuilder/"},{"categories":["docker"],"content":"docker 怎么走代理.","date":"212111-08-2151","objectID":"/docker_proxy/","tags":["docker"],"title":"docker - 代理","uri":"/docker_proxy/"},{"categories":["docker"],"content":"docker三种网络代理模式 有时因为网络原因，比如公司NAT，或其它啥的，需要使用代理。 Docker的代理配置，略显复杂，因为有三种场景。 但基本原理都是一致的，都是利用Linux的http_proxy等环境变量。 ","date":"212111-08-2151","objectID":"/docker_proxy/:0:0","tags":["docker"],"title":"docker - 代理","uri":"/docker_proxy/"},{"categories":["docker"],"content":"dockerd代理 在执行docker pull时，是由守护进程dockerd来执行。 因此，代理需要配在dockerd的环境中。 而这个环境，则是受systemd所管控，因此实际是systemd的配置。 sudo mkdir -p /etc/systemd/system/docker.service.d sudo touch /etc/systemd/system/docker.service.d/proxy.conf 在这个proxy.conf文件（可以是任意*.conf的形式）中，添加以下内容： [Service] Environment=\"HTTP_PROXY=http://proxy.example.com:8080/\" Environment=\"HTTPS_PROXY=http://proxy.example.com:8080/\" Environment=\"NO_PROXY=localhost,127.0.0.1,.example.com\" 其中，proxy.example.com:8080要换成可用的免密代理。 通常使用cntlm在本机自建免密代理，去对接公司的代理。 可参考《Linux下安装配置Cntlm代理》。 ","date":"212111-08-2151","objectID":"/docker_proxy/:1:0","tags":["docker"],"title":"docker - 代理","uri":"/docker_proxy/"},{"categories":["docker"],"content":"Container代理 在容器运行阶段，如果需要代理上网，则需要配置~/.docker/config.json。 以下配置，只在Docker 17.07及以上版本生效。 { \"proxies\": { \"default\": { \"httpProxy\": \"http://proxy.example.com:8080\", \"httpsProxy\": \"http://proxy.example.com:8080\", \"noProxy\": \"localhost,127.0.0.1,.example.com\" } } } 这个是用户级的配置，除了proxies，docker login等相关信息也会在其中。 而且还可以配置信息展示的格式、插件参数等。 此外，容器的网络代理，也可以直接在其运行时通过-e注入http_proxy等环境变量。 这两种方法分别适合不同场景。 config.json非常方便，默认在所有配置修改后启动的容器生效，适合个人开发环境。 在CI/CD的自动构建环境、或者实际上线运行的环境中，这种方法就不太合适，用-e注入这种显式配置会更好，减轻对构建、部署环境的依赖。 当然，在这些环境中，最好用良好的设计避免配置代理上网。 ","date":"212111-08-2151","objectID":"/docker_proxy/:2:0","tags":["docker"],"title":"docker - 代理","uri":"/docker_proxy/"},{"categories":["docker"],"content":"docker build代理 虽然docker build的本质，也是启动一个容器，但是环境会略有不同，用户级配置无效。 在构建时，需要注入http_proxy等参数。 docker build . \\ --build-arg \"HTTP_PROXY=http://proxy.example.com:8080/\" \\ --build-arg \"HTTPS_PROXY=http://proxy.example.com:8080/\" \\ --build-arg \"NO_PROXY=localhost,127.0.0.1,.example.com\" \\ -t your/image:tag 注意：无论是docker run还是docker build，默认是网络隔绝的。 如果代理使用的是localhost:3128这类，则会无效。 这类仅限本地的代理，必须加上--network host才能正常使用。 而一般则需要配置代理的外部IP，而且代理本身要开启gateway模式。 ","date":"212111-08-2151","objectID":"/docker_proxy/:3:0","tags":["docker"],"title":"docker - 代理","uri":"/docker_proxy/"},{"categories":["docker"],"content":"重启生效 代理配置完成后，reboot重启当然可以生效，但不重启也行。 docker build代理是在执行前设置的，所以修改后，下次执行立即生效。 Container代理的修改也是立即生效的，但是只针对以后启动的Container，对已经启动的Container无效。 dockerd代理的修改比较特殊，它实际上是改systemd的配置，因此需要重载systemd并重启dockerd才能生效。 sudo systemctl daemon-reload sudo systemctl restart docker ","date":"212111-08-2151","objectID":"/docker_proxy/:4:0","tags":["docker"],"title":"docker - 代理","uri":"/docker_proxy/"},{"categories":["docker"],"content":"参考 ¶ Control Docker with systemd | Docker Documentation Configure Docker to use a proxy server | Docker Documentation Use the Docker command line | Docker Documentation ","date":"212111-08-2151","objectID":"/docker_proxy/:5:0","tags":["docker"],"title":"docker - 代理","uri":"/docker_proxy/"},{"categories":["docker"],"content":"docker 常用指令介绍.","date":"212111-08-2151","objectID":"/docker_cmd/","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"docker 常用指令 ","date":"212111-08-2151","objectID":"/docker_cmd/:0:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"验证是否安装成功 $ docker version # 查看docker版本 # 或者 $ docker info # 查看docker系统的信息 ","date":"212111-08-2151","objectID":"/docker_cmd/:1:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"启动docker $ systemctl start docker ","date":"212111-08-2151","objectID":"/docker_cmd/:2:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"查看docker 镜像 $ docker image ls 或者 $ docker images ","date":"212111-08-2151","objectID":"/docker_cmd/:3:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"拉取镜像 $ docker pull \u003cimage\u003e:\u003ctag\u003e ","date":"212111-08-2151","objectID":"/docker_cmd/:4:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"删除镜像 $ docker rmi [image-id] # 删除镜像 $ docker rmi $(docker images -q) # 删除所有镜像 $ docker rmi $(sudo docker images --filter \"dangling=true\" -q --no-trunc) # 删除无用镜像 ","date":"212111-08-2151","objectID":"/docker_cmd/:5:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"列出正在运行的容器(containers) $ docker ps ","date":"212111-08-2151","objectID":"/docker_cmd/:6:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"列出所有的容器(包括不运行的容器) $ docker ps -a ","date":"212111-08-2151","objectID":"/docker_cmd/:7:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"容器相关操作 docker exec -it # 容器ID sh 进入容器 docker stop ‘container’ # 停止一个正在运行的容器，‘container’可以是容器ID或名称 docker start ‘container’ # 启动一个已经停止的容器 docker restart ‘container’ # 重启容器 docker rm ‘container’ # 删除容器 docker run -i -t -p :80 LAMP /bin/bash # 运行容器并做http端口转发 docker exec -it ‘container’ /bin/bash # 进入ubuntu类容器的bash docker exec -it \u003ccontainer\u003e /bin/sh # 进入alpine类容器的sh docker rm docker ps -a -q # 删除所有已经停止的容器 docker kill $(docker ps -a -q) # 杀死所有正在运行的容器，$()功能同`` ","date":"212111-08-2151","objectID":"/docker_cmd/:8:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"查看容器日志 查看容器日志 docker logs \u003cid/container_name\u003e 实时查看日志输出 docker logs -f \u003cid/container_name\u003e (类似 tail -f) (带上时间戳-t） ","date":"212111-08-2151","objectID":"/docker_cmd/:9:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"docker镜像的导出和导入 $ docker save -o \u003c保存路径\u003e \u003c镜像名称:标签\u003e # 导出镜像$ docker save -o ./ubuntu18.tar ubuntu:18.04 # 导出镜像$ docker load --input ./ubuntu18.tar # 导入镜像 ","date":"212111-08-2151","objectID":"/docker_cmd/:10:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"制作自己的镜像 **docker commit :**从容器创建一个新的镜像。 $ docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]] Dockerfile:命令用于使用 Dockerfile 创建镜像。 $ docker build [OPTIONS] PATH | URL | -例如：$ docker build -t ztc/ubuntu:v1 . ","date":"212111-08-2151","objectID":"/docker_cmd/:11:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"从容器中导出文件 # 从主机复制到容器 $ docker cp host_path containerID:container_path # 从容器复制到主机 $ docker cp containerID:container_path host_path ","date":"212111-08-2151","objectID":"/docker_cmd/:12:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"容器启动参数 $ docker run ：创建一个新的容器并运行一个命令 语法 docker run [OPTIONS] IMAGE [COMMAND] [ARG...] OPTIONS说明： -a stdin: 指定标准输入输出内容类型，可选 STDIN/STDOUT/STDERR 三项； -d: 后台运行容器，并返回容器ID； -i: 以交互模式运行容器，通常与 -t 同时使用； -P: 随机端口映射，容器内部端口随机映射到主机的端口 -p: 指定端口映射，格式为：主机(宿主)端口:容器端口 -t: 为容器重新分配一个伪输入终端，通常与 -i 同时使用； --name=\"nginx-lb\": 为容器指定一个名称； --dns 8.8.8.8: 指定容器使用的DNS服务器，默认和宿主一致； --dns-search example.com: 指定容器DNS搜索域名，默认和宿主一致； -h \"mars\": 指定容器的hostname； -e username=\"ritchie\": 设置环境变量； --env-file=[]: 从指定文件读入环境变量； --cpuset=\"0-2\" or --cpuset=\"0,1,2\": 绑定容器到指定CPU运行； -m :设置容器使用内存最大值； --net=\"bridge\": 指定容器的网络连接类型，支持 bridge/host/none/container: 四种类型； --link=[]: 添加链接到另一个容器； --expose=[]: 开放一个端口或一组端口； --volume , -v: 绑定一个卷 # 实例 # 使用docker镜像nginx:latest以后台模式启动一个容器,并将容器命名为mynginx。 $ docker run --name mynginx -d nginx:latest #使用镜像nginx:latest以后台模式启动一个容器,并将容器的80端口映射到主机随机端口。 $ docker run -P -d nginx:latest #使用镜像 nginx:latest，以后台模式启动一个容器,将容器的 80 端口映射到主机的 80 端口,主机的目录 /data 映射到容器的 /data。 $docker run -p 80:80 -v /data:/data -d nginx:latest #绑定容器的 8080 端口，并将其映射到本地主机 127.0.0.1 的 80 端口上。 $ docker run -p 127.0.0.1:80:8080/tcp ubuntu bash #使用镜像nginx:latest以交互模式启动一个容器,在容器内执行/bin/bash命令。 $ @ztc:~$ docker run -it nginx:latest /bin/bash ","date":"212111-08-2151","objectID":"/docker_cmd/:13:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["docker"],"content":"镜像仓库操作 docker login : 登陆到一个Docker镜像仓库，如果未指定镜像仓库地址，默认为官方仓库 Docker Hub docker logout : 登出一个Docker镜像仓库，如果未指定镜像仓库地址，默认为官方仓库 Docker Hub $ docker login -u 用户名 -p 密码$ docker logout ","date":"212111-08-2151","objectID":"/docker_cmd/:14:0","tags":["docker"],"title":"docker - 常用指令","uri":"/docker_cmd/"},{"categories":["istio"],"content":"istio 多集群管控流程介绍.","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"多云管理 Multi-cluster service mesh 多集群服务网格，配置工作负载来路由跨集群的流量。所有这些都符合应用Istio配置，如' VirtualServices ‘， ' DestinationRules ‘， ' Sidecar ‘等。 Mesh Federation 网格联邦，表示公开并支持两个独立服务网格的工作负载的通信 对于网格联邦，您可以查看Istio文档multiple mesh 或一个名为Gloo mesh的开源项目，以帮助自动化配置以支持它。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:0:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"多集群服务网格 多集群服务网格需要跨集群发现、连接和共同信任 跨集群工作负载发现 -控制平面必须发现对等集群中的工作负载，以便配置服务代理 (例如，集群的API server 必须可以访问对面集群中的Istio控制平面)。 跨集群工作负载连接性 -工作负载之间必须具有连接性。除非您可以初始化到工作负载端点的连接，否则对工作负载端点的感知是没有用的。 集群之间的共同信任 -跨集群的工作负载必须相互认证以使Istio的安全特性成为可能的PeerAuthentication和AuthorizationPolicy。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:1:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"多集群部署模型 集群分类： 主集群 （Primary Cluster） : 安装Istio控制平面的集群 远端集群（Remote Cluster）: 安装控制平面的远端集群 部署模型： Primary-Remote(共享控制平面) Primary-Primary(复用控制平面) External控制平面 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:2:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"Primary-Remote 部署模型 Primary-Remote部署模型有一个管理网格的单一控制平面，因此，它经常被称为单一控制平面或共享控制平面部署模型。 这种模型使用更少的资源，然而，主集群的中断会影响整个网格，因此它的可用性很低。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:2:1","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"Primary-Primary部署模型 Primary-Primary 部署模型有多个控制平面，这确保了更高的可用性，因为中断的范围仅限于发生中断的集群，但也需要更多的资源。我们将此模型称为复制控制平面部署模型。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:2:2","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"External控制平面 External控制平面 为所有集群远程到控制平面的部署模型。这种部署模型使云提供商能够将Istio作为托管服务提供。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:2:3","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"多集群下的服务发现 Istio的控制平面需要与Kubernetes API服务器通信，以收集相关信息来配置服务代理，比如 services 和 endpoints。 但是对 kubernetes 的 api server 的访问，存在安全的问题，因为您可以查找资源细节、查询敏感信息，并更新或删除资源，从而使集群处于糟糕且不可逆的状态。 可以参考 RBAC 来解决这个问题 为istiod 提供remote cluster的 service account token , istiod使用 这个token 对 remote cluster 进行身份验证，并发现运行在其中的工作负载。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:3:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"跨集群工作负载连接 两种情况： 集群处于同一网络平面，工作负载使用 IP 连接，天生满足条件。 集群处于不同网络，必须使用位于 网格边缘的特殊 istio 入口网关来代理跨集群网络 在“多网络”网格中桥接集群的入口网关 称为 “东西向网关”， 东西向网关将 作为一个 反向代理，将请求发送到个字集群中的工作负载。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:4:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"集群之间的相互信任 拥有公共信任可以确保不同集群的工作负载可以相互进行身份验证。 实现集群之间相互信任的方法： 第一种方法是使用我们称为Plug-In CA Certificates的东西，它是由一个公共根证书颁发机构颁发的用户定义的证书。 第二种方法是集成一个外部证书颁发机构，两个集群都使用它来签署证书。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:5:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"Plug-in CA Certificates 默认情况下，Istio CA生成自签名根证书和密钥，并使用它们对工作负载证书进行签名。要保护根CA密匙，应该使用在安全机器上脱机运行的根CA，并使用根CA向在每个集群中运行的Istio CA颁发中间证书。 Istio CA可以使用管理员指定的证书和密钥对工作负载证书进行签名，并将管理员指定的根证书作为信任根分发到工作负载。 因为，不是让 istio 生成中间证书颁发机构，而是通过在Istio安装名称空间上提供秘密证书来指定要使用的证书（就是创建一个secret 持久化到 etcd 中）。您可以对两个集群都这样做，并使用由公共根CA签名的中间CA。 上图为 使用由同一根签名的中间CA证书 但是上述方法存在安全风险： 如果暴露中间证书颁发机构，攻击者可以使用这些签名来签署证书，然后这些证书将被信任，直到检测到暴露和中间CA的证书被撤销。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:5:1","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"External certificate authority integration Cert-manager (待补充，见原文) Custom development （待补充，见原文） 多集群、多网络、多控制平面服务网格概述 West-cluster: Kubernetes cluster，该cluster在us-west区域拥有私有网络。我们将运行webapp服务。 East-cluster: Kubernetes cluster，它在us-east区域拥有自己的私有网络。在那里我们将运行catalog服务。 本文将建立一个一个多集群、多网络、多控制平面的服务网格，使用东西网关连接网络，并使用primary-primary部署模型 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:5:2","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"第一步 创建集群 创建两个 kubernetes 集群，每个集群都位于不同的网络上。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:6:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"第二步 配置 plug-in CA certificates 建立相互信任 Istio 默认会生成一个在 Istio-system namespace 下的 istio-ca-secret的 secret。 通过插入我们自己的证书颁发机构，可以覆盖这个 secret。 ​ cacert密钥由根CA的公钥和中间CA的公钥和私钥组成。根CA的私钥在集群外“安全”存储 创建一个cacerts secret，包含如下输入文件ca-cert.pem, ca-key.pem, root-cert.pem 和cert-chain.pem。需要注意的是创建出来的secret的名称必须是cacerts，这样才能被istio正确挂载。 ca-cert.pem - 中间CA的证书 ca-key.pem - 中间CA的私钥 root-cert.pem - 颁发中间CA的根CA的证书，用于验证由它颁发的任何中间CA颁发的证书 cert-chain.pem - 中间CA证书和根CA证书的连接，形成信任链 # 通过创建命名空间' istio-system '，然后将证书作为名为' cacerts '的秘密文件应用，在每个集群中配置中间CA。 # setting up certificates for the west-cluster （在 west-cluster 下操作） $ kubectl create namespace istio-system $ kubectl create secret generic cacerts -n istio-system \\ --from-file=ch12/certs/west-cluster/ca-cert.pem \\ --from-file=ch12/certs/west-cluster/ca-key.pem \\ --from-file=ch12/certs/root-cert.pem \\ --from-file=ch12/certs/west-cluster/cert-chain.pem # setting up certificates for the east-cluster (在 east-cluster 下操作) $ kubectl create namespace istio-system $ kubectl create secret generic cacerts -n istio-system \\ --from-file=ch12/certs/east-cluster/ca-cert.pem \\ --from-file=ch12/certs/east-cluster/ca-key.pem \\ --from-file=ch12/certs/root-cert.pem \\ --from-file=ch12/certs/east-cluster/cert-chain.pem ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:7:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"第三步 安装各个集群的控制面 继续安装Istio控制面，该控制面将挑选插件CA证书(即用户定义的中间证书)来签署工作负载证书。 标记集群的网络拓扑 # 标记 west-cluster的 网络拓扑 $ kubectl label namespace istio-system topology.istio.io/network=west-network # 标记 east-cluster的 网络拓扑 $ kubectl label namespace istio-system topology.istio.io/network=east-network 通过这些标签，Istio形成了对网络拓扑的理解，并使用它来决定如何配置工作负载。 部署控制面 部署 west 集群 # 方式一 $ istioctl install --set profile=demo \\ --set values.global.meshID=usmesh \\ --set values.global.multiCluster.clusterName=west-cluster \\ --set values.global.network=west-network # 方式二 使用 istioOperator 部署 apiVersion: install.istio.io/v1alpha1 metadata: name: istio-controlplane namespace: istio-system kind: IstioOperator spec: profile: demo components: egressGateways: - name: istio-egressgateway enabled: false values: global: meshID: usmesh # 属性' meshID '使我们能够识别这个安装属于哪个网格。 Istio提供了在集群中安装多个网格的选项，允许团队分别管理他们网格的操作。 multiCluster: clusterName: west-cluster network: west-network # 上述 yaml 使用如下命令安装： $ istioctl install -f xxx.yaml 使用同样的方式部署 east 集群， 和 west 集群的不同在于 clusterName 和 network 不同 apiVersion:install.istio.io/v1alpha1metadata:name:istio-controlplanenamespace:istio-systemkind:IstioOperatorspec:profile:democomponents:egressGateways:- name:istio-egressgatewayenabled:falsevalues:global:meshID:usmeshmultiCluster:clusterName:east-clusternetwork:east-network 在 west 和 east 集群 安装完控制面后，我们有了两个独立的网格，两个网格都运行着 istiod ，但是只发现本地的服务。当前网格的状态如下图所示： 当前网格缺乏跨集群工作负载发现和连接，会在 第四步 和 第五步 讨论。 此时可以在集群中创建一些工作负载，分别在 west 和 east 集群中，运行如下： # 在 west 集群安装应用和 网关 $ kubectl create ns istioinaction $ kubectl label namespace istioinaction istio-injection=enabled $ kubectl -n istioinaction apply -f ./webapp-deployment-svc.yaml $ kubectl -n istioinaction apply -f ./webapp-gw-vs.yaml $ kubectl -n istioinaction apply -f ./catalog-svc.yaml # 在 east 集群安装 catalog $ kubectl create ns istioinaction $ kubectl label namespace istioinaction istio-injection=enabled $ kubectl -n istioinaction apply -f ./catalog.yaml 测试用用例如上所示。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:8:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"第四步 启动跨集群的服务发现 为了从远程集群中查询信息，istio 需要使用 service account 进行身份验证。 istio 在安装时会创建一个名为 istio-reader-service-account 的 service account，具有可被另一个控制平面使用的最小权限集。 $ kubectl get sa -n istio-system NAME SECRETS AGE default 1 4d20h istio-egressgateway-service-account 1 4d20h istio-ingressgateway-service-account 1 4d20h istio-reader-service-account 1 4d20h istiod-service-account 1 4d20h 但是，我们需要使服务帐户令牌对对端的集群可用，以及证书来启动到远程集群的安全连接。 创建对端集群访问的secret # 在 east-cluster上执行如下： (创建时，务必指明集群名称 --name) $ istioctl x create-remote-secret --name=\"east-cluster\" # 得到如下结果 # This file is autogenerated, do not edit. apiVersion: v1 kind: Secret metadata: annotations: networking.istio.io/cluster: east-cluster creationTimestamp: null labels: istio/multiCluster: \"true\" name: istio-remote-secret-east-cluster namespace: istio-system stringData: east-cluster: | apiVersion: v1 clusters: # 包含集群地址和验证API服务器提供的连接的证书颁发机构数据的集群列表。 - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkF... server: https://10.20.144.83:6443 name: east-cluster contexts: # 上下文列表，每组一个用户和一个集群，这简化了切换集群(与我们的用例无关)。 - context: cluster: east-cluster user: east-cluster name: east-cluster current-context: east-cluster kind: Config preferences: {} users: # 一个列表，定义了包含要对API服务器进行身份验证的令牌的用户 - name: east-cluster user: token: eyJhbGciOiJSUzI1NiIsImtpZCI6.... --- # 该命令使用默认的 istio-reader-service-account service account 为远程集群访问创建secret。 # 上述的 secret 就是“kubectl”需要启动到Kubernetes API服务器的安全连接并对其进行身份验证的全部内容。 # 需要将 上述内容 手动 执行 kubectl apply -f xxx.yaml 应到到 west-cluster ⚠️ 注意 上述的内容，先在一个集群中(east-cluster)生成 secret 的 yaml ，然后将生成的 yaml 放到 remote 集群中(west-cluster) 中 执行 apply 一旦在 remote 集群中创建了secret，istiod就会获取它，并在新添加的 remote 集群中查询工作负载。通过查看日志可以更详细看到细节： # 在 remote 集群中(west-cluster) 查看 istiod 的日志 $ kubectl logs deploy/istiod -n istio-system | grep 'Adding cluster' 2021-10-08T08:47:32.408052Z info Adding cluster_id=east-cluster from secret=istio-system/istio-remote-secret-east-cluster # 通过日志可以验证集群是否初始化完成，并且west-cluster的控制面可以发现 east-cluster 的 workload 为了配置成 primary-primary 部署模式，同样要对 对端集群做同样的操作，使得 east-cluster 的控制面可以发现 west-cluster 的 workload。 # 在west-cluster 中执行如下： $ kubectl x create-remote-secret --name=\"west-cluster\" # 将生成的secret 放到 east-cluster中apply,配置 east-cluster查看west-cluster $ kubectl apply -f 刚刚生成的secret.yaml secret/istio-remote-secret-west-cluster created ❤️ 现在，双方的控制面都可以查询到对端集群的 workload，接下来要去设置跨集群连接。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:9:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"第五步 建立跨集群连接 不同内部网络(在我们的实例中是集群网络)之间的流量称为“东西流量” 将内部服务通过网关定向到内部网络的流量称为“南北向流量” 南北向流量管理：使用Ingress将Kubernetes中的应用暴露成对外提供的服务，针对这个对外暴露的服务可以实现灰度发布、A/B测试、全量发布、流量管理等。我们把这种流量管理称为南北向流量管理，也就是入口请求到集群服务的流量管理。 东西向流量管理：Istio还有侧重于集群服务网格之间的流量管理，我们把这类管理称为东西向流量管理。 对于不同云服务商的集群，或者网络并非对等连接的地方，istio 提供了东西向网关作为解决方案。 Istio 东西向网关 东西向网关的目标初了成为跨集群东西向流量的入口点之外，还在于使这对运营服务的团队透明。为了实现这一目标，它必须: 开启跨集群的细粒度流量管理 通过路由加密流量，实现负载之间的双向认证 istio 有两个特性： SNI cluster SNI Auto Passthrough 东西向网关配置 SNI cluster # cluster-east-eastwest-gateway.yamlapiVersion:install.istio.io/v1alpha1kind:IstioOperatormetadata:name:istio-eastwestgateway # 一定不能和前面的 istioOperator 资源相同，如果相同，将会覆盖之前的istioOpeatornamespace:istio-systemspec:profile:emptycomponents:ingressGateways:- name:istio-eastwestgatewaylabel:istio:eastwestgatewayapp:istio-eastwestgatewayenabled:truek8s:env:- name:ISTIO_META_ROUTER_MODE # SNI集群的配置是一个可选特性value:\"sni-dnat\"# 网关路由器模式设置为' snii -dnat '来启用, 自动配置SNI集群- name:ISTIO_META_REQUESTED_NETWORK_VIEW# The network to which traffic is routedvalue:east-networkservice:ports:# redacted for brevityvalues:global:meshID:usmeshmultiCluster:clusterName:east-cluster # 注意：这是 east-clusternetwork:east-network # 在 east 集群中执行如下，apply 上述的yaml $ istioctl install -y -f ./cluster-east-eastwest-gateway.yaml ✔ Ingress gateways installed ✔ Installation complete 安装了东西向网关后，可以查询网关的集群代理配置并将输出过滤为仅包含“catalog”文本的行，来验证SNI集群是否配置了。 # 在 east 集群中执行如下： $ istioctl pc clusters deploy/istio-eastwestgateway.istio-system \\ | grep catalog | awk '{printf \"CLUSTER: %s\\n\", $1}' CLUSTER: catalog.istioinaction.svc.cluster.local CLUSTER: outbound_.80_._.catalog.istioinaction.svc.cluster.local 如何使用SNI Auto Passthrough路由跨集群流量 SNI Auto Passthrough，顾名思义，不需要手动创建“VirtualServices”来路由允许的流量。 这是使用SNI集群完成的，这些集群使用“snii -dnat”路由器模式在东西网关中自动配置。 SNI Auto Passthrough 模式 使用 Istio Gateway 配置 apiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:cross-network-gatewaynamespace:istio-systemspec:selector:istio:eastwestgatewayservers:- port:number:15443name:tlsprotocol:TLStls:mode:AUTO_PASSTHROUGHhosts:- \"*.local\" # 把east-cluster的workload 暴露给 west-cluster # 在 east-cluster 上操作 $ kubectl apply -n istio-system -f 上述.yaml gateway.networking.istio.io/cross-network-gateway created 同样，需要在west-cluster上进行相似的操作 # west-cluster 环境中 # 首先部署 istioOperator 创建东西向网关， kubectl apply -f 下述yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: name: istio-eastwestgateway namespace: istio-system spec: profile: empty components: ingressGateways: - name: istio-eastwestgateway label: istio: eastwestgateway app: istio-eastwestgateway topology.istio.io/network: west-network enabled: true k8s: env: - name: ISTIO_META_ROUTER_MODE value: \"sni-dnat\" # The network to which traffic is routed - name: ISTIO_META_REQUESTED_NETWORK_VIEW value: west-network service: ports: - name: status-port port: 15021 targetPort: 15021 - name: mtls port: 15443 targetPort: 15443 - name: tcp-istiod port: 15012 targetPort: 15012 - name: tcp-webhook port: 15017 targetPort: 15017 values: global: meshID: usmesh multiCluster: clusterName: west-cluster # 注意：这是 west-cluster network: west-network # 然后将集群中的服务暴露给 east-cluster, kubectl apply -f 下述yaml (其实这个和在east-cluster中配置的yaml是一样) apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: cross-network-gateway namespace: istio-system spec: selector: istio: eastwestgateway servers: - port: number: 15443 name: tls protocol: TLS tls: mode: AUTO_PASSTHROUGH hosts: - \"*.local\" 使用 SNI 自动直通配置网关后，网关上的传入流量将使用 SNI 集群路由到预期的工作负载。 Istio 的控制平面侦听这些资源的创建并发现现在存在路由跨集群流量的路径。 因此，它使用新发现的端点更新所有工作负载。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:10:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"第六步 验证跨集群 workload discovery 之前的步骤中，我们已经准备好了测试用例，现在可以派上用场了。 测试用例描述： 在 west-cluster 中，部署了 webapp ，它有 deployment 和 service ，同时部署了 catalog，但是catalog只部署了service 在easy-cluster中，部署了catalog，它有 deployment 和 service 经过之前的部署，现在 east-cluster 的 workloads 已经暴露于 west-cluster ，我们希望 west-cluster 中的 webapp 的 envoy cluster 配置中，存在一个 catalog 的 endpoints， 因为我们并没有在 west-cluster 中部署 catalog 的deployment，而是在 east-cluster 中部署了 catalog 的 deployment，如果集群之间没有打通，那么在 west-cluster 中 是不会存在 catalog 的 endpoints。 并且，这个 endpoints 应该指向 东西向网关的地址，该地址将请求代理到其网络中的 catalog workload # 首先获取 east-cluster 的 east-west gateway 的 地址 (在 east-cluster 下操作) $ kubectl -n istio-system get svc istio-eastwestgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 40.114.190.251 # 将其与 west-cluster 中的 workload 在跨集群通讯时使用的地址进行比较 (在 west-cluster 下操作) $ istioctl pc endpoints deploy/webapp.istioinaction | grep catalog 40.114.190.251:15443 HEALTHY OK outbound|80||catalog.istioinaction.svc.cluster.local ^-----------^^---^ | | | # 如果 catalog 的 endpoint 与 东西向网关的地址匹配，那么就可以发现 workload ,实现跨集群通讯。 # 让我们在west-cluster中手动发起一个到catalog 的请求 $ EXT_IP=$(kubectl -n istio-system get svc istio-ingressgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ curl http://$EXT_IP/api/catalog -H \"Host: webapp.istioinaction.io\" [ { \"id\": 0, \"color\": \"teal\", \"department\": \"Clothing\", \"name\": \"Small Metal Shoes\", \"price\": \"232.00\" } ] # 可以看到，对入口网关的请求被路由到了 west-cluster 中的 webapp 中， 然后被分配到了 east-cluster 中的 caltalog workload，并最终服务于请求。 至此，我们已经验证集群、多网络、多控制平面服务网格的建立，并发现了跨集群的工作负载，它们可以使用东西网关作为通道启动相互验证的连接。 让我们回顾一下建立多集群服务网格需要做些什么: 跨集群工作负载发现通过使用包含服务帐户令牌和证书的* kubecconfig *为每个控制平面提供对对等集群的访问，这个过程使用了’ istioctl ‘，我们只将它应用到相反的集群。 跨集群工作负载连接性通过配置东西网关在不同集群(驻留在不同网络中)的工作负载之间路由流量，并为每个集群标记网络信息，以便Istio知道网络负载驻留。 通过使用颁发相反集群的中间证书的公共信任根来配置集群之间的信任。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:11:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"第七步 集群间的负载均衡 这部分放在 负载均衡 部分，和本地负载均衡一起介绍。 ","date":"212111-08-2151","objectID":"/%E5%A4%9A%E4%BA%91/:12:0","tags":["istio","多集群"],"title":"istio - 多集群管控入门","uri":"/%E5%A4%9A%E4%BA%91/"},{"categories":["istio"],"content":"istio 流量走向详解.","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"istio 流量走向详解 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:0:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"前言 Istio作为一个service mesh开源项目,其中最重要的功能就是对网格中微服务之间的流量进行管理,包括服务发现,请求路由和服务间的可靠通信。Istio实现了service mesh的控制面，并整合Envoy开源项目作为数据面的sidecar，一起对流量进行控制。 Istio体系中流量管理配置下发以及流量规则如何在数据面生效的机制相对比较复杂，通过官方文档容易管中窥豹，难以了解其实现原理。本文尝试结合系统架构、配置文件和代码对Istio流量管理的架构和实现机制进行分析，以达到从整体上理解Pilot和Envoy的流量管理机制的目的。 Pilot高层架构 Istio控制面中负责流量管理的组件为Pilot，Pilot的高层架构如下图所示： Pilot Architecture（来自Isio官网文档[1]) 根据上图,Pilot主要实现了下述功能： ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:1:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"统一的服务模型 Pilot定义了网格中服务的标准模型，这个标准模型独立于各种底层平台。由于有了该标准模型，各个不同的平台可以通过适配器和Pilot对接，将自己特有的服务数据格式转换为标准格式，填充到Pilot的标准模型中。 例如Pilot中的Kubernetes适配器通过Kubernetes API服务器得到kubernetes中service和pod的相关信息，然后翻译为标准模型提供给Pilot使用。通过适配器模式，Pilot还可以从Mesos, Cloud Foundry, Consul等平台中获取服务信息，还可以开发适配器将其他提供服务发现的组件集成到Pilot中。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:2:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"标准数据面 API Pilo使用了一套起源于Envoy项目的标准数据面API[2]来将服务信息和流量规则下发到数据面的sidecar中。 通过采用该标准API，Istio将控制面和数据面进行了解耦，为多种数据面sidecar实现提供了可能性。事实上基于该标准API已经实现了多种Sidecar代理和Istio的集成，除Istio目前集成的Envoy外，还可以和Linkerd, Nginmesh等第三方通信代理进行集成，也可以基于该API自己编写Sidecar实现。 控制面和数据面解耦是Istio后来居上，风头超过Service mesh鼻祖Linkerd的一招妙棋。Istio站在了控制面的高度上，而Linkerd则成为了可选的一种sidecar实现，可谓降维打击的一个典型成功案例！ 数据面标准API也有利于生态圈的建立，开源，商业的各种sidecar以后可能百花齐放，用户也可以根据自己的业务场景选择不同的sidecar和控制面集成，如高吞吐量的，低延迟的，高安全性的等等。有实力的大厂商可以根据该API定制自己的sidecar，例如蚂蚁金服开源的Golang版本的Sidecar MOSN(Modular Observable Smart Netstub)（SOFAMesh中Golang版本的Sidecar)；小厂商则可以考虑采用成熟的开源项目或者提供服务的商业sidecar实现。 备注：Istio和Envoy项目联合制定了Envoy V2 API,并采用该API作为Istio控制面和数据面流量管理的标准接口。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:3:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"业务DSL语言 Pilot还定义了一套DSL（Domain Specific Language）语言，DSL语言提供了面向业务的高层抽象，可以被运维人员理解和使用。运维人员使用该DSL定义流量规则并下发到Pilot，这些规则被Pilot翻译成数据面的配置，再通过标准API分发到Envoy实例，可以在运行期对微服务的流量进行控制和调整。 Pilot的规则DSL是采用K8S API Server中的Custom Resource (CRD)[3]实现的，因此和其他资源类型如Service Pod Deployment的创建和使用方法类似，都可以用Kubectl进行创建。 通过运用不同的流量规则，可以对网格中微服务进行精细化的流量控制，如按版本分流，断路器，故障注入，灰度发布等。 Istio流量管理相关组件 我们可以通过下图了解Istio流量管理涉及到的相关组件。虽然该图来自Istio Github old pilot repo, 但图中描述的组件及流程和目前Pilot的最新代码的架构基本是一致的。 Pilot Design Overview (来自Istio old_pilot_repo[4]) 图例说明：图中红色的线表示控制流，黑色的线表示数据流。蓝色部分为和Pilot相关的组件。 从上图可以看到，Istio中和流量管理相关的有以下组件： ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:4:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"控制面组件 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:5:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Discovery Services 对应的docker为gcr.io/istio-release/pilot,进程为pilot-discovery，该组件的功能包括： 从Service provider（如kubernetes或者consul）中获取服务信息 从K8S API Server中获取流量规则(K8S CRD Resource) 将服务信息和流量规则转化为数据面可以理解的格式，通过标准的数据面API下发到网格中的各个sidecar中。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:5:1","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"K8S API Server 提供Pilot相关的CRD Resource的增、删、改、查。和Pilot相关的CRD有以下几种: Virtualservice：用于定义路由规则，如根据来源或 Header 制定规则，或在不同服务版本之间分拆流量。 DestinationRule：定义目的服务的配置策略以及可路由子集。策略包括断路器、负载均衡以及 TLS 等。 ServiceEntry：可以使用ServiceEntry向Istio中加入附加的服务条目，以使网格内可以向istio 服务网格之外的服务发出请求。 Gateway：为网格配置网关，以允许一个服务可以被网格外部访问。 EnvoyFilter：可以为Envoy配置过滤器。由于Envoy已经支持Lua过滤器，因此可以通过EnvoyFilter启用Lua过滤器，动态改变Envoy的过滤链行为。我之前一直在考虑如何才能动态扩展Envoy的能力，EnvoyFilter提供了很灵活的扩展性。 Sidecar：缺省情况下，Pilot将会把和Envoy Sidecar所在namespace的所有services的相关配置，包括inbound和outbound listenter, cluster, route等，都下发给Enovy。使用Sidecar可以对Pilot向Envoy Sidcar下发的配置进行更细粒度的调整，例如只向其下发该Sidecar 所在服务需要访问的那些外部服务的相关outbound配置。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:5:2","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"数据面组件 在数据面有两个进程Pilot-agent和envoy，这两个进程被放在一个docker容器gcr.io/istio-release/proxyv2中。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:6:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Pilot-agent 该进程根据K8S API Server中的配置信息生成Envoy的配置文件，并负责启动Envoy进程。注意Envoy的大部分配置信息都是通过xDS接口从Pilot中动态获取的，因此Agent生成的只是用于初始化Envoy的少量静态配置。在后面的章节中，本文将对Agent生成的Envoy配置文件进行进一步分析。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:6:1","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Envoy Envoy由Pilot-agent进程启动，启动后，Envoy读取Pilot-agent为它生成的配置文件，然后根据该文件的配置获取到Pilot的地址，通过数据面标准API的xDS接口从pilot获取动态配置信息，包括路由（route），监听器（listener），服务集群（cluster）和服务端点（endpoint）。Envoy初始化完成后，就根据这些配置信息对微服务间的通信进行寻址和路由。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:6:2","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"命令行工具 kubectl和Istioctl，由于Istio的配置是基于K8S的CRD，因此可以直接采用kubectl对这些资源进行操作。Istioctl则针对Istio对CRD的操作进行了一些封装。Istioctl支持的功能参见该表格。 数据面标准API 前面讲到，Pilot采用了一套标准的API来向数据面Sidecar提供服务发现，负载均衡池和路由表等流量管理的配置信息。该标准API的文档参见Envoy v2 API[5]。Data Plane API Protocol Buffer Definition[6])给出了v2 grpc接口相关的数据结构和接口定义。 （备注：Istio早期采用了Envoy v1 API，目前的版本中则使用V2 API，V1已被废弃）。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:7:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"基本概念和术语 首先我们需要了解数据面API中涉及到的一些基本概念： Host：能够进行网络通信的实体（如移动设备、服务器上的应用程序）。在此文档中，主机是逻辑网络应用程序。一块物理硬件上可能运行有多个主机，只要它们是可以独立寻址的。在EDS接口中，也使用“Endpoint”来表示一个应用实例，对应一个IP+Port的组合。 Downstream：下游主机连接到 Envoy，发送请求并接收响应。 Upstream：上游主机接收来自 Envoy 的连接和请求，并返回响应。 Listener：监听器是命名网地址（例如，端口、unix domain socket等)，可以被下游客户端连接。Envoy 暴露一个或者多个监听器给下游主机连接。在Envoy中,Listener可以绑定到端口上直接对外服务，也可以不绑定到端口上，而是接收其他listener转发的请求。 Cluster：集群是指 Envoy 连接的一组上游主机，集群中的主机是对等的，对外提供相同的服务，组成了一个可以提供负载均衡和高可用的服务集群。Envoy 通过服务发现来发现集群的成员。可以选择通过主动健康检查来确定集群成员的健康状态。Envoy 通过负载均衡策略决定将请求路由到哪个集群成员。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:8:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"XDS服务接口 Istio数据面API定义了xDS服务接口，Pilot通过该接口向数据面sidecar下发动态配置信息，以对Mesh中的数据流量进行控制。xDS中的DS表示discovery service，即发现服务，表示xDS接口使用动态发现的方式提供数据面所需的配置数据。而x则是一个代词，表示有多种discover service。这些发现服务及对应的数据结构如下： LDS (Listener Discovery Service) envoy.api.v2.Listener CDS (Cluster Discovery Service) envoy.api.v2.RouteConfiguration EDS (Endpoint Discovery Service) envoy.api.v2.Cluster RDS (Route Discovery Service) envoy.api.v2.ClusterLoadAssignment ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:9:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"XDS服务接口的最终一致性考虑 xDS的几个接口是相互独立的，接口下发的配置数据是最终一致的。但在配置更新过程中，可能暂时出现各个接口的数据不匹配的情况，从而导致部分流量在更新过程中丢失。 设想这种场景：在CDS/EDS只知道cluster X的情况下,RDS的一条路由配置将指向Cluster X的流量调整到了Cluster Y。在CDS/EDS向Mesh中Envoy提供Cluster Y的更新前，这部分导向Cluster Y的流量将会因为Envoy不知道Cluster Y的信息而被丢弃。 对于某些应用来说，短暂的部分流量丢失是可以接受的，例如客户端重试可以解决该问题，并不影响业务逻辑。对于另一些场景来说，这种情况可能无法容忍。可以通过调整xDS接口的更新逻辑来避免该问题，对上面的情况，可以先通过CDS/EDS更新Y Cluster，然后再通过RDS将X的流量路由到Y。 一般来说，为了避免Envoy配置数据更新过程中出现流量丢失的情况，xDS接口应采用下面的顺序： CDS 首先更新Cluster数据（如果有变化） EDS 更新相应Cluster的Endpoint信息（如果有变化） LDS 更新CDS/EDS相应的Listener。 RDS 最后更新新增Listener相关的Route配置。 删除不再使用的CDS cluster和 EDS endpoints。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:10:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"ADS聚合发现服务 保证控制面下发数据一致性，避免流量在配置更新过程中丢失的另一个方式是使用ADS(Aggregated Discovery Services)，即聚合的发现服务。ADS通过一个gRPC流来发布所有的配置更新，以保证各个xDS接口的调用顺序，避免由于xDS接口更新顺序导致的配置数据不一致问题。 关于XDS接口的详细介绍可参考xDS REST and gRPC protocol[7] Bookinfo 示例程序分析 下面我们以Bookinfo为例对Istio中的流量管理实现机制，以及控制面和数据面的交互进行进一步分析。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:11:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Bookinfo程序结构 下图显示了Bookinfo示例程序中各个组件的IP地址，端口和调用关系，以用于后续的分析。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:12:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"xDS接口调试方法 首先我们看看如何对xDS接口的相关数据进行查看和分析。Envoy v2接口采用了gRPC，由于gRPC是基于二进制的RPC协议，无法像V1的REST接口一样通过curl和浏览器进行进行分析。但我们还是可以通过Pilot和Envoy的调试接口查看xDS接口的相关数据。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:13:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Pilot调试方法 Pilot在15014端口提供了下述调试接口[8]下述方法查看xDS接口相关数据。 PILOT=istio-pilot.istio-system:15014 # What is sent to envoy # Listeners and routes curl $PILOT/debug/adsz # Endpoints curl $PILOT/debug/edsz # Clusters curl $PILOT/debug/cdsz ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:13:1","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Envoy调试方法 Envoy提供了管理接口，缺省为localhost的15000端口，可以获取listener，cluster以及完整的配置数据导出功能。 kubectl exec productpage-v1-6d8bc58dd7-ts8kw -c istio-proxy curl http://127.0.0.1:15000/help /: Admin home page /certs: print certs on machine /clusters: upstream cluster status /config_dump: dump current Envoy configs (experimental) /cpuprofiler: enable/disable the CPU profiler /healthcheck/fail: cause the server to fail health checks /healthcheck/ok: cause the server to pass health checks /help: print out list of admin commands /hot_restart_version: print the hot restart compatibility version /listeners: print listener addresses /logging: query/change logging levels /quitquitquit: exit the server /reset_counters: reset all counters to zero /runtime: print runtime values /runtime_modify: modify runtime values /server_info: print server version/status information /stats: print server stats /stats/prometheus: print server stats in prometheus format 进入productpage pod 中的istio-proxy(Envoy) container，可以看到有下面的监听端口 9080: productpage进程对外提供的服务端口 15001: Envoy的Virtual Outbound监听器，iptable会将productpage服务发出的出向流量导入该端口中由Envoy进行处理 15006: Envoy的Virtual Inbound监听器，iptable会将发到productpage的入向流量导入该端口中由Envoy进行处理 15000: Envoy管理端口，该端口绑定在本地环回地址上，只能在Pod内访问。 15090：指向127.0.0.1：15000/stats/prometheus, 用于对外提供Envoy的性能统计指标 master $ kubectl exec productpage-v1-6d8bc58dd7-ts8kw -c istio-proxy -- netstat -ln Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:15090 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:9080 0.0.0.0:* LISTEN tcp 0 0 127.0.0.1:15000 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:15001 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:15006 0.0.0.0:* LISTEN tcp6 0 0 :::15020 :::* LISTEN ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:13:2","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Envoy启动过程分析 Istio通过K8s的Admission webhook[9]机制实现了sidecar的自动注入，Mesh中的每个微服务会被加入Envoy相关的容器。下面是Productpage微服务的Pod内容，可见除productpage之外，Istio还在该Pod中注入了两个容器istio-init和istio-proxy，为了节约下载镜像的时间，加快业务Pod的启动速度，这两个容器使用了相同的镜像文件，但启动命令不同。 备注：下面Pod description中只保留了需要关注的内容，删除了其它不重要的部分。为方便查看，本文中后续的其它配置文件以及命令行输出也会进行类似处理。 master $ kubectl describe pod productpage-v1-6d8bc58dd7-ts8kw Name: productpage-v1-6d8bc58dd7-ts8kw Namespace: default Labels: app=productpage version=v1 Init Containers: istio-init: Image: docker.io/istio/proxyv2:1.4.1 Command: istio-iptables -p 15001 -z 15006 -u 1337 -m REDIRECT -i * -x -b * -d 15020 Containers: productpage: Image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0 Port: 9080/TCP istio-proxy: Image: docker.io/istio/proxyv2:1.4.1 Port: 15090/TCP Args: proxy sidecar --domain $(POD_NAMESPACE).svc.cluster.local --configPath /etc/istio/proxy --binaryPath /usr/local/bin/envoy --serviceCluster productpage.$(POD_NAMESPACE) --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istio-pilot.istio-system:15010 --zipkinAddress zipkin.istio-system:9411 --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --connectTimeout 10s --proxyAdminPort 15000 --concurrency 2 --controlPlaneAuthPolicy NONE --dnsRefreshRate 300s --statusPort 15020 --applicationPorts 9080 --trust-domain=cluster.local ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:14:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Proxy_init Productpage的Pod中有一个InitContainer proxy_init，InitContrainer是K8S提供的机制，用于在Pod中执行一些初始化任务.在Initialcontainer执行完毕并退出后，才会启动Pod中的其它container。 从上面的Pod description可以看到，proxy_init容器执行的命令是istio-iptables，这是一个go编译出来的二进制文件，该二进制文件会调用iptables命令创建了一些列iptables规则来劫持Pod中的流量。该命令有这些关键的参数： 命令行参数 -p 15001表示出向流量被iptable重定向到Envoy的15001端口 命令行参数 -z 15006表示入向流量被iptable重定向到Envoy的15006端口 命令行参数 -u 1337参数用于排除用户ID为1337，即Envoy自身的流量，以避免Iptable把Envoy发出的数据又重定向到Envoy，形成死循环。 Iptables规则的详细内容参见istio源码中的shell脚本tools/packaging/common/istio-iptables.sh。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:14:1","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Proxyv2 前面提到，该容器中有两个进程Pilot-agent和envoy。我们进入容器中看看这两个进程的相关信息。 master $ kubectl exec productpage-v1-6d8bc58dd7-ts8kw -c istio-proxy -- ps -ef UID PID PPID C STIME TTY TIME CMD istio-p+ 1 0 0 10:46 ? 00:00:02 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --configPath /etc/istio/proxy --binaryPath/usr/local/bin/envoy --serviceCluster productpage.default --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istio-pilot.istio-system:15010 --zipkinAddress zipkin.istio-system:9411 --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --connectTimeout 10s --proxyAdminPort 15000 --concurrency 2 --controlPlaneAuthPolicy NONE --dnsRefreshRate 300s --statusPort 15020 --applicationPorts 9080 --trust-domain=cluster.local istio-p+ 20 1 0 10:46 ? 00:00:07 /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster productpage.default --service-node sidecar~10.40.0.18~productpage-v1-6d8bc58dd7-ts8kw.default~default.svc.cluster.local --max-obj-name-len 189 --local-address-ip-version v4 --log-format [Envoy (Epoch 0)] [%Y-%m-%d %T.%e][%t][%l][%n] %v -l warning --component-log-level misc:error --concurrency 2 istio-p+ 68 0 0 11:27 ? 00:00:00 ps -ef Envoy的大部分配置都是dynamic resource，包括网格中服务相关的service cluster, listener, route规则等。这些dynamic resource是通过xDS接口从Istio控制面中动态获取的。但Envoy如何知道xDS server的地址呢？这是在Envoy初始化配置文件中以static resource的方式配置的。 Envoy初始配置文件 Pilot-agent进程根据启动参数和K8S API Server中的配置信息生成Envoy的初始配置文件，并负责启动Envoy进程。从ps命令输出可以看到Pilot-agent在启动Envoy进程时传入了pilot地址和zipkin地址，并为Envoy生成了一个初始化配置文件envoy-rev0.json。 可以使用下面的命令将productpage pod中该文件导出来查看其中的内容： kubectl exec productpage-v1-6d8bc58dd7-ts8kw -c istio-proxy cat /etc/istio/proxy/envoy-rev0.json \u003e envoy-rev0.json 配置文件的结构如图所示： 其中各个配置节点的内容如下： Node 包含了Envoy所在节点相关信息。 { \"node\": { \"id\": \"sidecar~10.40.0.18~productpage-v1-6d8bc58dd7-ts8kw.default~default.svc.cluster.local\", \"cluster\": \"productpage.default\", \"locality\": {}, \"metadata\": { \"CLUSTER_ID\": \"Kubernetes\", \"CONFIG_NAMESPACE\": \"default\", \"EXCHANGE_KEYS\": \"NAME,NAMESPACE,INSTANCE_IPS,LABELS,OWNER,PLATFORM_METADATA,WORKLOAD_NAME,CANONICAL_TELEMETRY_SERVICE,MESH_ID,SERVICE_ACCOUNT\", \"INCLUDE_INBOUND_PORTS\": \"9080\", \"INSTANCE_IPS\": \"10.40.0.18,fe80::94df:47ff:fef3:bc99\", \"INTERCEPTION_MODE\": \"REDIRECT\", \"ISTIO_PROXY_SHA\": \"istio-proxy:3af92d895f6cb80993fc8bb04dc9b2008183f2ba\", \"ISTIO_VERSION\": \"1.4.1\", \"LABELS\": { \"app\": \"productpage\", \"pod-template-hash\": \"6d8bc58dd7\", \"security.istio.io/tlsMode\": \"istio\", \"version\": \"v1\" }, \"MESH_ID\": \"cluster.local\", \"NAME\": \"productpage-v1-6d8bc58dd7-ts8kw\", \"NAMESPACE\": \"default\", \"OWNER\": \"kubernetes://api/apps/v1/namespaces/default/deployments/productpage-v1\", \"POD_NAME\": \"productpage-v1-6d8bc58dd7-ts8kw\", \"POD_PORTS\": \"[{\\\"containerPort\\\":9080,\\\"protocol\\\":\\\"TCP\\\"},{\\\"name\\\":\\\"http-envoy-prom\\\",\\\"containerPort\\\":15090,\\\"protocol\\\":\\\"TCP\\\"}]\", \"SERVICE_ACCOUNT\": \"bookinfo-productpage\", \"WORKLOAD_NAME\": \"productpage-v1\", \"app\": \"productpage\", \"pod-template-hash\": \"6d8bc58dd7\", \"security.istio.io/tlsMode\": \"istio\", \"sidecar.istio.io/status\": \"{\\\"version\\\":\\\"8d80e9685defcc00b0d8c9274b60071ba8810537e0ed310ea96c1de0785272c7\\\",\\\"initContainers\\\":[\\\"istio-init\\\"],\\\"containers\\\":[\\\"istio-proxy\\\"],\\\"volumes\\\":[\\\"istio-envoy\\\",\\\"istio-certs\\\"],\\\"imagePullSecrets\\\":null}\", \"version\": \"v1\" } } } Admin 配置Envoy的日志路径以及管理端口。 \"admin\": { \"access_log_path\": \"/dev/null\", \"address\": { \"socket_address\": { \"address\": \"127.0.0.1\", \"port_value\": 15000 } } } Dynamic_resources 配置动态资源,这里配置了ADS服务器。 { \"dynamic_resources\": { \"lds_config\": { \"ads\": {} }, \"cds_config\": { \"ads\": {} }, \"ads_config\": { \"api_type\": \"GRPC\", \"grpc_services\": [ { \"envoy_grpc\": { \"cluster_name\": \"xds-grpc\" } } ] } } } Static_resources 配置静态资源，包括了prometheus_stats、xds-grpc和zipkin三个cluster和一个在15090上监听的listener。其中xds-grpc cluster对应前面dynamic_resources中ADS配置，指明了Envoy用于获取动态资源的服务器地址。prometheus_stats cluster和15090 listener用于对外提供兼容prometheus格式的统计指标。zipkin cluster则是外部的z","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:14:2","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Envoy配置分析 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:15:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"通过管理接口获取完整配置 从Envoy初始化配置文件中，我们可以大致看到Istio通过Envoy来实现服务发现和流量管理的基本原理。即控制面将xDS server信息通过static resource的方式配置到Envoy的初始化配置文件中，Envoy启动后通过xDS server获取到dynamic resource，包括网格中的service信息及路由规则。 Envoy配置初始化流程： Pilot-agent根据启动参数和K8S API Server中的配置信息生成Envoy的初始配置文件envoy-rev0.json，该文件告诉Envoy从xDS server中获取动态配置信息，并配置了xDS server的地址信息，即控制面的Pilot。 Pilot-agent使用envoy-rev0.json启动Envoy进程。 Envoy根据初始配置获得Pilot地址，采用xDS接口从Pilot获取到Listener，Cluster，Route等d动态配置信息。 Envoy根据获取到的动态配置启动Listener，并根据Listener的配置，结合Route和Cluster对拦截到的流量进行处理。 可以看到，Envoy中实际生效的配置是由初始化配置文件中的静态配置和从Pilot获取的动态配置一起组成的。因此只对envoy-rev0 .json进行分析并不能看到Mesh中流量管理的全貌。那么有没有办法可以看到Envoy中实际生效的完整配置呢？答案是可以的，我们可以通过Envoy的管理接口来获取Envoy的完整配置。 kubectl exec -it productpage-v1-6d8bc58dd7-ts8kw -c istio-proxy curl http://127.0.0.1:15000/config_dump \u003e config_dump 该文件内容长达近一万行，本文中就不贴出来了，在https://github.com/zhaohuabing/bookinfo-bookinfo-config-dump/blob/istio1.4.0/productpage-config-dump中可以查看到文件的全部内容。 ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:15:1","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Envoy配置文件结构 从dump文件中可以看到Envoy中包括下述配置： Bootstrap 从名字可以大致猜出这是Envoy的初始化配置，打开该节点，可以看到文件中的内容和前一章节中介绍的envoy-rev0.json是一致的，这里不再赘述。 Clusters 在Envoy中，Cluster是一个服务集群，Cluster中包含一个到多个endpoint，每个endpoint都可以提供服务，Envoy根据负载均衡算法将请求发送到这些endpoint中。 在Productpage的clusters配置中包含static_clusters和dynamic_active_clusters两部分，其中static_clusters是来自于envoy-rev0.json的初始化配置中的prometheus_stats、xDS server和zipkin server信息。dynamic_active_clusters是通过xDS接口从Istio控制面获取的动态服务信息。 Dynamic Cluster中有以下几类Cluster： Outbound Cluster 这部分的Cluster占了绝大多数，该类Cluster对应于Envoy所在节点的外部服务。以reviews为例，对于Productpage来说,reviews是一个外部服务，因此其Cluster名称中包含outbound字样。 从reviews 服务对应的cluster配置中可以看到，其类型为EDS，即表示该Cluster的endpoint来自于动态发现，动态发现中eds_config则指向了ads，最终指向static Resource中配置的xds-grpc cluster,即Pilot的地址。 { \"version_info\": \"2019-12-04T03:08:06Z/13\", \"cluster\": { \"name\": \"outbound|9080||reviews.default.svc.cluster.local\", \"type\": \"EDS\", \"eds_cluster_config\": { \"eds_config\": { \"ads\": {} }, \"service_name\": \"outbound|9080||reviews.default.svc.cluster.local\" }, \"connect_timeout\": \"1s\", \"circuit_breakers\": { \"thresholds\": [ { \"max_connections\": 4294967295, \"max_pending_requests\": 4294967295, \"max_requests\": 4294967295, \"max_retries\": 4294967295 } ] } }, \"last_updated\": \"2019-12-04T03:08:22.658Z\" } 可以通过Pilot的调试接口获取该Cluster的endpoint： curl http://10.97.222.108:15014/debug/edsz \u003e pilot_eds_dump 导出的文件较长，本文只贴出reviews服务相关的endpoint配置，完整文件参见:https://github.com/zhaohuabing/bookinfo-bookinfo-config-dump/blob/istio1.4.0/pilot_eds_dump 从下面的文件内容可以看到，reviews cluster配置了3个endpoint地址，是reviews的pod ip。 { \"clusterName\": \"outbound|9080||reviews.default.svc.cluster.local\", \"endpoints\": [ { \"lbEndpoints\": [ { \"endpoint\": { \"address\": { \"socketAddress\": { \"address\": \"10.40.0.15\", \"portValue\": 9080 } } }, \"metadata\": { \"filterMetadata\": { \"envoy.transport_socket_match\": { \"tlsMode\": \"istio\" }, \"istio\": { \"uid\": \"kubernetes://reviews-v1-75b979578c-pw8zs.default\" } } }, \"loadBalancingWeight\": 1 }, { \"endpoint\": { \"address\": { \"socketAddress\": { \"address\": \"10.40.0.16\", \"portValue\": 9080 } } }, \"metadata\": { \"filterMetadata\": { \"envoy.transport_socket_match\": { \"tlsMode\": \"istio\" }, \"istio\": { \"uid\": \"kubernetes://reviews-v3-54c6c64795-wbls7.default\" } } }, \"loadBalancingWeight\": 1 }, { \"endpoint\": { \"address\": { \"socketAddress\": { \"address\": \"10.40.0.17\", \"portValue\": 9080 } } }, \"metadata\": { \"filterMetadata\": { \"envoy.transport_socket_match\": { \"tlsMode\": \"istio\" }, \"istio\": { \"uid\": \"kubernetes://reviews-v2-597bf96c8f-l2fp8.default\" } } }, \"loadBalancingWeight\": 1 } ], \"loadBalancingWeight\": 3 } ] } Inbound Cluster 该类Cluster对应于Envoy所在节点上的服务。如果该服务接收到请求，当然就是一个入站请求。对于Productpage Pod上的Envoy，其对应的Inbound Cluster只有一个，即productpage。该cluster对应的host为127.0.0.1,即环回地址上productpage的监听端口。由于iptable规则中排除了127.0.0.1,入站请求通过该Inbound cluster处理后将跳过Envoy，直接发送给Productpage进程处理。 { \"version_info\": \"2019-12-04T03:08:06Z/13\", \"cluster\": { \"name\": \"inbound|9080|http|productpage.default.svc.cluster.local\", \"type\": \"STATIC\", \"connect_timeout\": \"1s\", \"circuit_breakers\": { \"thresholds\": [ { \"max_connections\": 4294967295, \"max_pending_requests\": 4294967295, \"max_requests\": 4294967295, \"max_retries\": 4294967295 } ] }, \"load_assignment\": { \"cluster_name\": \"inbound|9080|http|productpage.default.svc.cluster.local\", \"endpoints\": [ { \"lb_endpoints\": [ { \"endpoint\": { \"address\": { \"socket_address\": { \"address\": \"127.0.0.1\", \"port_value\": 9080 } } } } ] } ] } }, \"last_updated\": \"2019-12-04T03:08:22.658Z\" } BlackHoleCluster 这是一个特殊的Cluster，并没有配置后端处理请求的Host。如其名字所暗示的一样，请求进入后将被直接丢弃掉。如果一个请求没有找到其对的目的服务，则被发到cluste。 { \"version_info\": \"2019-12-04T03:08:06Z/13\", \"cluster\": { \"name\": \"BlackHoleCluster\", \"type\": \"STATIC\", \"connect_timeout\": \"1s\" }, \"last_updated\": \"2019-12-04T03:08:22.658Z\" } PassthroughCluster 和BlackHoleCluter相反，发向PassthroughCluster的请求会被直接发送到其请求中要求的原始目地的，Envoy不会对请求进行重新路由。 { \"version_info\": \"2019-12-04T03:08:06Z/13\", \"cluster\": { \"name\": \"PassthroughCluster\", \"type\": \"ORIGINAL_DST\", \"connect_timeout\": \"1s\", \"lb_policy\": \"CLUSTER_PROVIDED\", \"circuit_breakers\": { \"thresholds\": ","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:15:2","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":["istio"],"content":"Bookinfo端到端调用分析 通过前面章节对Envoy配置文件的分析，我们了解到Istio控制面如何将服务和路由信息通过xDS接口下发到数据面中；并介绍了Envoy上生成的各种配置数据的结构，包括listener,cluster,route和endpoint。 下面我们来分析一个端到端的调用请求，通过调用请求的流程把这些配置串连起来，以从全局上理解Istio控制面的流量控制能力是如何在数据面的Envoy上实现的。 下图描述了一个Productpage服务调用Reviews服务的请求流程： Virtual Inbound Listener Productpage发起对Reviews服务的调用：http://reviews:9080/reviews/0 。 请求被Productpage Pod的iptable规则拦截，重定向到本地的15001端口。 在15001端口上监听的Envoy Virtual Outbound Listener收到了该请求。 请求被Virtual Outbound Listener根据原目标IP（通配）和端口（9080）转发到0.0.0.0_9080这个 outbound listener。 { \"version_info\": \"2019-12-04T03:08:06Z/13\", \"listener\": { \"name\": \"virtualOutbound\", \"address\": { \"socket_address\": { \"address\": \"0.0.0.0\", \"port_value\": 15001 } }, ...... \"use_original_dst\": true //请求转发给和原始目的IP:Port匹配的listener }, \"last_updated\": \"2019-12-04T03:08:22.919Z\" } 根据0.0.0.0_9080 listener的http_connection_manager filter配置,该请求采用“9080” route进行分发。 { \"version_info\": \"2019-12-04T03:08:06Z/13\", \"listener\": { \"name\": \"0.0.0.0_9080\", \"address\": { \"socket_address\": { \"address\": \"0.0.0.0\", \"port_value\": 9080 } }, \"filter_chains\": [ { \"filters\": [ { \"name\": \"envoy.http_connection_manager\", \"typed_config\": { \"@type\": \"type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\", \"stat_prefix\": \"outbound_0.0.0.0_9080\", \"http_filters\": [ { \"name\": \"mixer\", ...... }, { \"name\": \"envoy.cors\" }, { \"name\": \"envoy.fault\" }, { \"name\": \"envoy.router\" } ], ...... \"rds\": { \"config_source\": { \"ads\": {} }, \"route_config_name\": \"9080\" //采用“9080” route进行分发 } } } ] } ], \"deprecated_v1\": { \"bind_to_port\": false }, \"listener_filters_timeout\": \"0.100s\", \"traffic_direction\": \"OUTBOUND\", \"continue_on_listener_filters_timeout\": true }, \"last_updated\": \"2019-12-04T03:08:22.822Z\" } “9080”这个route的配置中，host name为reviews:9080的请求对应的cluster为outbound|9080||reviews.default.svc.cluster.local { \"name\": \"reviews.default.svc.cluster.local:9080\", \"domains\": [ \"reviews.default.svc.cluster.local\", \"reviews.default.svc.cluster.local:9080\", \"reviews\", \"reviews:9080\", \"reviews.default.svc.cluster\", \"reviews.default.svc.cluster:9080\", \"reviews.default.svc\", \"reviews.default.svc:9080\", \"reviews.default\", \"reviews.default:9080\", \"10.102.108.56\", \"10.102.108.56:9080\" ], \"routes\": [ { \"match\": { \"prefix\": \"/\" }, \"route\": { \"cluster\": \"outbound|9080||reviews.default.svc.cluster.local\", \"timeout\": \"0s\", \"retry_policy\": { \"retry_on\": \"connect-failure,refused-stream,unavailable,cancelled,resource-exhausted,retriable-status-codes\", \"num_retries\": 2, \"retry_host_predicate\": [ { \"name\": \"envoy.retry_host_predicates.previous_hosts\" } ], \"host_selection_retry_max_attempts\": \"5\", \"retriable_status_codes\": [ 503 ] }, \"max_grpc_timeout\": \"0s\" }, \"decorator\": { \"operation\": \"reviews.default.svc.cluster.local:9080/*\" }, \"typed_per_filter_config\": { \"mixer\": { \"@type\": \"type.googleapis.com/istio.mixer.v1.config.client.ServiceConfig\", \"disable_check_calls\": true, \"mixer_attributes\": { \"attributes\": { \"destination.service.host\": { \"string_value\": \"reviews.default.svc.cluster.local\" }, \"destination.service.name\": { \"string_value\": \"reviews\" }, \"destination.service.namespace\": { \"string_value\": \"default\" }, \"destination.service.uid\": { \"string_value\": \"istio://default/services/reviews\" } } }, \"forward_attributes\": { \"attributes\": { \"destination.service.host\": { \"string_value\": \"reviews.default.svc.cluster.local\" }, \"destination.service.name\": { \"string_value\": \"reviews\" }, \"destination.service.namespace\": { \"string_value\": \"default\" }, \"destination.service.uid\": { \"string_value\": \"istio://default/services/reviews\" } } } } }, \"name\": \"default\" } ] } outbound|9080||reviews.default.svc.cluster.local cluster为动态资源，通过eds查询得到该cluster中有3个endpoint。 { \"clusterName\": \"outbound|9080||reviews.default.svc.cluster.local\", \"endpoints\": [ { \"lbEndpoints\": [ { \"endpoint\": { \"address\": { \"socketAddress\": { \"address\": \"10.40.0.15\", \"portValue\": 9080 } } }, \"metadata\": { \"filterMetadata\": { \"envoy.transport_socket_match\": { \"tlsMode\": \"istio\" }, \"istio\": { \"uid\": \"kubernete","date":"212111-08-2151","objectID":"/zhaohuabin_traffic/:16:0","tags":["istio","envoy"],"title":"istio - 流量走向详解","uri":"/zhaohuabin_traffic/"},{"categories":null,"content":"介绍¶ Envoy 是一个面向服务架构的 L7 代理和通信总线而设计的，这个项目诞生是出于以下目标： 对于应用程序而言，网络应该是透明的，当发生网络和应用程序故障时，能够很容易定位出问题的根源。 Envoy 的核心功能： 非侵入的架构：Envoy 是和应用服务并行运行的，透明地代理应用服务发出/接收的流量。应用服务只需要和 Envoy 通信，无需知道其他微服务应用在哪里。 基于 Modern C++11 实现，性能优异。 L3/L4 过滤器架构：Envoy 的核心是一个 L3/L4 代理，然后通过插件式的过滤器(network filters)链条来执行 TCP/UDP 的相关任务，例如 TCP 转发，TLS 认证等工作。 HTTP L7 过滤器架构：HTTP 在现代应用体系里是地位非常特殊的应用层协议，所以 Envoy 内置了一个非常核心的过滤器: http_connection_manager。http_connection_manager 本身是如此特殊和复杂，支持丰富的配置，以及本身也是过滤器架构，可以通过一系列 http 过滤器来实现 http 协议层面的任务，例如：http 路由，重定向，CORS 支持等等。 HTTP/2 作为第一公民：Envoy 支持 HTTP/1.1 和 HTTP/2，推荐使用 HTTP/2。 gRPC 支持：因为对 HTTP/2 的良好支持，Envoy 可以方便的支持 gRPC，特别是在负载和代理上。 服务发现： 支持包括 DNS, EDS 在内的多种服务发现方案。 健康检查：内置健康检查子系统。 高级的负载均衡方案：除了一般的负载均衡，Envoy 还支持基于 rate limit 服务的多种高级负载均衡方案，包括： automatic retries、circuit breaking、global rate limiting 等。 Tracing：方便集成 Open Tracing 系统，追踪请求 统计与监控：内置 stats 模块，方便集成诸如 prometheus/statsd 等监控方案 动态配置：通过“动态配置API”实现配置的动态调整，而无需重启 Envoy 服务的。 Envoy 入门¶ Envoy 是一个开源的边缘服务代理，也是 Istio Service Mesh 默认的数据平面，专为云原生应用程序设计。 下面我们通过一个简单的示例来介绍 Envoy 的基本使用。 ","date":"5059-08-529","objectID":"/envoy_1/:0:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"1. 配置¶ ","date":"5059-08-529","objectID":"/envoy_1/:1:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"创建代理配置¶ Envoy 使用 YAML 配置文件来控制代理的行为。在下面的步骤中，我们将使用静态配置接口来构建配置，也意味着所有设置都是预定义在配置文件中的。此外 Envoy 也支持动态配置，这样可以通过外部一些源来自动发现进行设置。 ","date":"5059-08-529","objectID":"/envoy_1/:1:1","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"资源¶ Envoy 配置的第一行定义了正在使用的接口配置，在这里我们将配置静态 API，因此第一行应为 static_resources： static_resources: ","date":"5059-08-529","objectID":"/envoy_1/:1:2","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"监听器¶ 在配置的开始定义了监听器（Listeners）。监听器是 Envoy 监听请求的网络配置，例如 IP 地址和端口。我们这里的 Envoy 在 Docker 容器内运行，因此它需要监听 IP 地址 0.0.0.0，在这种情况下，Envoy 将在端口 10000 上进行监听。 下面是定义监听器的配置： static_resources:listeners:- name:listener_0address:socket_address:{address: 0.0.0.0, port_value:10000} ","date":"5059-08-529","objectID":"/envoy_1/:1:3","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"过滤器¶ 通过 Envoy 监听传入的流量，下一步是定义如何处理这些请求。每个监听器都有一组过滤器，并且不同的监听器可以具有一组不同的过滤器。 在我们这个示例中，我们将所有流量代理到 baidu.com，配置完成后我们应该能够通过请求 Envoy 的端点就可以直接看到百度的主页了，而无需更改 URL 地址。 过滤器是通过 filter_chains 来定义的，每个过滤器的目的是找到传入请求的匹配项，以使其与目标地址进行匹配： static_resources: listeners: - name: listener_0 address: socket_address: { address: 0.0.0.0, port_value: 10000 } filter_chains: - filters: - name: envoy.http_connection_manager config: stat_prefix: ingress_http route_config: name: local_route virtual_hosts: - name: local_service domains: [\"*\"] # 匹配的主机域名，满足才会路由转发 routes: # 路由转发 - match: { prefix: \"/\" } # 请求匹配前缀 route: { host_rewrite: www.baidu.com, cluster: service_baidu } # 更改requeset header 中的host，以及处理请求的cluster http_filters: - name: envoy.router 该过滤器使用了 envoy.http_connection_manager，这是为 HTTP 连接设计的一个内置过滤器: stat_prefix：为连接管理器发出统计信息时使用的一个前缀。 route_config：路由配置，如果虚拟主机 virtual_hosts 匹配上了则检查路由。在我们这里的配置中，无论请求的主机域名是什么，route_config 都匹配所有传入的 HTTP 请求，因为domains 是 *，所以匹配所有的请求。 routes：如果 URL 前缀匹配，则一组路由规则定义了下一步将发生的状况。/ 表示匹配根路由。 host_rewrite：更改 HTTP 请求的入站 Host 头信息。 cluster: 将要处理请求的集群名称，下面会有相应的实现。 http_filters: 该过滤器允许 Envoy 在处理请求时去适应和修改请求。 ","date":"5059-08-529","objectID":"/envoy_1/:1:4","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"集群¶ 当请求于过滤器匹配时，该请求将会传递到集群cluster。下面的配置就是将主机定义为访问 HTTPS 的 baidu.com 域名，如果定义了多个主机，则 Envoy 将执行轮询（Round Robin）策略。配置如下所示： static_resources:listeners:- name:listener_0address:socket_address:{address: 0.0.0.0, port_value:10000}filter_chains:- filters:- name:envoy.http_connection_managerconfig:stat_prefix:ingress_httproute_config:name:local_routevirtual_hosts:- name:local_servicedomains:[\"*\"]routes:- match:{prefix:\"/\"}route:{host_rewrite: www.baidu.com, cluster:service_baidu }http_filters:- name:envoy.routerclusters:# 集群定义- name:service_baiduconnect_timeout:0.25s # 连接超时type:LOGICAL_DNSdns_lookup_family:V4_ONLYlb_policy:ROUND_ROBINhosts:[{socket_address:{address: www.baidu.com, port_value:443}}]tls_context:{sni:baidu.com } ","date":"5059-08-529","objectID":"/envoy_1/:1:5","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"管理¶ 最后，还需要配置一个管理模块： admin:access_log_path:/tmp/admin_access.logaddress:socket_address:{address: 0.0.0.0, port_value:9901} 上面的配置定义了 Envoy 的静态配置模板，监听器定义了 Envoy 的端口和 IP 地址，监听器具有一组过滤器来匹配传入的请求，匹配请求后，将请求转发到集群。 ","date":"5059-08-529","objectID":"/envoy_1/:1:6","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"2. 开启代理¶ 配置完成后，可以通过 Docker 容器来启动 Envoy，将上面的配置文件通过 Volume 挂载到容器中的 /etc/envoy/envoy.yaml 文件。 然后使用以下命令启动绑定到端口 80 的 Envoy 容器： $ docker run --name=envoy -d \\ -p 80:10000 \\ -v $(pwd)/manifests/envoy.yaml:/etc/envoy/envoy.yaml \\ envoyproxy/envoy:latest 启动后，我们可以在本地的 80 端口上去访问应用 curl localhost 来测试代理是否成功。同样我们也可以通过在本地浏览器中访问 localhost 来查看： 可以看到请求被代理到了 baidu.com，而且应该也可以看到 URL 地址没有变化，还是 localhost。 ","date":"5059-08-529","objectID":"/envoy_1/:2:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"3. 管理视图¶ Envoy 提供了一个管理视图，可以让我们去查看配置、统计信息、日志以及其他 Envoy 内部的一些数据。 我们可以通过添加其他的资源定义来配置 admin，其中也可以定义管理视图的端口，不过需要注意该端口不要和其他监听器配置冲突。 admin:access_log_path:/tmp/admin_access.logaddress:socket_address:{address: 0.0.0.0, port_value:9901} 当然我们也可以通过 Docker 容器将管理端口暴露给外部用户。上面的配置就会将管理页面暴露给外部用户，当然我们这里仅仅用于演示是可以的，如果你是用于线上环境还需要做好一些安全保护措施，可以查看 Envoy 的相关文档 了解更多安全配置。 要将管理页面也暴露给外部用户，我们使用如下命令运行另外一个容器： $ docker run --name=envoy-with-admin -d \\ -p 9901:9901 \\ -p 10000:10000 \\ -v $(pwd)/manifests/envoy.yaml:/etc/envoy/envoy.yaml \\ envoyproxy/envoy:latest 运行成功后，现在我们可以在浏览器里面输入 localhost:9901 来访问 Envoy 的管理页面： 迁移 NGINX 到 Envoy¶ 大部分的应用可能还是使用的比较传统的 Nginx 来做服务代理，本文我们将介绍如何将 Nginx 的配置迁移到 Envoy 上来。我们将学到： 如何设置 Envoy 代理配置 配置 Envoy 代理转发请求到外部服务 配置访问和错误日志 最后我们还会了解到 Envoy 代理的核心功能，以及如何将现有的 Nginx 配置迁移到 Envoy 上来。 ","date":"5059-08-529","objectID":"/envoy_1/:3:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"1. Nginx 示例¶ 这里我们使用 Nginx 官方 Wiki 的完整示例来进行说明，完整的 nginx.conf 配置如下所示： user www www; pid /var/run/nginx.pid; worker_processes 2; events { worker_connections 2000; } http { gzip on; gzip_min_length 1100; gzip_buffers 4 8k; gzip_types text/plain; log_format main '$remote_addr - $remote_user [$time_local] ' '\"$request\" $status $bytes_sent ' '\"$http_referer\" \"$http_user_agent\" ' '\"$gzip_ratio\"'; log_format download '$remote_addr - $remote_user [$time_local] ' '\"$request\" $status $bytes_sent ' '\"$http_referer\" \"$http_user_agent\" ' '\"$http_range\" \"$sent_http_content_range\"'; upstream targetCluster { 172.17.0.3:80; 172.17.0.4:80; } server { listen 8080; server_name one.example.com www.one.example.com; access_log /var/log/nginx.access_log main; error_log /var/log/nginx.error_log info; location / { proxy_pass http://targetCluster/; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; } } } 上面的 Nginx 配置有3个核心配置： 配置 Nginx 服务、日志结构和 Gzip 功能 配置 Nginx 在端口 8080 上接受对 one.example.com 域名的请求 根据不同的路径配置将流量转发给目标服务 并不是所有的 Nginx 的配置都适用于 Envoy，有些方面的配置我们可以不用关系。Envoy 代理主要有4种主要的配置类型，它们是支持 Nginx 提供的核心基础结构的： Listeners（监听器）：他们定义 Envoy 代理如何接收传入的网络请求，建立连接后，它会传递到一组过滤器进行处理 Filters（过滤器）：过滤器是处理传入和传出请求的管道结构的一部分，比如可以开启类似于 Gzip 之类的过滤器，该过滤器就会在将数据发送到客户端之前进行压缩 Routers（路由器）：这些路由器负责将流量转发到定义的目的集群去 Clusters（集群）：集群定义了流量的目标端点和相关配置。 我们将使用这4个组件来创建 Envoy 代理配置，去匹配 Nginx 中的配置。Envoy 的重点一直是在 API 和动态配置上，但是我们这里需要使用静态配置。 ","date":"5059-08-529","objectID":"/envoy_1/:4:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"2. Nginx 配置¶ 现在我们来仔细看下上面的 nginx.conf 配置文件的内容。 ","date":"5059-08-529","objectID":"/envoy_1/:5:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"工作连接¶ 首先是 Worker 连接数配置，主要是用于定义工作进程和连接的数量，用来配置 Nginx 扩展请求： worker_processes 2; events { worker_connections 2000; } Envoy 代理用另外一种不同的方式来管理工作进程和连接。Envoy 使用单进程-多线程的架构模型。一个 master 线程管理各种琐碎的任务，而一些 worker 线程则负责执行监听、过滤和转发。当监听器接收到一个连接请求时，该连接将其生命周期绑定到一个单独的 worker 线程。这使得 Envoy 主要使用大量单线程处理工作，并且只有少量的复杂代码用于实现 worker 线程之间的协调工作。通常情况下，Envoy 实现了100%的非阻塞。对于大多数工作负载，我们建议将 worker 线程数配置为物理机器的线程数。关于 Envoy 线程模型的更多信息，可以查看 Envoy 官方博客介绍：Envoy threading model ","date":"5059-08-529","objectID":"/envoy_1/:5:1","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"HTTP 配置¶ 然后 Nginx 配置的下一个部分是定义 HTTP 配置，比如： 定义支持哪些 MIME 类型 默认的超时时间 Gzip 配置 我们可以通过 Envoy 代理中的过滤器来配置这些内容。在 HTTP 配置部分，Nginx 配置指定了监听的端口 8080，并响应域名 one.example.com 和 www.one.example.com 的传入请求： server { listen 8080; server_name one.example.com www.one.example.com; ...... } 在 Envoy 中，这部分就是监听器来管理的。开始一个 Envoy 代理最重要的方面就是定义监听器，我们需要创建一个配置文件来描述我们如何去运行 Envoy 实例。 下面的配置将创建一个新的监听器并将其绑定到 8080 端口上，该配置指示了 Envoy 代理用于接收网络请求的端口。Envoy 配置使用的是 YAML 文件，如果你对 YAML 文件格式语法不是很熟悉的，可以点此先查看官方对应的介绍。 static_resources:listeners:- name:listener_0address:socket_address:{address: 0.0.0.0, port_value:8080} 需要注意的是我们没有在监听器部分定义 server_name，我们会在过滤器部分进行处理。 ","date":"5059-08-529","objectID":"/envoy_1/:5:2","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"Location 配置¶ 当请求进入 Nginx 时，location 部分定义了如何处理流量以及在什么地方转发流量。在下面的配置中，站点的所有传入流量都将被代理到一个名为 targetCluster 的上游（upstream）集群，上游集群定义了处理请求的节点。 location / { proxy_pass http://targetCluster/; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; } 在 Envoy 中，这部分将由过滤器来进行配置管理。在静态配置中，过滤器定义了如何处理传入的请求，在我们这里，将配置一个过滤器去匹配上一步中的 server_names，当接收到与定义的域名和路由匹配的传入请求时，流量将转发到集群，集群和 Nginx 配置中的 upstream 是一致的。 filter_chains:- filters:- name:envoy.http_connection_managerconfig:codec_type:autostat_prefix:ingress_httproute_config:name:local_routevirtual_hosts:- name:backenddomains:- \"one.example.com\"- \"www.one.example.com\"routes:- match:prefix:\"/\"route:cluster:targetClusterhttp_filters:- name:envoy.router 其中 envoy.http_connection_manager 是 Envoy 内置的一个过滤器，用于处理 HTTP 连接的，除此之外，还有其他的一些内置的过滤器，比如 Redis、Mongo、TCP。 ","date":"5059-08-529","objectID":"/envoy_1/:5:3","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"upstream 配置¶ 在 Nginx 中，upstream（上游）配置定义了处理请求的目标服务器集群，在我们这里的示例中，分配了两个集群。 upstream targetCluster { 172.17.0.3:80; 172.17.0.4:80; } 在 Envoy 代理中，这部分是通过集群进行配置管理的。upstream 等同与 Envoy 中的 clusters 定义，我们这里通过集群定义了主机被访问的方式，还可以配置超时和负载均衡等方面更精细的控制。 clusters:- name:targetClusterconnect_timeout:0.25s # 负载均衡type:STRICT_DNSdns_lookup_family:V4_ONLYlb_policy:ROUND_ROBIN # 负载均衡hosts:[{socket_address:{address: 172.17.0.3, port_value:80}},{socket_address:{address: 172.17.0.4, port_value:80}}] 上面我们配置了 STRICT_DNS 类型的服务发现，Envoy 会持续异步地解析指定的 DNS 目标。DNS 解析结果返回的每个 IP 地址都将被视为上游集群的主机。所以如果产线返回两个 IP 地址，则 Envoy 将认为集群由两个主机，并且两个主机都应进行负载均衡，如果从结果中删除了一个主机，则 Envoy 会从现有的连接池中将其剔出掉。 ","date":"5059-08-529","objectID":"/envoy_1/:5:4","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"日志配置¶ 最后需要配置的日志部分，Envoy 采用云原生的方式，将应用程序日志都输出到 stdout 和 stderr，而不是将错误日志输出到磁盘。 当用户发起一个请求时，访问日志默认是被禁用的，我们可以手动开启。要为 HTTP 请求开启访问日志，需要在 HTTP 连接管理器中包含一个 access_log 的配置，该路径可以是设备，比如 stdout，也可以是磁盘上的某个文件，这依赖于我们自己的实际情况。 下面过滤器中的配置就会将所有访问日志通过管理传输到 stdout： - name:envoy.http_connection_managerconfig:codec_type:autostat_prefix:ingress_httpaccess_log:- name:envoy.file_access_logconfig:path:\"/dev/stdout\"route_config:...... 默认情况下，Envoy 访问日志格式包含整个 HTTP 请求的详细信息： [%START_TIME%] \"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\" %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \"%REQ(X-FORWARDED-FOR)%\" \"%REQ(USER-AGENT)%\" \"%REQ(X-REQUEST-ID)%\" \"%REQ(:AUTHORITY)%\" \"%UPSTREAM_HOST%\"\\n 输出结果格式化后如下所示： [2020-04-08T04:51:00.281Z] \"GET / HTTP/1.1\" 200 - 0 58 4 1 \"-\" \"curl/7.47.0\" \"f21ebd42-6770-4aa5-88d4-e56118165a7d\" \"one.example.com\" \"172.18.0.4:80\" 我们也可以通过设置 format 字段来自定义输出日志的格式，例如： access_log:- name:envoy.file_access_logconfig:path:\"/dev/stdout\"format:\"[%START_TIME%] \"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\" %RESPONSE_CODE% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \"%REQ(X-REQUEST-ID)%\" \"%REQ(:AUTHORITY)%\" \"%UPSTREAM_HOST%\"\\n\" 此外我们也可以通过设置 json_format 字段来将日志作为 JSON 格式输出，例如： access_log:- name:envoy.file_access_logconfig:path:\"/dev/stdout\"json_format:{\"protocol\": \"%PROTOCOL%\", \"duration\": \"%DURATION%\", \"request_method\": \"%REQ(:METHOD)%\"} 要注意的是，访问日志会在未设置、或者空值的位置加入一个字符：-。不同类型的访问日志（例如 HTTP 和 TCP）共用同样的格式字符串。不同类型的日志中，某些字段可能会有不同的含义。有关 Envoy 日志的更多信息，可以查看官方文档对应的说明。当然日志并不是 Envoy 代理获得请求可见性的唯一方法，Envoy 还内置了高级跟踪和指标功能，我们会在后续章节中慢慢接触到。 ","date":"5059-08-529","objectID":"/envoy_1/:5:5","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"3. 测试¶ 现在我们已经将 Nginx 配置转换为了 Envoy 代理，接下来我们可以来启动 Envoy 代理进行测试验证。 在 Nginx 配置的顶部，有一行配置 user www www;，表示用非 root 用户来运行 Nginx 以提高安全性。而 Envoy 代理采用云原生的方法来管理使用这，我们通过容器启动 Envoy 代理的时候，可以指定一个低特权的用户。 下面的命令将通过 Docker 容器来启动一个 Envoy 实例，该命令使 Envoy 可以监听 80 端口上的流量请求，但是我们在 Envoy 的监听器配置中指定的是 8080 端口，所以我们用一个低特权用户身份来运行： $ docker run --name proxy1 -p 80:8080 --user 1000:1000 -v $(pwd)/manifests/envoy.yaml:/etc/envoy/envoy.yaml envoyproxy/envoy 启动代理后，就可以开始测试了，下面我们用 curl 命令使用代理配置的 host 头发起一个网络请求： $ curl -H \"Host: one.example.com\" localhost -i HTTP/1.1 503 Service Unavailable content-length: 91 content-type: text/plain date: Wed, 08 Apr 2020 04:25:59 GMT server: envoy upstream connect error or disconnect/reset before headers. reset reason: connection failure% 我们可以看到会出现 503 错误，这是因为我们配置的上游集群主机根本就没有运行，所以 Envoy 代理请求到不可用的主机上去了，就出现了这样的错误。我们可以使用下面的命令启动两个 HTTP 服务，用来表示上游主机： $ docker run -d cnych/docker-http-server; docker run -d cnych/docker-http-server; $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES fd3018535a52 cnych/docker-http-server \"/app\" 2 minutes ago Up 2 minutes 80/tcp practical_golick ace25541d654 cnych/docker-http-server \"/app\" 2 minutes ago Up 2 minutes 80/tcp lucid_hugle 3c83dfb9392f envoyproxy/envoy \"/docker-entrypoint.…\" 2 minutes ago Up 2 minutes 10000/tcp, 0.0.0.0:80-\u003e8080/tcp proxy1 当上面两个服务启动成功后，现在我们再通过 Envoy 去访问目标服务就正常了： $ curl -H \"Host: one.example.com\" localhost -i HTTP/1.1 200 OK date: Wed, 08 Apr 2020 04:32:01 GMT content-length: 58 content-type: text/html; charset=utf-8 x-envoy-upstream-service-time: 3 server: envoy \u003ch1\u003eThis request was processed by host: fd3018535a52\u003c/h1\u003e $ curl -H \"Host: one.example.com\" localhost -i HTTP/1.1 200 OK date: Wed, 08 Apr 2020 04:32:05 GMT content-length: 58 content-type: text/html; charset=utf-8 x-envoy-upstream-service-time: 0 server: envoy \u003ch1\u003eThis request was processed by host: ace25541d654\u003c/h1\u003e 当访问请求的时候，我们可以看到是哪个容器处理了请求，在 Envoy 代理容器中，也可以看到请求的日志输出： [2020-04-08T04:32:06.201Z] \"GET / HTTP/1.1\" 200 - 0 58 1 0 \"-\" \"curl/7.54.0\" \"ac61099b-f100-46a9-9c08-c323c5ac2320\" \"one.example.com\" \"172.17.0.3:80\" [2020-04-08T04:32:08.168Z] \"GET / HTTP/1.1\" 200 - 0 58 0 0 \"-\" \"curl/7.54.0\" \"15ee6ca9-b161-4630-a51c-c641d0760cd0\" \"one.example.com\" \"172.17.0.4:80\" 最后转换过后的完整的 Envoy 配置如下： static_resources:listeners:- name:listener_0address:socket_address:{address: 0.0.0.0, port_value:8080}filter_chains:- filters:- name:envoy.http_connection_managerconfig:codec_type:autostat_prefix:ingress_httpaccess_log:- name:envoy.file_access_logconfig:path:\"/dev/stdout\"route_config:name:local_routevirtual_hosts:- name:backenddomains:- \"one.example.com\"- \"www.one.example.com\"routes:- match:prefix:\"/\"route:cluster:targetClusterhttp_filters:- name:envoy.routerclusters:- name:targetClusterconnect_timeout:0.25stype:STRICT_DNSdns_lookup_family:V4_ONLYlb_policy:ROUND_ROBINhosts:[{socket_address:{address: 172.17.0.3, port_value:80}},{socket_address:{address: 172.17.0.4, port_value:80}}] 使用 SSL/TLS 保护流量¶ 本节我们将演示如何使用 Envoy 保护 HTTP 网络请求。确保 HTTP 流量安全对于保护用户隐私和数据是至关重要的。下面我们来了解下如何在 Envoy 中配置 SSL 证书。 ","date":"5059-08-529","objectID":"/envoy_1/:6:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"1. SSL 证书¶ 这里我们将为 example.com 域名生成一个自签名的证书，当然如果在生产环境时候，需要使用正规 CA 机构购买的证书，或者 Let's Encrypt 的免费证书服务。 下面的命令会在目录 certs/ 中创建一个新的证书和密钥： $ mkdir certs; cd certs; $ openssl req -nodes -new -x509 \\ -keyout example-com.key -out example-com.crt \\ -days 365 \\ -subj '/CN=example.com/O=youdianzhishi./C=CN'; Generating a RSA private key ..+++++ .................................................................................................................+++++ writing new private key to 'example-com.key' ----- $ cd - ","date":"5059-08-529","objectID":"/envoy_1/:7:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"2. 流量保护¶ 在 Envoy 中保护 HTTP 流量，需要通过添加 tls_context 过滤器，TLS 上下文提供了为 Envoy 代理中配置的域名指定证书的功能，请求 HTTPS 请求时候，就使用匹配的证书。我们这里直接使用上一步中生成的自签名证书即可。 我们这里的 Envoy 配置文件中包含了所需的 HTTPS 支持的配置，我们添加了两个监听器，一个监听器在 8080 端口上用于 HTTP 通信，另外一个监听器在 8443 端口上用于 HTTPS 通信。 在 HTTPS 监听器中定义了 HTTP 连接管理器，该代理将代理 /service/1 和 /service/2 这两个端点的传入请求，这里我们需要通过 tls_context 配置相关证书，如下所示： tls_context:common_tls_context:tls_certificates:- certificate_chain:filename:\"/etc/envoy/certs/example-com.crt\"private_key:filename:\"/etc/envoy/certs/example-com.key\" 在 TLS 上下文中定义了生成的证书和密钥，如果我们有多个域名，每个域名都有自己的证书，则需要通过 tls_certificates 定义多个证书链。 ","date":"5059-08-529","objectID":"/envoy_1/:8:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"3. 自动跳转¶ 定义了 TLS 上下文后，该站点将能够通过 HTTPS 提供流量了，但是如果用户是通过 HTTP 来访问的服务，为了确保安全，我们也可以将其重定向到 HTTPS 版本服务上去。 在 HTTP 配置中，我们将 https_redirect: true 的标志添加到过滤器的配置中即可实现跳转功能。 route_config:virtual_hosts:- name:backenddomains:- \"example.com\"routes:- match:prefix:\"/\"redirect:path_redirect:\"/\"https_redirect:true# 实现https跳转 当用户访问网站的 HTTP 版本时，Envoy 代理将根据过滤器配置来匹配域名和路径，匹配到过后将请求重定向到站点的 HTTPS 版本去。完整的 Envoy 配置如下所示： static_resources:listeners:- name:listener_httpaddress:socket_address:{address: 0.0.0.0, port_value:8080}filter_chains:- filters:- name:envoy.http_connection_managerconfig:codec_type:autostat_prefix:ingress_httproute_config:virtual_hosts:- name:backenddomains:- \"example.com\"routes:- match:prefix:\"/\"redirect:path_redirect:\"/\"https_redirect:truehttp_filters:- name:envoy.routerconfig:{}- name:listener_httpsaddress:socket_address:{address: 0.0.0.0, port_value:8443}filter_chains:- filters:- name:envoy.http_connection_managerconfig:codec_type:autostat_prefix:ingress_httproute_config:name:local_routevirtual_hosts:- name:backenddomains:- \"example.com\"routes:- match:prefix:\"/service/1\"route:cluster:service1- match:prefix:\"/service/2\"route:cluster:service2http_filters:- name:envoy.routerconfig:{}tls_context:common_tls_context:tls_certificates:- certificate_chain:filename:\"/etc/envoy/certs/example-com.crt\"private_key:filename:\"/etc/envoy/certs/example-com.key\"clusters:- name:service1connect_timeout:0.25stype:strict_dnslb_policy:round_robinhosts:- socket_address:address:172.17.0.3port_value:80- name:service2connect_timeout:0.25stype:strict_dnslb_policy:round_robinhosts:- socket_address:address:172.17.0.4port_value:80admin:access_log_path:/tmp/admin_access.logaddress:socket_address:address:0.0.0.0port_value:8001 ","date":"5059-08-529","objectID":"/envoy_1/:9:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"4. 测试¶ 现在配置已经完成了，我们就可以启动 Envoy 实例来进行测试了。在我们这个示例中，Envoy 暴露 80 端口来处理 HTTP 请求，暴露 443 端口来处理 HTTPS 请求，此外还在 8001 端口上暴露了管理页面，我们可以通过管理页面查看有关证书的信息。 使用如下命令启动 Envoy 代理： $ docker run -it --name proxy1 -p 80:8080 -p 443:8443 -p 8001:8001 -v $(pwd):/etc/envoy/ envoyproxy/envoy 启动完成后所有的 HTTPS 和 TLS 校验都是通过 Envoy 来进行处理的，所以我们不需要去修改应该程序。同样我们启动两个 HTTP 服务来处理传入的请求： $ docker run -d cnych/docker-http-server; docker run -d cnych/docker-http-server; 145738e12c174606f9e6e085ad2ec0ae9bf15a75d372b2bec8929e5d5df96be3 8f9a9355333d91b06a14d2bccc1a0d4a9afd20b258df561278fb94f01cdcd881 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8f9a9355333d cnych/docker-http-server \"/app\" 5 seconds ago Up 4 seconds 80/tcp eager_kapitsa 145738e12c17 cnych/docker-http-server \"/app\" 6 seconds ago Up 5 seconds 80/tcp beautiful_hermann a499a8ccaedc envoyproxy/envoy \"/docker-entrypoint.…\" About a minute ago Up About a minute 0.0.0.0:8001-\u003e8001/tcp, 10000/tcp, 0.0.0.0:80-\u003e8080/tcp, 0.0.0.0:443-\u003e8443/tcp proxy1 上面的几个容器启动完成后，就可以进行测试了，首先我们请求 HTTP 的服务，由于配置了自动跳转，所以应该会被重定向到 HTTPS 的版本上去： $ curl -H \"Host: example.com\" http://localhost -i HTTP/1.1 301 Moved Permanently # 重定向响应 location: https://example.com/ date: Wed, 08 Apr 2020 06:53:51 GMT server: envoy content-length: 0 我们可以看到上面有 HTTP/1.1 301 Moved Permanently 这样的重定向响应信息。然后我们尝试直接请求 HTTPS 的服务： $ curl -k -H \"Host: example.com\" https://localhost/service/1 -i HTTP/1.1 200 OK date: Wed, 08 Apr 2020 06:55:27 GMT content-length: 58 content-type: text/html; charset=utf-8 x-envoy-upstream-service-time: 0 server: envoy \u003ch1\u003eThis request was processed by host: 145738e12c17\u003c/h1\u003e $ curl -k -H \"Host: example.com\" https://localhost/service/2 -i HTTP/1.1 200 OK date: Wed, 08 Apr 2020 06:55:49 GMT content-length: 58 content-type: text/html; charset=utf-8 x-envoy-upstream-service-time: 0 server: envoy \u003ch1\u003eThis request was processed by host: 8f9a9355333d\u003c/h1\u003e 我们可以看到通过 HTTPS 进行访问可以正常得到对应的响应，需要注意的是由于我们这里使用的是自签名的证书，所以需要加上 -k 参数来忽略证书校验，如果没有这个参数则在请求的时候会报错： $ curl -H \"Host: example.com\" https://localhost/service/2 -i curl: (60) SSL certificate problem: self signed certificate More details here: https://curl.haxx.se/docs/sslcerts.html curl performs SSL certificate verification by default, using a \"bundle\" of Certificate Authority (CA) public keys (CA certs). If the default bundle file isn't adequate, you can specify an alternate file using the --cacert option. If this HTTPS server uses a certificate signed by a CA represented in the bundle, the certificate verification probably failed due to a problem with the certificate (it might be expired, or the name might not match the domain name in the URL). If you'd like to turn off curl's verification of the certificate, use the -k (or --insecure) option. HTTPS-proxy has similar options --proxy-cacert and --proxy-insecure. 我们也可以通过管理页面去查看证书相关的信息，上面我们启动容器的时候绑定了宿主机的 8001 端口，所以我们可以通过访问 http://localhost:8001/certs 来获取到证书相关的信息： 基于文件的动态配置¶ Envoy 除了支持静态配置之外，还支持动态配置，而且动态配置也是 Envoy 重点关注的功能，本节我们将学习如何将 Envoy 静态配置转换为动态配置，从而允许 Envoy 自动更新。 ","date":"5059-08-529","objectID":"/envoy_1/:10:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"1. Envoy 动态配置¶ 前面的章节中，我们都是直接使用的静态配置，但是当我们需要更改配置的时候就比较麻烦了，需要重启 Envoy 代理才会生效。要解决这个问题，我们可以将静态配置更改成动态配置，当我们使用动态配置的时候，更改了配置，Envoy 将会自动去重新加载配置。 Envoy 支持不同的模块进行动态配置，可配置的有如下几个 API： EDS：端点发现服务（EDS）可以让 Envoy 自动发现上游集群的成员，这使得我们可以动态添加或者删除处理流量请求的服务。 CDS：集群发现服务（CDS）可以让 Envoy 通过该机制自动发现在路由过程中使用的上游集群。 RDS：路由发现服务（RDS）可以让 Envoy 在运行时自动发现 HTTP 连接管理过滤器的整个路由配置，这可以让我们来完成诸如动态更改流量分配或者蓝绿发布之类的功能。 VHDS：虚拟主机发现服务（VHDS）允许根据需要与路由配置本身分开请求属于路由配置的虚拟主机。该 API 通常用于路由配置中有大量虚拟主机的部署中。 SRDS：作用域路由发现服务（SRDS）允许将路由表分解为多个部分。该 API 通常用于具有大量路由表的 HTTP 路由部署中。 LDS：监听器发现服务（LDS）可以让 Envoy 在运行时自动发现整个监听器。 SDS：密钥发现服务（SDS）可以让 Envoy 自动发现监听器的加密密钥（证书、私钥等）以及证书校验逻辑（受信任的根证书、吊销等）。 可以使用普通的文件来进行动态配置，也可以通过 REST-JSON 或者 gRPC 端点来提供。我们可以在 xDS 配置概述文档 中找到更多相关 API 的介绍。 在接下来的步骤中，我们将先更改配置来使用 EDS，让 Envoy 根据配置文件的数据来动态添加节点。 ","date":"5059-08-529","objectID":"/envoy_1/:11:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"Cluster ID¶ 首先我们这里定义了一个基本的 Envoy 配置文件，如下所示：(envoy.yaml) admin:access_log_path:\"/dev/null\"address:socket_address:address:0.0.0.0port_value:9901static_resources:listeners:- name:listener_0address:socket_address:{address: 0.0.0.0, port_value:10000}filter_chains:- filters:- name:envoy.http_connection_managerconfig:codec_type:autostat_prefix:ingress_httproute_config:name:local_routevirtual_hosts:- name:backenddomains:- \"*\"routes:- match:prefix:\"/\"route:cluster:targetClusterhttp_filters:- name:envoy.router 我们可以看到现在还没有配置 clusters 集群部分，这是因为我们要通过使用 EDS 来进行自动发现。 首先我们需要添加一个节点让 Envoy 来识别并应用这一个唯一的配置，将下面的配置放置在 envoy.yaml 文件的顶部区域： node:id:id_1cluster:test 除了 id 和 cluster 之外，我们还可以配置基于区域的一些位置信息来进行声明，比如 region、zone、sub_zone。 ","date":"5059-08-529","objectID":"/envoy_1/:11:1","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"2. EDS 配置¶ 接下来我们就可以来定义 EDS 配置了，可以来动态控制上游集群数据。在前面章节中，这部分的静态配置是这样的： clusters:- name:targetClusterconnect_timeout:0.25stype:STRICT_DNSdns_lookup_family:V4_ONLYlb_policy:ROUND_ROBINhosts:[{socket_address:{address: 172.17.0.3, port_value:80}},{socket_address:{address: 172.17.0.4, port_value:80}}] 现在我们将上面的静态配置转换成动态配置，首先需要转换为基于 EDS 的 eds_cluster_config 属性，并将类型更改为 EDS，将下面的集群配置添加到 Envoy 配置的末尾： clusters:- name:targetClusterconnect_timeout:0.25slb_policy:ROUND_ROBINtype:EDSeds_cluster_config:service_name:localservices # 可选，代替集群的名称，提供给 EDS 服务eds_config:# 集群的 EDS 更新源配置path:'/etc/envoy/eds.yaml' 上游的服务器 172.17.0.3 和 172.17.0.4 就将来自于 /etc/envoy/eds.yaml 文件，创建一个 eds.yaml 文件，内容如下所示： version_info:\"0\"resources:- \"@type\": \"type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\"cluster_name:\"localservices\"endpoints:- lb_endpoints:- endpoint:address:socket_address:address:\"172.17.0.3\"port_value:80 上面我们只定义了 172.17.0.3 这一个端点。 现在配置完成后，我们可以启动 Envoy 代理来进行测试。执行下面的命令启动 Envoy 容器： $ docker run --name=proxy-eds -d \\ -p 9901:9901 \\ -p 80:10000 \\ -v $(pwd)/manifests:/etc/envoy \\ envoyproxy/envoy:latest 然后同样和前面一样运行两个 HTTP 服务来作为上游服务器： $ docker run -d cnych/docker-http-server; docker run -d cnych/docker-http-server; $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b9f1955e2704 envoyproxy/envoy:latest \"/docker-entrypoint.…\" 4 seconds ago Up 2 seconds 0.0.0.0:9901-\u003e9901/tcp, 0.0.0.0:80-\u003e10000/tcp proxy-eds 4fd8eb3bd415 cnych/docker-http-server \"/app\" About a minute ago Up 59 seconds 80/tcp wonderful_hoover 73b616391920 cnych/docker-http-server \"/app\" About a minute ago Up About a minute 80/tcp pedantic_moser 根据上面的 EDS 配置，Envoy 将把所有的流量都发送到 172.17.0.3 这一个节点上去，我们可以使用 curl localhost 来测试下： $ curl localhost \u003ch1\u003eThis request was processed by host: 73b616391920\u003c/h1\u003e $ curl localhost \u003ch1\u003eThis request was processed by host: 73b616391920\u003c/h1\u003e 接下来我们来尝试更新上面的 EDS 配置添加上另外的一个节点，观察 Envoy 代理是否会自动生效。 由于我们这里使用的是 EDS 动态配置，所以当我们要扩展上游服务的时候，只需要将新的端点添加到上面我们指定的 eds.yaml 配置文件中即可，然后 Envoy 就会自动将新添加的端点包含进来。用上面同样的方式添加 172.17.0.4 这个端点，eds.yaml 内容如下所示： version_info:\"0\"resources:- \"@type\": \"type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\"cluster_name:\"localservices\"endpoints:- lb_endpoints:- endpoint:address:socket_address:address:\"172.17.0.3\"port_value:80- endpoint:address:socket_address:address:\"172.17.0.4\"port_value:80 由于我们这里是使用的 Docker 容器将配置文件挂载到容器中的，如果直接更改宿主机的配置文件，有时候可能不会立即触发文件变更，我们可以使用如下所示的命令来强制变更： $ mv manifests/eds.yaml tmp; mv tmp manifests/eds.yaml 这个时候正常情况下 Envoy 就会自动重新加载配置并将新的端点添加到负载均衡中去，这个时候我们再来访问代理： $ curl localhost \u003ch1\u003eThis request was processed by host: 73b616391920\u003c/h1\u003e $ curl localhost \u003ch1\u003eThis request was processed by host: 4fd8eb3bd415\u003c/h1\u003e 可以看到已经可以自动访问到另外的端点去了。 注意 我在测试阶段发现在 Mac 系统下面没有自动热加载，在 Linux 系统下面是可以正常重新加载的。 ","date":"5059-08-529","objectID":"/envoy_1/:12:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"3. CDS 配置¶ 现在已经配置好了 EDS，接下来我们就可以去扩大上游集群的规模了，如果我们想要能够动态添加新的域名和集群，就需要实现集群发现服务（CDS）API，在下面的示例中，我们将配置集群发现服务（CDS）和监听器发现服务（LDS）来进行动态配置。 创建一个名为 cds.yaml 的文件来配置集群服务发现的数据，文件内容如下所示： version_info:\"0\"resources:- \"@type\": \"type.googleapis.com/envoy.api.v2.Cluster\"name:targetClusterconnect_timeout:0.25slb_policy:ROUND_ROBINtype:EDSeds_cluster_config:service_name:localserviceseds_config:path:/etc/envoy/eds.yaml 此外，还需要创建一个名为 lds.yaml 的文件来放置监听器的配置，文件内容如下所示： version_info:\"0\"resources:- \"@type\": \"type.googleapis.com/envoy.api.v2.Listener\"name:listener_0address:socket_address:{address: 0.0.0.0, port_value:10000}filter_chanins:- filters:- name:envoy.http_connection_managerconfig:stat_prefix:ingress_httpcodec_type:AUTOroute_config:name:local_routevirtual_hosts:- name:local_servicedomains:[\"*\"]routes:- match:prefix:\"/\"route:cluster:targetClusterhttp_filters:- name:envoy.router 仔细观察可以发现 cds.yaml 和 lds.yaml 配置文件的内容基本上和上面的静态配置文件一致的。我们这里只是将集群和监听器拆分到外部文件中去，这个时候我们需要修改 Envoy 的配置来引用这些文件，我们可以通过将 static_resources 更改为 dynamic_resources 来进行配置。 重新新建一个 Envoy 配置文件，命名为 envoy1.yaml，内容如下所示： node:id:id_1cluster:testadmin:access_log_path:\"/dev/null\"address:socket_address:address:0.0.0.0port_value:9901# 动态配置dynamic_resources:lds_config:path:\"/etc/envoy/lds.yaml\"cds_config:path:\"/etc/envoy/cds.yaml\" 然后使用上面的配置文件重新启动一个新的 Envoy 代理，命令如下所示： $ docker run --name=proxy-xds -d \\ -p 9902:9901 \\ -p 81:10000 \\ -v $(pwd)/manifests:/etc/envoy \\ -v $(pwd)/manifests/envoy1.yaml:/etc/envoy/envoy.yaml \\ envoyproxy/envoy:latest 注意 为了避免和前面的 Envoy 实例端口冲突，这里我都修改了和宿主机上绑定的端口。 启动完成后，同样可以访问 Envoy 代理来测试是否生效了： $ curl localhost:81 \u003ch1\u003eThis request was processed by host: 4fd8eb3bd415\u003c/h1\u003e $ curl localhost:81 \u003ch1\u003eThis request was processed by host: 73b616391920\u003c/h1\u003e 现在我们基于上面配置的 CDS、LDS、EDS 的配置来动态添加一个新的集群。现在我们添加一个名为 newTargetCluster 的集群，内容如下所示： version_info:\"0\"resources:- \"@type\": \"type.googleapis.com/envoy.api.v2.Cluster\"name:targetClusterconnect_timeout:0.25slb_policy:ROUND_ROBINtype:EDSeds_cluster_config:service_name:localserviceseds_config:path:/etc/envoy/eds.yaml- \"@type\": \"type.googleapis.com/envoy.api.v2.Cluster\"name:newTargetClusterconnect_timeout:0.25slb_policy:ROUND_ROBINtype:EDSeds_cluster_config:service_name:localserviceseds_config:path:/etc/envoy/eds1.yaml 上面我们新增了一个新的集群，对应的 eds_config 配置文件是 eds1.yaml，所以我们同样需要去创建该文件去配置新的端点服务数据，内容如下所示： version_info:\"0\"resources:- \"@type\": \"type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\"cluster_name:\"localservices\"endpoints:- lb_endpoints:- endpoint:address:socket_address:address:\"172.17.0.6\"port_value:80- endpoint:address:socket_address:address:\"172.17.0.7\"port_value:80 这个时候新的集群添加上了，但是还没有任何路由来使用这个新集群，我们可以在 lds.yaml 中去配置，将之前配置的 targetCluster 替换成 newTargetCluster。 当然同样我们这里还需要运行两个简单的 HTTP 服务来作为上游服务提供服务，执行如下所示的命令： $ docker run -d cnych/docker-http-server; docker run -d cnych/docker-http-server; 上面的配置完成后，我们可以执行如下所示的命令来强制动态配置文件更新： $ mv manifests/cds.yaml tmp; mv tmp manifests/cds.yaml; mv manifests/lds.yaml tmp; mv tmp manifests/lds.yaml 这个时候 Envoy 应该就会自动重新加载并添加新的集群，我们同样可以执行 curl localhost:81 命令来验证： $ curl localhost:81 \u003ch1\u003eThis request was processed by host: f92b16426da5\u003c/h1\u003e $ curl localhost:81 \u003ch1\u003eThis request was processed by host: d89d590082dc\u003c/h1\u003e 可以看到已经变成了新的两个端点数据了。 基于 API 的动态端点发现¶ 当在 Envoy 配置中定义了上游集群后，Envoy 需要知道如何解析集群成员，这就是服务发现。端点发现服务（EDS）是 Envoy 基于 gRPC 或者用来获取集群成员的 REST-JSON API 服务的 xDS 管理服务。在本节我们将学习如何使用 REST-JSOn API 来配置端点的自动发现。 ","date":"5059-08-529","objectID":"/envoy_1/:13:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"1. 介绍¶ 在前面的章节中，我们使用文件来定义了静态和动态配置，在这里我们将介绍另外一种方式来进行动态配置：API 动态配置。 端点发现服务（EDS）是 Envoy 基于 gRPC 或者用来获取集群成员的 REST-JSON API 服务的 xDS 管理服务，集群成员在 Envoy 术语中成为端点，对于每个集群，Envoy 都从发现服务中获取端点。其中 EDS 就是最常用的服务发现机制，因为下面几个原因： Envoy 对每个上游主机都有一定的了解（相对于通过 DNS 解析的负载均衡器进行路由），可以做出更加智能的负载均衡策略。 发现 API 返回的每个主机的一些属性会将主机的负载均衡权重、金丝雀状态、区域等等告知 Envoy，这个额外的属性在负载均衡、统计数据收集等会被 Envoy 网格在全局中使用到 Envoy 项目在 Java 和 Golang 中都提供了 EDS 和其他服务发现的 gRPC 实现参考 接下来我们将更改配置来使用 EDS，从而允许基于来自 REST-JSON API 服务的数据进行动态添加节点。 ","date":"5059-08-529","objectID":"/envoy_1/:14:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"2. EDS 配置¶ 下面是提供的一个 Envoy 配置的初始配置 envoy.yaml，文件内容如下所示： admin:access_log_path:/dev/nulladdress:socket_address:address:127.0.0.1port_value:9000node:cluster:myclusterid:test-idstatic_resources:listeners:- name:listener_0address:socket_address:{address: 0.0.0.0, port_value:10000}filter_chains:- filters:- name:envoy.http_connection_managerconfig:stat_prefix:ingress_httpcodec_type:AUTOroute_config:name:local_routevirtual_hosts:- name:local_servicedomains:[\"*\"]routes:- match:{prefix:\"/\"}route:{cluster:targetCluster }http_filters:- name:envoy.router 接下来需要添加一个 EDS 类型的集群配置，并在 eds_config 中配置使用 REST API： clusters:- name:targetClustertype:EDSconnect_timeout:0.25seds_cluster_config:service_name:myserviceeds_config:api_config_source:api_type:REST cluster_names:[eds_cluster]refresh_delay:5s 然后需要定义 eds_cluster 的解析方式，这里我们可以使用静态配置： - name:eds_clustertype:STATICconnect_timeout:0.25shosts:[{socket_address:{address: 172.17.0.4, port_value:8080}}] 然后同样启动一个 Envoy 代理实例来进行测试： $ docker run --name=api-eds -d \\ -p 9901:9901 \\ -p 80:10000 \\ -v $(pwd)/manifests:/etc/envoy \\ envoyproxy/envoy:latest 然后启动一个如下所示的上游端点服务： $ docker run -p 8081:8081 -d -e EDS_SERVER_PORT='8081' cnych/docker-http-server:v4 启动完成后我们可以使用如下命令来测试上游的端点服务： $ curl http://localhost:8081 -i HTTP/1.0 200 OK Content-Type: text/html; charset=utf-8 Content-Length: 36 Server: Werkzeug/0.15.4 Python/2.7.16 Date: Tue, 14 Apr 2020 06:32:56 GMT 355d92db-9295-4a22-8b2c-fc0e5956ecf6 现在我们启动了 Envoy 代理和上游的服务集群，但是由于我们这里启动的服务并不是 eds_cluster 中配置的服务，所以还没有连接它们。这个时候我们去查看 Envoy 代理得日志，可以看到如下所示的一些错误： $ docker logs -f api-eds [2020-04-14 06:50:07.334][1][warning][config] [source/common/config/http_subscription_impl.cc:110] REST update for /v2/discovery:endpoints failed ...... ","date":"5059-08-529","objectID":"/envoy_1/:15:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"3. 启动 EDS¶ 为了让 Envoy 获取端点服务，我们需要启动 eds_cluster，我们这里将使用 python 实现的一个示例 eds_server。 使用如下所示的命令来启动 eds_server 服务： $ docker run -p 8080:8080 -d cnych/eds_server 服务启动后，可以在服务日志中查看到如下所示的日志信息，表明一个 Envoy 发现请求成功： * Serving Flask app \"main\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: on * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit) * Restarting with stat * Debugger is active! * Debugger PIN: 185-412-562 172.17.0.2 - - [14/Apr/2020 07:12:00] \"POST /v2/discovery:endpoints HTTP/1.1\" 200 - Inbound v2 request for discovery. POST payload: {u'node': {u'user_agent_name': u'envoy', u'cluster': u'mycluster', u'extensions': [{......}], u'user_agent_build_version': {u'version': {u'minor_number': 14, u'major_number': 1, u'patch': 1}, u'metadata': {u'ssl.version': u'BoringSSL', u'build.type': u'RELEASE', u'revision.status': u'Clean', u'revision.sha': u'3504d40f752eb5c20bc2883053547717bcb92fd8'}}, u'build_version': u'3504d40f752eb5c20bc2883053547717bcb92fd8/1.14.1/Clean/RELEASE/BoringSSL', u'id': u'test-id'}, u'type_url': u'type.googleapis.com/envoy.api.v2.ClusterLoadAssignment', u'resource_names': [u'myservice'], u'version_info': u'v1'} 172.17.0.2 - - [14/Apr/2020 07:12:08] \"POST /v2/discovery:endpoints HTTP/1.1\" 200 - 现在我们就可以将上游的服务配置添加到 EDS 服务中去了，这样可以让 Envoy 来自动发现上游服务。 我们在 Envoy 配置中将服务定义为了 myservice，所以我们需要针对该服务注册一个端点： $ curl -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' -d '{ \"hosts\": [ { \"ip_address\": \"172.17.0.3\", \"port\": 8081, \"tags\": { \"az\": \"cn-beijing-a\", \"canary\": false, \"load_balancing_weight\": 50 } } ] }' http://localhost:8080/edsservice/myservice 由于我们已经启动了上面注册的上游服务，所以现在我们可以通过 Envoy 代理访问到它了： $ curl -i http://localhost HTTP/1.1 200 OK content-type: text/html; charset=utf-8 content-length: 36 server: envoy date: Tue, 14 Apr 2020 07:33:04 GMT x-envoy-upstream-service-time: 4 355d92db-9295-4a22-8b2c-fc0e5956ecf6 接下来我们在上游集群中运行更多的节点，并调用 API 来进行动态注册，使用如下所示的命令来向上游集群再添加4个节点： for i in 8082 8083 8084 8085 do docker run -d -e EDS_SERVER_PORT=$i cnych/docker-http-server:v4; sleep .5 done 然后将上面的4个节点注册到 EDS 服务上面去，同样使用如下所示的 API 接口调用： $ curl -X PUT --header 'Content-Type: application/json' --header 'Accept: application/json' -d '{ \"hosts\": [ { \"ip_address\": \"172.17.0.3\", \"port\": 8081, \"tags\": { \"az\": \"cn-beijing-a\", \"canary\": false, \"load_balancing_weight\": 50 } }, { \"ip_address\": \"172.17.0.5\", \"port\": 8082, \"tags\": { \"az\": \"cn-beijing-a\", \"canary\": false, \"load_balancing_weight\": 50 } }, { \"ip_address\": \"172.17.0.6\", \"port\": 8083, \"tags\": { \"az\": \"cn-beijing-a\", \"canary\": false, \"load_balancing_weight\": 50 } }, { \"ip_address\": \"172.17.0.7\", \"port\": 8084, \"tags\": { \"az\": \"cn-beijing-a\", \"canary\": false, \"load_balancing_weight\": 50 } }, { \"ip_address\": \"172.17.0.8\", \"port\": 8085, \"tags\": { \"az\": \"cn-beijing-a\", \"canary\": false, \"load_balancing_weight\": 50 } } ] }' http://localhost:8080/edsservice/myservice 注册成功后，我们可以通过如下所示的命令来验证网络请求是否与注册的节点之间是均衡的： $ while true; do curl http://localhost; sleep .5; printf '\\n'; done d671262d-39b5-4150-9e25-94fb4f733959 dd1519ef-e03a-4708-bcd1-71890d38e40c b0c218f0-99f4-43e4-87fc-8989d49fccec 355d92db-9295-4a22-8b2c-fc0e5956ecf6 d671262d-39b5-4150-9e25-94fb4f733959 34690963-0887-4d36-8776-c35cf37fa901 ...... 根据上面的输出结果可以看到每次请求的服务是不同的响应，我们一共注册了5个端点服务。 现在我们来通过 API 删除 EDS 服务上面注册的主机来测试下，执行如下所示的命令清空 hosts： $ curl -X PUT --header 'Content-Type: application/json' --header 'Accept: application/json' -d '{ \"hosts\": [] }' http://localhost:8080/edsservice/myservice 现在如果我们尝试向 Envoy 发送请求，我们将会看到如下所示的不健康的日志信息： $ curl -v http://localhost * Rebuilt URL to: http://localhost/ * Trying ::1... * TCP_NODELAY set * Connected to localhost (::1) port 80 (#0) \u003e GET / HTTP/1.1 \u003e Host: localhost \u003e User-Agent: curl/7.54.0 \u003e Accept: */* \u003e \u003c HTTP/1.1 503 Service Unavailable \u003c content-length: 19 \u003c content-type: text/plain \u003c date: Tue, 14 Apr 2020 07:50:06 GMT \u003c server: envoy \u003c * Conne","date":"5059-08-529","objectID":"/envoy_1/:16:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"1. 代理配置¶ 首先创建一个 Envoy 配置文件 envoy.yaml，配置将任何域名的请求都代理到 172.17.0.3 和 172.17.0.4 这两个上游服务去。完整的配置如下所示： static_resources:listeners:- name:listener_0address:socket_address:{address: 0.0.0.0, port_value:8080}filter_chains:- filters:- name:envoy.http_connection_managerconfig:codec_type:autostat_prefix:ingress_httproute_config:name:local_routevirtual_hosts:- name:backenddomains:- \"*\"routes:- match:prefix:\"/\"route:cluster:targetClusterhttp_filters:- name:envoy.routerclusters:- name:targetClusterconnect_timeout:0.25stype:STRICT_DNSdns_lookup_family:V4_ONLYlb_policy:ROUND_ROBINhosts:[{socket_address:{address: 172.17.0.3, port_value:80}},{socket_address:{address: 172.17.0.4, port_value:80}}] 假设目前 172.17.0.3 这个上游服务出现了故障，现在的 Envoy 代理还是会继续向该服务转发流量过来的，这样当用户访问服务的时候就会遇到不可用的情况。对于这种情况我们更希望的是 Envoy 能够检测到服务不可用的时候自动将其从节点中移除掉，这其实就可以通过向集群中添加健康检查来完成。 ","date":"5059-08-529","objectID":"/envoy_1/:17:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"2. 添加健康检查¶ 健康检查可以添加到 Envoy 的集群配置中，如下所示的配置将在定义的每个节点内使用 /health 端点来进行健康检查，Envoy 会根据端点返回的 HTTP 状态来确定其是否健康。 health_checks:- timeout:1sinterval:10sinterval_jitter:1sunhealthy_threshold:6healthy_threshold:1http_health_check:path:\"/health\" 这里我们简单对上面配置的健康检查的关键字段进行下说明： interval：执行一次健康检查的时间间隔 unhealthy_threshold：将主机标记为不健康状态之前需要进行的不健康状态检查数量（相当于就是检测到几次不健康就认为是不健康的） healthy_threshold：将主机标记为健康状态之前需要进行的健康状态检查数量（相当于就是检测到几次健康就认为是健康的） http_health_check.path：用于健康检查请求的路径 关于健康检查的更多字段介绍可以查看官方的文档说明：https://www.envoyproxy.io/docs/envoy/latest/api-v2/api/v2/core/health_check.proto ","date":"5059-08-529","objectID":"/envoy_1/:18:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"3. 启动代理¶ 添加了健康检查之后，Envoy 将检查集群中定义的每个节点的运行状况。同样使用如下所示的命令启动 Envoy 代理： $ docker run -d --name proxy1 -p 80:8080 -v $(pwd)/manifests:/etc/envoy envoyproxy/envoy:latest 然后启动两个节点，都处于正常运行状态： $ docker run -d cnych/docker-http-server:healthy; docker run -d cnych/docker-http-server:healthy; 启动完成后，我们可以向 Envoy 发送请求，正常都可以从上面的两个上游服务中返回正常的请求： $ curl localhost -i HTTP/1.1 200 OK date: Wed, 15 Apr 2020 04:13:01 GMT content-length: 63 content-type: text/html; charset=utf-8 x-envoy-upstream-service-time: 0 server: envoy \u003ch1\u003eA healthy request was processed by host: b6336e79951d\u003c/h1\u003e $ curl localhost -i HTTP/1.1 200 OK date: Wed, 15 Apr 2020 04:13:02 GMT content-length: 63 content-type: text/html; charset=utf-8 x-envoy-upstream-service-time: 0 server: envoy \u003ch1\u003eA healthy request was processed by host: 9a4c07cc4306\u003c/h1\u003e ","date":"5059-08-529","objectID":"/envoy_1/:19:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"4. 测试¶ 接下来我们来测试下 Envoy 是如何处理不正常的节点的。在一个独立的命令行终端中，启动一个循环来发送请求，可以让我们来观察状态变化： $ while true; do curl localhost; sleep .5; done ...... 然后使用如下命令，我们可以来确定哪个 Docker 容器的 IP 为 172.17.0.3，然后将这个节点变成不健康的，然后 Envoy 就会自动将其从负载均衡中移除掉。 $ docker ps -q | xargs -n 1 docker inspect --format '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}} {{ .Config.Hostname }}' | sed 's/ \\// /' 172.17.0.4 b6336e79951d 172.17.0.3 9a4c07cc4306 172.17.0.2 6928df882c4f 要让该一个节点变成不健康的状态，我们可以直接请求 unhealthy 的端点： $ curl 172.18.0.3/unhealthy 这个时候可以看到另外一个终端中循环请求的日志信息中就出现了 unhealthy 的相关信息： ...... \u003ch1\u003eA unhealthy request was processed by host: 9a4c07cc4306\u003c/h1\u003e \u003ch1\u003eA unhealthy request was processed by host: 9a4c07cc4306\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: b6336e79951d\u003c/h1\u003e \u003ch1\u003eA unhealthy request was processed by host: 9a4c07cc4306\u003c/h1\u003e ...... 这个时候访问该容器就会返回 500 状态码了： $ curl 172.18.0.3 -i HTTP/1.1 500 Internal Server Error Date: Wed, 15 Apr 2020 04:23:59 GMT Content-Length: 65 Content-Type: text/html; charset=utf-8 \u003ch1\u003eA unhealthy request was processed by host: 9a4c07cc4306\u003c/h1\u003e 在这段时间内，Envoy会将请求发送到健康检查的端点。如果健康检查的端点发生了故障，它将继续向该服务发送流量，直到达到 unhealthy_threshold 这么多次不健康的请求，此时，Envoy 将从负载均衡器中将其删除。这个时候可以看到另外一个终端中循环请求的日志信息中就只有一个容器的信息了： ...... \u003ch1\u003eA healthy request was processed by host: b6336e79951d\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: b6336e79951d\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: b6336e79951d\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: b6336e79951d\u003c/h1\u003e ...... 与此同时，Envoy 还会继续检查健康状态的端点，来查看它是否再次变得可用了，一旦可用，它将又会被添加回到 Envoy 的上游服务器集群中去。 我们可以访问下上面不健康容器的 healthy 端点让其变成正常运行状态： $ curl 172.17.0.3/healthy 我们健康检查的间隔是10s，healthy_threshold 阈值是1，所以检测到成功后 Envoy 就会将该容器再次添加回来。这个时候可以看到另外一个终端中循环请求的日志信息中就又出现了两个容器的信息： ...... \u003ch1\u003eA healthy request was processed by host: 9a4c07cc4306\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 9a4c07cc4306\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: b6336e79951d\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: b6336e79951d\u003c/h1\u003e ...... 接下来我们再来测试下所有服务均不可用时发生的情况。目前已经有两个运行正常的上游服务器，Envoy 代理会在它们之间进行负载均衡。 和上面方法一样，对两个上游服务访问 unhealthy 端点，这样就可以将两个服务变成不健康的状态： $ curl 172.18.0.3/unhealthy $ curl 172.18.0.4/unhealthy 现在两个上游服务都已经不健康了，所以当我们请求 Envoy 时，将得到如下所示的信息： $ curl localhost -i HTTP/1.1 500 Internal Server Error date: Wed, 15 Apr 2020 06:19:01 GMT content-length: 65 content-type: text/html; charset=utf-8 x-envoy-upstream-service-time: 0 server: envoy \u003ch1\u003eA unhealthy request was processed by host: b6336e79951d\u003c/h1\u003e ","date":"5059-08-529","objectID":"/envoy_1/:20:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"},{"categories":null,"content":"5. 被动健康检查¶ 和前面的主动健康检查不同，被动健康检查从真实的请求响应来确定端点是否健康。一旦端点被删除后，Envoy 将使用基于超时的方法进行重新插入，使用该方法可以通过配置 interval 将不正常的主机重新添加到集群中去，后续的每次删除都会增加一定的时间间隔，这样的话不健康的端点对用户的流量影响就会尽可能小。 和前面的主动健康检查一样，被动健康检查也需要针对每个集群进行配置。如下所示的配置表示返回3个连续的5xx错误时，该配置会将主机删除30s： outlier_detection:consecutive_5xx:\"3\"base_ejection_time:\"30s\" consecutive_5xx：表示上游主机返回一定数量的连续 5xx 状态，则将其移除。需要注意的是在这种情况下，5xx表示实际的5xx响应码值，或者是一个导致 HTTP 路由器返回一个上游的事件行为（比如重置、连接失败等） base_ejection_time：表示移除主机的基准时间。真实的时间等于基准时间乘以主机移除的次数，默认为 30000ms 或 30s。 当启用被动健康检查过后，Envoy 会根据实际的请求响应来删除主机。同样首先我们先运行两个新的上游节点: $ docker run -d cnych/docker-http-server:healthy; docker run -d cnych/docker-http-server:healthy; 然后启动一个新的 Envoy 代理，对应的配置文件为 envoy1.yaml，内容如下所示： static_resources:listeners:- name:listener_0address:socket_address:{address: 0.0.0.0, port_value:8080}filter_chains:- filters:- name:envoy.http_connection_managerconfig:codec_type:autostat_prefix:ingress_httproute_config:name:local_routevirtual_hosts:- name:backenddomains:- \"*\"routes:- match:prefix:\"/\"route:cluster:targetClusterhttp_filters:- name:envoy.routerclusters:- name:targetClusterconnect_timeout:0.25stype:STRICT_DNSdns_lookup_family:V4_ONLYlb_policy:ROUND_ROBINhosts:[{socket_address:{address: 172.17.0.5, port_value:80}},{socket_address:{address: 172.17.0.6, port_value:80}}]outlier_detection:consecutive_5xx:\"3\"base_ejection_time:\"30s\" 然后执行如下命令启动 Envoy 代理： $ docker run -d --name proxy2 -p 81:8080 \\ -v $(pwd)/manifests/envoy1.yaml:/etc/envoy/envoy.yaml \\ envoyproxy/envoy 启动完成后，在单独的一个命令行终端中，执行下面的命令来循环发送请求观察状态的变化： $ while true; do curl localhost:81; sleep .5; done 然后我们将 172.17.0.5 这个端点变成不健康的状态： $ curl 172.17.0.5/unhealthy 该命令会将该端点的所有请求变成 500 错误： $ curl 172.17.0.5 -i HTTP/1.1 500 Internal Server Error Date: Wed, 15 Apr 2020 06:55:02 GMT Content-Length: 65 Content-Type: text/html; charset=utf-8 \u003ch1\u003eA unhealthy request was processed by host: 55e0950029b8\u003c/h1\u003e 然后我们会在循环的终端中看到会收到3个不健康的请求，然后 Envoy 就会将该上游服务给移除掉： ...... \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 55e0950029b8\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e \u003ch1\u003eA unhealthy request was processed by host: 55e0950029b8\u003c/h1\u003e \u003ch1\u003eA unhealthy request was processed by host: 55e0950029b8\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e \u003ch1\u003eA unhealthy request was processed by host: 55e0950029b8\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e ...... 然后我们也可以再次将 172.17.0.5 标记为健康，执行如下命令即可： $ curl 172.17.0.5/healthy 然后差不多 30s 过后，我们查看 Envoy 又将该端点添加回来参与负载均衡了： ...... \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 55e0950029b8\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 55e0950029b8\u003c/h1\u003e \u003ch1\u003eA healthy request was processed by host: 5749edc61125\u003c/h1\u003e ...... 到这里我们就完成了在 Envoy 中的健康检查相关的配置。 ","date":"5059-08-529","objectID":"/envoy_1/:21:0","tags":["envoy"],"title":"Envoy入门介绍","uri":"/envoy_1/"}]